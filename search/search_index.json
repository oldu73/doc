{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to doc Hello, doc ;)","title":"Home"},{"location":"#welcome-to-doc","text":"Hello, doc ;)","title":"Welcome to doc"},{"location":"agilemethodology/1-agilemethodology-misc/","text":"Agile Methodology - 01 - Misc Agile Guide M\u00e9thode agile scrum, exemple Definitions A Complete Guide to Agile Epics User story A single request Epic A group of user stories Initiative A group of epics Theme A label for organizational goals Story How to Create User Stories \u201cwho,\u201d \u201cwhat,\u201d and \u201cwhy\u201d of a particular requirement. Who wants something? What do they want? Why do they want it? Common template: \u201cAs [persona], I want to [action], so that I can [benefit].\u201d Cut a Story (US = User Story) D\u00e9couper une user Story ECRIRE ET D\u00c9COUPER SES USERS STORIES COMME UN NINJA Appendix Product Do Product as Unicorns do IoT Explorez le panorama des objets connect\u00e9s Swisscom et Feldschl\u00f6sschen mettent la bi\u00e8re en r\u00e9seau","title":"Agile Methodology - 01 - Misc"},{"location":"agilemethodology/1-agilemethodology-misc/#agile-methodology-01-misc","text":"Agile Guide M\u00e9thode agile scrum, exemple","title":"Agile Methodology - 01 - Misc"},{"location":"agilemethodology/1-agilemethodology-misc/#definitions","text":"A Complete Guide to Agile Epics User story A single request Epic A group of user stories Initiative A group of epics Theme A label for organizational goals","title":"Definitions"},{"location":"agilemethodology/1-agilemethodology-misc/#story","text":"How to Create User Stories \u201cwho,\u201d \u201cwhat,\u201d and \u201cwhy\u201d of a particular requirement. Who wants something? What do they want? Why do they want it? Common template: \u201cAs [persona], I want to [action], so that I can [benefit].\u201d","title":"Story"},{"location":"agilemethodology/1-agilemethodology-misc/#cut-a-story-us-user-story","text":"D\u00e9couper une user Story ECRIRE ET D\u00c9COUPER SES USERS STORIES COMME UN NINJA","title":"Cut a Story (US = User Story)"},{"location":"agilemethodology/1-agilemethodology-misc/#appendix","text":"","title":"Appendix"},{"location":"agilemethodology/1-agilemethodology-misc/#product","text":"Do Product as Unicorns do","title":"Product"},{"location":"agilemethodology/1-agilemethodology-misc/#iot","text":"Explorez le panorama des objets connect\u00e9s Swisscom et Feldschl\u00f6sschen mettent la bi\u00e8re en r\u00e9seau","title":"IoT"},{"location":"docker/1-docker-misc/","text":"Docker - 01 - Misc Maven, Java Mainly for Maven projects but maybe easily adapted for standard Java project. Project From IntelliJ Java (Maven) project to a container hosted on a Raspberry. Deployment process: (IntelliJ) maven <project> package Dockerfile-arm64v8 build image (terminal) docker login docker push oldu73/msc:<project>_arm64v8_latest (raspberry) maybe stop/remove previous existing <project> container instance docker login sudo docker pull oldu73/msc:<project>_arm64v8_latest sudo docker run -d -p <port>:<port> --name <container_name> oldu73/msc:<project>_arm64v8_latest sudo docker container ps -a sudo docker exec -it <container_name> sh (container) tail -1000f /var/log/<file>.log Dockerfile-arm64v8: FROM arm64v8/openjdk:8-oraclelinux7 COPY ./target/project.jar /tmp WORKDIR /tmp EXPOSE <port> ENTRYPOINT [\"java\", \"-jar\", \"project.jar\"] Debug, HotSwap (IntelliJ) maven <project> package Dockerfile run image // bind port 8080 Dockerfile: FROM openjdk:8 COPY ./target/<project>.jar /tmp WORKDIR /tmp EXPOSE 8080 ENTRYPOINT [\"java\", \"-jar\", \"-agentlib:jdwp=transport=dt_socket,address=8080,server=y,suspend=n\", \"<project>.jar\"] IntelliJ, Remote JVM Debug, configuration: Debugger mode: Attach to remote JVM Transport: Socket Host: localhost Port: 8080 Command line arguments for remote JVM: -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8080 Run IntelliJ remote debug configuration. Modify code, set breakpoint and then reload changed class. By adding some system out you may observe live changes in container log console. Disk usage docker system df Docker reclaim unused image space: docker image prune -a Docker container size: docker ps --size","title":"Docker - 01 - Misc"},{"location":"docker/1-docker-misc/#docker-01-misc","text":"","title":"Docker - 01 - Misc"},{"location":"docker/1-docker-misc/#maven-java","text":"Mainly for Maven projects but maybe easily adapted for standard Java project.","title":"Maven, Java"},{"location":"docker/1-docker-misc/#project","text":"From IntelliJ Java (Maven) project to a container hosted on a Raspberry. Deployment process: (IntelliJ) maven <project> package Dockerfile-arm64v8 build image (terminal) docker login docker push oldu73/msc:<project>_arm64v8_latest (raspberry) maybe stop/remove previous existing <project> container instance docker login sudo docker pull oldu73/msc:<project>_arm64v8_latest sudo docker run -d -p <port>:<port> --name <container_name> oldu73/msc:<project>_arm64v8_latest sudo docker container ps -a sudo docker exec -it <container_name> sh (container) tail -1000f /var/log/<file>.log Dockerfile-arm64v8: FROM arm64v8/openjdk:8-oraclelinux7 COPY ./target/project.jar /tmp WORKDIR /tmp EXPOSE <port> ENTRYPOINT [\"java\", \"-jar\", \"project.jar\"]","title":"Project"},{"location":"docker/1-docker-misc/#debug-hotswap","text":"(IntelliJ) maven <project> package Dockerfile run image // bind port 8080 Dockerfile: FROM openjdk:8 COPY ./target/<project>.jar /tmp WORKDIR /tmp EXPOSE 8080 ENTRYPOINT [\"java\", \"-jar\", \"-agentlib:jdwp=transport=dt_socket,address=8080,server=y,suspend=n\", \"<project>.jar\"] IntelliJ, Remote JVM Debug, configuration: Debugger mode: Attach to remote JVM Transport: Socket Host: localhost Port: 8080 Command line arguments for remote JVM: -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8080 Run IntelliJ remote debug configuration. Modify code, set breakpoint and then reload changed class. By adding some system out you may observe live changes in container log console.","title":"Debug, HotSwap"},{"location":"docker/1-docker-misc/#disk-usage","text":"docker system df Docker reclaim unused image space: docker image prune -a Docker container size: docker ps --size","title":"Disk usage"},{"location":"docker/10-compose-services/","text":"Compose - 10 - Services Docker Compose - Services Use of Docker Compose with many services. In this chapter we setup a complete development/production environnement close to what we may found in real life projects, scalable and so on. Project architecture Introduction to project architecture: React (Application), could be Vue.js or Angular, would be the same. NGINX (Http request). Node.js (API). MongoDB (database). Development NGINX as reverse proxy follow requests: != /api + == /sockjs-node (websocket for live reload feature) -> React. == /api -> Node.js -> MongoDB. Production NGINX as reverse proxy (on big application may also be used as a load balancer) follow requests: != /api -> another NGINX that handle React production build output ('build' folder). == /api -> Node.js -> MongoDB. Setup mkdir fullstack cd fullstack React application: mkdir client Node.js application: mkdir api MongoDB database: mkdir db NGINX as a reverse proxy, in charge of dispatching http requests: mkdir reverse-proxy Docker Compose: touch docker-compose.dev.yml touch docker-compose.prod.yml Client configuration Setting up the client configuration cd client npx create-react-app client mv client/* . rm -rf client rm -rf node_modules touch Dockerfile.dev /fullstack/client/Dockerfile.dev: FROM node:alpine WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . CMD [\"npm\", \"start\"] /fullstack/docker-compose.dev.yml: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 From 'fullstack' folder, we'll let '--build' option to ensure building images before starting containers: code . docker-compose -f docker-compose.dev.yml up --build Test by browsing at localhost:3000 We add a function to '/fullstack/client/src/App.js' file in order to get a counter from api (not ready yet but we prepare here the application in advance). /fullstack/client/src/App.js: import logo from './logo.svg' ; import './App.css' ; import { useState , useEffect } from 'react' ; function App () { const [ count , setCount ] = useState (); useEffect (() => { async function fetchCount () { try { const response = await fetch ( '/api/count' ) if ( response . ok ) { setCount ( await response . json ()); } } catch ( e ) { console . log ( e ); } } fetchCount (); }, []); return ( < div className = \"App\" > < header className = \"App-header\" > < img src = { logo } className = \"App-logo\" alt = \"logo\" /> < p > Edit < code > src / App . js < /code> and save to reload. < /p> < p > Count : { count } < /p> < a className = \"App-link\" href = \"https://reactjs.org\" target = \"_blank\" rel = \"noopener noreferrer\" > Learn React < /a> < /header> < /div> ); } export default App ; In Docker Compose logs we may observe successful compilation: . . client_1 | webpack 5.65.0 compiled successfully in 191 ms Test again by browsing at localhost:3000 Stop/remove stack: docker-compose -f docker-compose.dev.yml down Note: - webpack is exclusively devoted for development. Set up Node.js API From project root folder: - .../fullstack cd api npm init -y npm i express nodemon mongodb mkdir src cd src touch index.js index.js (from previous node server project) const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; const MongUrl = process . env . NODE_ENV === 'production' ? `mongodb:// ${ process . env . MONGO_USERNAME } : ${ process . env . MONGO_PWD } @db` : `mongodb://db` console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( MongUrl , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/api/count' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . all ( '*' , ( req , res ) => { res . status ( 404 ). end (); }); app . listen ( 80 ); .../fullstack/docker-compose.dev.yml version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 api : build : context : ./api dockerfile : Dockerfile volumes : - type : bind source : ./api/src target : /app/src ports : - 3001:80 ../fullstack/api touch Dockerfile .../fullstack/api/Dockerfile FROM node:alpine WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . EXPOSE 80 CMD [\"npm\", \"start\"] .../fullstack/api/package.json { \"name\" : \"api\" , \"version\" : \"1.0.0\" , \"description\" : \"\" , \"main\" : \"index.js\" , \"scripts\" : { \"start\" : \"nodemon ./src/index.js\" , \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\" : [], \"author\" : \"\" , \"license\" : \"ISC\" , \"dependencies\" : { \"express\" : \"^4.17.2\" , \"mongodb\" : \"^4.3.0\" , \"nodemon\" : \"^2.0.15\" } } Test API in a terminal from .../fullstack/ folder code . docker-compose -f docker-compose.dev.yml run api Set up database service Add db service to Docker Compose configuration file. .../fullstack/docker-compose.dev.yml: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 api : build : context : ./api dockerfile : Dockerfile volumes : - type : bind source : ./api/src target : /app/src ports : - 3001:80 db : image : mongo volumes : - type : volume source : dbtest target : /data/db volumes : dbtest : Initialize database container. From '.../fullstack' folder: docker-compose -f docker-compose.dev.yml run db Now it should be launch and we connect on it in a second terminal: docker container exec -it fullstack_db_run_c97d6cfbd602 sh mongo use test db.count.insertOne({ count: 0 }) exit exit Now it's OK, we may close second terminal and 'Ctrl+c' database container. We may test application with (from '.../fullstack' folder): (code . // it's always better to have VS Code started from application root folder) docker-compose -f docker-compose.dev.yml up Test application by browsing to http://localhost:3001/api/count We may also observe that React is running by browsing to http://localhost:3000/ Then 'Ctrl-c' to stop stack. Set up the NGINX reverse proxy From '.../fullstack/reverse-proxy' folder: mkdir conf touch conf/dev.conf touch Dockerfile.dev .../fullstack/reverse-proxy/Dockerfile.dev: FROM nginx:latest COPY ./conf/dev.conf /etc/nginx/conf.d/default.conf EXPOSE 80 .../fullstack/reverse-proxy/conf/dev.conf: server { listen 80; location / { proxy_pass http://client:3000; } location /api { proxy_pass http://api; } location /sockjs-node { proxy_pass http://client:3000; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } No need to specify port for api because inside stack (network), not from a host point of view, it's listening on default http port ('80'). Among HTTP standards, by default some 'headers' called 'hop-by-hop' aren't passed by proxy to server and then avoid live reload to work. To fix this, the 'sockjs-node' is a needed technical part to make live reload works. .../fullstack/docker-compose.dev.yml: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 api : build : context : ./api dockerfile : Dockerfile volumes : - type : bind source : ./api/src target : /app/src ports : - 3001:80 db : image : mongo volumes : - type : volume source : dbtest target : /data/db reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.dev ports : - 80:80 depends_on : - api - db volumes : dbtest : Test, everything is supposed to work: docker-compose -f docker-compose.dev.yml up --build In a second terminal: docker-compose -f docker-compose.dev.yml ps NAME COMMAND SERVICE STATUS PORTS fullstack_api_1 \"docker-entrypoint.s\u2026\" api running 0.0.0.0:3001->80/tcp fullstack_client_1 \"docker-entrypoint.s\u2026\" client running 0.0.0.0:3000->3000/tcp fullstack_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp fullstack_reverse-proxy_1 \"/docker-entrypoint.\u2026\" reverse-proxy running 0.0.0.0:80->80/tcp We may observe then the 4 components composing our application are running. And test by browsing to http://localhost/ , note that there's not need to specify port this time thanks to NGINX that do his job as a reverse proxy by distributing requests through our application by listening on default http port ('80'). By refreshing the page, you also may observe counter increasing. To test live reload edit file '.../fullstack/client/src/App.js' and observe live changes in browser window (even without refreshing the page). We now have a full functional development stack that maybe up with only one command: docker-compose -f docker-compose.dev.yml up Set up production configuration Reset Docker environnement (if needed): docker system prune -a docker volume prune Client From folder '.../fullstack/client': touch Dockerfile.prod .../fullstack/client/Dockerfile.prod FROM node:alpine as build WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . RUN npm run build FROM nginx:latest COPY --from=build /app/build /usr/share/nginx/html EXPOSE 80 ! Be aware of RUN npm run build, NOT CMD! .../fullstack/docker-compose.prod.yml version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped Start VS Code in '.../fullstack' folder with 'code .' command and then: docker-compose -f docker-compose.prod.yml run -p 80:80 client Browse to http://localhost/ to validate React application is running. API In '.../fullstack/api/src/index.js' notice mongodb connection URL with MONGO_USERNAME and MONGO_PWD credentials that comes from process (production) environnement: const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; const MongUrl = process . env . NODE_ENV === 'production' ? `mongodb:// ${ process . env . MONGO_USERNAME } : ${ process . env . MONGO_PWD } @db` : `mongodb://db` console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( MongUrl , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/api/count' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . all ( '*' , ( req , res ) => { res . status ( 404 ). end (); }); app . listen ( 80 ); In '.../fullstack/api' folder: touch .env .../fullstack/api/.env: MONGO_USERNAME=paul MONGO_PWD=123 We don't want that secret information to be copied in container: touch .dockerignore .../fullstack/api/.dockerignore: .env .../fullstack/docker-compose.prod.yml version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped Test API with (tanks to \"console.log(process.env)\" in \".../fullstack/api/src/index.js\" we may observe environment variables): docker-compose -f docker-compose.prod.yml run api { . . . MONGO_USERNAME: 'paul', . . MONGO_PWD: '123', . NODE_ENV: 'production', . . } Hit 'Ctrl+c' to stop. DB Secure the database by providing root username and password through an environment file. From folder '.../fullstack/db': touch .env Get credential syntax from 'Environment Variables' section in Mongo image on Docker Hub . In file '.../fullstack/db/.env' add credentials: MONGO_INITDB_ROOT_USERNAME=admin MONGO_INITDB_ROOT_PASSWORD=password Provide environment file (for root user credentials) and external volume for db service in docker compose configuration file. Set up db service in '.../fullstack/docker-compose.prod.yml' file: version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped db : image : mongo volumes : - type : volume source : dbprod target : /data/db env_file : - ./db/.env restart : unless-stopped volumes : dbprod : external : true Create volume 'dbprod': docker volume create dbprod Run 'db' container in detached mode from '.../fullstack' folder: docker-compose -f docker-compose.prod.yml run -d db Initialize database by connecting to it with root user credentials. docker container exec -it fullstack_db_run_e9fa9328e2b8 sh mongo use admin db.auth({ user: 'admin', pwd: 'password' }) Create the needed user for API access, the one described in '.env' environnement file that reside in '.../fullstack/api' folder. db.createUser({ user: 'paul', pwd: '123', roles: [{ role: 'readWrite', db: 'test' }] }) Setup collection for 'count'. use test db.count.insertOne({ count: 0 }) exit exit docker stop fullstack_db_run_e9fa9328e2b8 Now db is ready and API may communicate with it. To test, launch complete stack with port 80 open for 'api' service (only for testing, because after, in this production context, this is the reverse proxy role to communicate on port 80). .../fullstack/docker-compose.prod.yml: version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped ports : - 80:80 db : image : mongo volumes : - type : volume source : dbprod target : /data/db env_file : - ./db/.env restart : unless-stopped volumes : dbprod : external : true Launch full stack: docker-compose -f docker-compose.prod.yml up (--build) Browse to localhost/api/count and refresh page to observe counter incrementing. Hit 'Ctrl+c' to stop. Remove port 80 for 'api' service in 'docker-compose.prod.yml' file. Reverse proxy Production configuration file in '.../fullstack/reverse-proxy/conf' folder: touch prod.conf Production Docker file in '.../fullstack/reverse-proxy' folder: touch Dockerfile.prod .../fullstack/reverse-proxy/Dockerfile.prod: FROM nginx:latest COPY ./conf/prod.conf /etc/nginx/conf.d/default.conf EXPOSE 80 .../fullstack/reverse-proxy/conf/prod.conf: server { listen 80; location / { proxy_pass http://client; } location /api { proxy_pass http://api; } } Add 'reverse-proxy' service in '.../fullstack/docker-compose.prod.yml' file: version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped depends_on : - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db env_file : - ./db/.env restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true Test, from '.../fullstack' folder: docker-compose -f docker-compose.prod.yml down -v docker-compose -f docker-compose.prod.yml up --build Browse to localhost and refresh page to observe counter incrementing.","title":"Compose - 10 - Services"},{"location":"docker/10-compose-services/#compose-10-services","text":"Docker Compose - Services Use of Docker Compose with many services. In this chapter we setup a complete development/production environnement close to what we may found in real life projects, scalable and so on.","title":"Compose - 10 - Services"},{"location":"docker/10-compose-services/#project-architecture","text":"Introduction to project architecture: React (Application), could be Vue.js or Angular, would be the same. NGINX (Http request). Node.js (API). MongoDB (database).","title":"Project architecture"},{"location":"docker/10-compose-services/#development","text":"NGINX as reverse proxy follow requests: != /api + == /sockjs-node (websocket for live reload feature) -> React. == /api -> Node.js -> MongoDB.","title":"Development"},{"location":"docker/10-compose-services/#production","text":"NGINX as reverse proxy (on big application may also be used as a load balancer) follow requests: != /api -> another NGINX that handle React production build output ('build' folder). == /api -> Node.js -> MongoDB.","title":"Production"},{"location":"docker/10-compose-services/#setup","text":"mkdir fullstack cd fullstack React application: mkdir client Node.js application: mkdir api MongoDB database: mkdir db NGINX as a reverse proxy, in charge of dispatching http requests: mkdir reverse-proxy Docker Compose: touch docker-compose.dev.yml touch docker-compose.prod.yml","title":"Setup"},{"location":"docker/10-compose-services/#client-configuration","text":"Setting up the client configuration cd client npx create-react-app client mv client/* . rm -rf client rm -rf node_modules touch Dockerfile.dev /fullstack/client/Dockerfile.dev: FROM node:alpine WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . CMD [\"npm\", \"start\"] /fullstack/docker-compose.dev.yml: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 From 'fullstack' folder, we'll let '--build' option to ensure building images before starting containers: code . docker-compose -f docker-compose.dev.yml up --build Test by browsing at localhost:3000 We add a function to '/fullstack/client/src/App.js' file in order to get a counter from api (not ready yet but we prepare here the application in advance). /fullstack/client/src/App.js: import logo from './logo.svg' ; import './App.css' ; import { useState , useEffect } from 'react' ; function App () { const [ count , setCount ] = useState (); useEffect (() => { async function fetchCount () { try { const response = await fetch ( '/api/count' ) if ( response . ok ) { setCount ( await response . json ()); } } catch ( e ) { console . log ( e ); } } fetchCount (); }, []); return ( < div className = \"App\" > < header className = \"App-header\" > < img src = { logo } className = \"App-logo\" alt = \"logo\" /> < p > Edit < code > src / App . js < /code> and save to reload. < /p> < p > Count : { count } < /p> < a className = \"App-link\" href = \"https://reactjs.org\" target = \"_blank\" rel = \"noopener noreferrer\" > Learn React < /a> < /header> < /div> ); } export default App ; In Docker Compose logs we may observe successful compilation: . . client_1 | webpack 5.65.0 compiled successfully in 191 ms Test again by browsing at localhost:3000 Stop/remove stack: docker-compose -f docker-compose.dev.yml down Note: - webpack is exclusively devoted for development.","title":"Client configuration"},{"location":"docker/10-compose-services/#set-up-nodejs-api","text":"From project root folder: - .../fullstack cd api npm init -y npm i express nodemon mongodb mkdir src cd src touch index.js index.js (from previous node server project) const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; const MongUrl = process . env . NODE_ENV === 'production' ? `mongodb:// ${ process . env . MONGO_USERNAME } : ${ process . env . MONGO_PWD } @db` : `mongodb://db` console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( MongUrl , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/api/count' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . all ( '*' , ( req , res ) => { res . status ( 404 ). end (); }); app . listen ( 80 ); .../fullstack/docker-compose.dev.yml version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 api : build : context : ./api dockerfile : Dockerfile volumes : - type : bind source : ./api/src target : /app/src ports : - 3001:80 ../fullstack/api touch Dockerfile .../fullstack/api/Dockerfile FROM node:alpine WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . EXPOSE 80 CMD [\"npm\", \"start\"] .../fullstack/api/package.json { \"name\" : \"api\" , \"version\" : \"1.0.0\" , \"description\" : \"\" , \"main\" : \"index.js\" , \"scripts\" : { \"start\" : \"nodemon ./src/index.js\" , \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\" : [], \"author\" : \"\" , \"license\" : \"ISC\" , \"dependencies\" : { \"express\" : \"^4.17.2\" , \"mongodb\" : \"^4.3.0\" , \"nodemon\" : \"^2.0.15\" } } Test API in a terminal from .../fullstack/ folder code . docker-compose -f docker-compose.dev.yml run api","title":"Set up Node.js API"},{"location":"docker/10-compose-services/#set-up-database-service","text":"Add db service to Docker Compose configuration file. .../fullstack/docker-compose.dev.yml: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 api : build : context : ./api dockerfile : Dockerfile volumes : - type : bind source : ./api/src target : /app/src ports : - 3001:80 db : image : mongo volumes : - type : volume source : dbtest target : /data/db volumes : dbtest : Initialize database container. From '.../fullstack' folder: docker-compose -f docker-compose.dev.yml run db Now it should be launch and we connect on it in a second terminal: docker container exec -it fullstack_db_run_c97d6cfbd602 sh mongo use test db.count.insertOne({ count: 0 }) exit exit Now it's OK, we may close second terminal and 'Ctrl+c' database container. We may test application with (from '.../fullstack' folder): (code . // it's always better to have VS Code started from application root folder) docker-compose -f docker-compose.dev.yml up Test application by browsing to http://localhost:3001/api/count We may also observe that React is running by browsing to http://localhost:3000/ Then 'Ctrl-c' to stop stack.","title":"Set up database service"},{"location":"docker/10-compose-services/#set-up-the-nginx-reverse-proxy","text":"From '.../fullstack/reverse-proxy' folder: mkdir conf touch conf/dev.conf touch Dockerfile.dev .../fullstack/reverse-proxy/Dockerfile.dev: FROM nginx:latest COPY ./conf/dev.conf /etc/nginx/conf.d/default.conf EXPOSE 80 .../fullstack/reverse-proxy/conf/dev.conf: server { listen 80; location / { proxy_pass http://client:3000; } location /api { proxy_pass http://api; } location /sockjs-node { proxy_pass http://client:3000; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"upgrade\"; } } No need to specify port for api because inside stack (network), not from a host point of view, it's listening on default http port ('80'). Among HTTP standards, by default some 'headers' called 'hop-by-hop' aren't passed by proxy to server and then avoid live reload to work. To fix this, the 'sockjs-node' is a needed technical part to make live reload works. .../fullstack/docker-compose.dev.yml: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 api : build : context : ./api dockerfile : Dockerfile volumes : - type : bind source : ./api/src target : /app/src ports : - 3001:80 db : image : mongo volumes : - type : volume source : dbtest target : /data/db reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.dev ports : - 80:80 depends_on : - api - db volumes : dbtest : Test, everything is supposed to work: docker-compose -f docker-compose.dev.yml up --build In a second terminal: docker-compose -f docker-compose.dev.yml ps NAME COMMAND SERVICE STATUS PORTS fullstack_api_1 \"docker-entrypoint.s\u2026\" api running 0.0.0.0:3001->80/tcp fullstack_client_1 \"docker-entrypoint.s\u2026\" client running 0.0.0.0:3000->3000/tcp fullstack_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp fullstack_reverse-proxy_1 \"/docker-entrypoint.\u2026\" reverse-proxy running 0.0.0.0:80->80/tcp We may observe then the 4 components composing our application are running. And test by browsing to http://localhost/ , note that there's not need to specify port this time thanks to NGINX that do his job as a reverse proxy by distributing requests through our application by listening on default http port ('80'). By refreshing the page, you also may observe counter increasing. To test live reload edit file '.../fullstack/client/src/App.js' and observe live changes in browser window (even without refreshing the page). We now have a full functional development stack that maybe up with only one command: docker-compose -f docker-compose.dev.yml up","title":"Set up the NGINX reverse proxy"},{"location":"docker/10-compose-services/#set-up-production-configuration","text":"Reset Docker environnement (if needed): docker system prune -a docker volume prune","title":"Set up production configuration"},{"location":"docker/10-compose-services/#client","text":"From folder '.../fullstack/client': touch Dockerfile.prod .../fullstack/client/Dockerfile.prod FROM node:alpine as build WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . RUN npm run build FROM nginx:latest COPY --from=build /app/build /usr/share/nginx/html EXPOSE 80 ! Be aware of RUN npm run build, NOT CMD! .../fullstack/docker-compose.prod.yml version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped Start VS Code in '.../fullstack' folder with 'code .' command and then: docker-compose -f docker-compose.prod.yml run -p 80:80 client Browse to http://localhost/ to validate React application is running.","title":"Client"},{"location":"docker/10-compose-services/#api","text":"In '.../fullstack/api/src/index.js' notice mongodb connection URL with MONGO_USERNAME and MONGO_PWD credentials that comes from process (production) environnement: const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; const MongUrl = process . env . NODE_ENV === 'production' ? `mongodb:// ${ process . env . MONGO_USERNAME } : ${ process . env . MONGO_PWD } @db` : `mongodb://db` console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( MongUrl , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/api/count' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . all ( '*' , ( req , res ) => { res . status ( 404 ). end (); }); app . listen ( 80 ); In '.../fullstack/api' folder: touch .env .../fullstack/api/.env: MONGO_USERNAME=paul MONGO_PWD=123 We don't want that secret information to be copied in container: touch .dockerignore .../fullstack/api/.dockerignore: .env .../fullstack/docker-compose.prod.yml version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped Test API with (tanks to \"console.log(process.env)\" in \".../fullstack/api/src/index.js\" we may observe environment variables): docker-compose -f docker-compose.prod.yml run api { . . . MONGO_USERNAME: 'paul', . . MONGO_PWD: '123', . NODE_ENV: 'production', . . } Hit 'Ctrl+c' to stop.","title":"API"},{"location":"docker/10-compose-services/#db","text":"Secure the database by providing root username and password through an environment file. From folder '.../fullstack/db': touch .env Get credential syntax from 'Environment Variables' section in Mongo image on Docker Hub . In file '.../fullstack/db/.env' add credentials: MONGO_INITDB_ROOT_USERNAME=admin MONGO_INITDB_ROOT_PASSWORD=password Provide environment file (for root user credentials) and external volume for db service in docker compose configuration file. Set up db service in '.../fullstack/docker-compose.prod.yml' file: version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped db : image : mongo volumes : - type : volume source : dbprod target : /data/db env_file : - ./db/.env restart : unless-stopped volumes : dbprod : external : true Create volume 'dbprod': docker volume create dbprod Run 'db' container in detached mode from '.../fullstack' folder: docker-compose -f docker-compose.prod.yml run -d db Initialize database by connecting to it with root user credentials. docker container exec -it fullstack_db_run_e9fa9328e2b8 sh mongo use admin db.auth({ user: 'admin', pwd: 'password' }) Create the needed user for API access, the one described in '.env' environnement file that reside in '.../fullstack/api' folder. db.createUser({ user: 'paul', pwd: '123', roles: [{ role: 'readWrite', db: 'test' }] }) Setup collection for 'count'. use test db.count.insertOne({ count: 0 }) exit exit docker stop fullstack_db_run_e9fa9328e2b8 Now db is ready and API may communicate with it. To test, launch complete stack with port 80 open for 'api' service (only for testing, because after, in this production context, this is the reverse proxy role to communicate on port 80). .../fullstack/docker-compose.prod.yml: version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped ports : - 80:80 db : image : mongo volumes : - type : volume source : dbprod target : /data/db env_file : - ./db/.env restart : unless-stopped volumes : dbprod : external : true Launch full stack: docker-compose -f docker-compose.prod.yml up (--build) Browse to localhost/api/count and refresh page to observe counter incrementing. Hit 'Ctrl+c' to stop. Remove port 80 for 'api' service in 'docker-compose.prod.yml' file.","title":"DB"},{"location":"docker/10-compose-services/#reverse-proxy","text":"Production configuration file in '.../fullstack/reverse-proxy/conf' folder: touch prod.conf Production Docker file in '.../fullstack/reverse-proxy' folder: touch Dockerfile.prod .../fullstack/reverse-proxy/Dockerfile.prod: FROM nginx:latest COPY ./conf/prod.conf /etc/nginx/conf.d/default.conf EXPOSE 80 .../fullstack/reverse-proxy/conf/prod.conf: server { listen 80; location / { proxy_pass http://client; } location /api { proxy_pass http://api; } } Add 'reverse-proxy' service in '.../fullstack/docker-compose.prod.yml' file: version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped depends_on : - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db env_file : - ./db/.env restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true Test, from '.../fullstack' folder: docker-compose -f docker-compose.prod.yml down -v docker-compose -f docker-compose.prod.yml up --build Browse to localhost and refresh page to observe counter incrementing.","title":"Reverse proxy"},{"location":"docker/11-compose-production/","text":"Compose - 11 - Production Docker Compose - Production Introduction Almost same components as the ones presented/used in preceding chapter Docker Compose - Services . For this note (Compose - 11 - Production), we refer to folder architecture and contained files from chapter Docker Compose - Services . This time all those component are going to be set to work in production, on a real server. Project components Introduction to project components: Certbot , to get your site on https:// (encrypted communications) with TLS certificate (little lock before address in Internet browser). NGINX (Http request and reverse proxy). React (Application), could be Vue.js or Angular, would be the same. PM2 , process manager (load balancer) for Node.js. Node.js (API), to get better performance it's advised to get one running instance of Node.js by CPU core (on host server). MongoDB (database), only one instance here (scale is out of scope here). OVHcloud (VPS, virtual private server and DNS). GitLab , code hosting. Project architecture Below is a detailed schema of proposed architecture, obviously, everything in dedicated Docker containers: React (build) / / NGINX / / !=/api / --Certbot--NGINX--/ \\ ==/api \\ \\ PM2 \\ --------------- | --------------------- | | | Node.js Node.js Node.js \u2191 \u2191 \u2191 \u2193 \u2193 \u2193 --------------------- | MongoDB | --------------------- Real scalable and performing architecture for single physical server. For huge application running on multiple physical server, see Docker Swarm . SIGINT SIGINT: - A signal sent to a process in order to cause it to be interrupted properly (graceful shutdown). This is a bit off topic, but here we will properly handle the Node.js application (API) interrupt signal. In file '.../fullstack/api/src/index.js': const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; const MongUrl = process . env . NODE_ENV === 'production' ? `mongodb:// ${ process . env . MONGO_USERNAME } : ${ process . env . MONGO_PWD } @db` : `mongodb://db` console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( MongUrl , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/api/count' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . all ( '*' , ( req , res ) => { res . status ( 404 ). end (); }); app . listen ( 80 ); process . addListener ( 'SIGINT' , () => { console . log ( 'Received interruption signal!' ); }) In a terminal: docker-compose -f docker-compose.dev.yml build api docker-compose -f docker-compose.dev.yml run api CTRL + C ^CReceived interruption signal! npm ERR! path /app npm ERR! command failed npm ERR! signal SIGINT npm ERR! command sh -c nodemon ./src/index.js It was just to observe in logs that process correctly receive interruption signal and then there are some red error messages. Below is correct management of process interruption by closing database connection and server that return 1 on exit error. In file '.../fullstack/api/src/index.js': const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let clientDb ; let count ; const MongUrl = process . env . NODE_ENV === 'production' ? `mongodb:// ${ process . env . MONGO_USERNAME } : ${ process . env . MONGO_PWD } @db` : `mongodb://db` console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( MongUrl , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); clientDb = client ; count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/api/count' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . all ( '*' , ( req , res ) => { res . status ( 404 ). end (); }); const server = app . listen ( 80 ); process . addListener ( 'SIGINT' , () => { console . log ( 'Received interruption signal!' ); server . close (( err ) => { if ( err ) { process . exit ( 1 ); } else { if ( clientDb ) { clientDb . close (( err ) => process . exit ( err ? 1 : 0 )); } else { process . exit ( 0 ); } } }) }) In production we also use LTS (Long-term support) version of alpine node image. In '.../fullstack/api/Dockerfile' file: FROM node:lts-alpine WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . EXPOSE 80 CMD [\"npm\", \"start\"] In a terminal (from '.../fullstack' folder), to tests if it's running well. docker-compose -f docker-compose.dev.yml build api docker-compose -f docker-compose.dev.yml run api CTRL + C Still experiment some red error messages in logs after SIGINT received by the process, maybe due to no database connection for now (let's see further on this chapter notes). PM2 One Node.js instance by CPU available threads. Cluster module. In fullstack/api folder, rename Dockerfile as Dockerfile.dev and create new Dockerfile.prod for production: mv Dockerfile Dockerfile.dev touch Dockerfile.prod Edit docker-compose.dev.yml accordingly to use Dockerfile.dev for api service: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 api : build : context : ./api dockerfile : Dockerfile.dev volumes : - type : bind source : ./api/src target : /app/src ports : - 3001:80 db : image : mongo volumes : - type : volume source : dbtest target : /data/db reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.dev ports : - 80:80 depends_on : - api - db volumes : dbtest : Dockerfile.prod: FROM node:lts-alpine WORKDIR /app COPY package.json . RUN npm install RUN npm i pm2 -g # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . EXPOSE 80 CMD [ \"npm\", \"run\", \"prod\" ] package.json: { \"name\" : \"api\" , \"version\" : \"1.0.0\" , \"description\" : \"\" , \"main\" : \"index.js\" , \"scripts\" : { \"start\" : \"nodemon ./src/index.js\" , \"prod\" : \"pm2-runtime ecosystem.config.js\" , \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\" : [], \"author\" : \"\" , \"license\" : \"ISC\" , \"dependencies\" : { \"express\" : \"^4.17.2\" , \"mongodb\" : \"^4.3.0\" , \"nodemon\" : \"^2.0.15\" } } S t ill i n fullsta ck/api f older crea te ecosys te m.co nf ig.js f ile : ```co ns ole t ouch ecosys te m.co nf ig.js ecosystem.config.js: module . exports = [ { script : 'src/index.js' , name : 'api' , exec_mode : 'cluster' , instances : 'max' } ] In fullstack folder, edit docker-compose.prod.yml accordingly (for now, db dependency of api service is commented): version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile.prod env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped #depends_on: # - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db env_file : - ./db/.env restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true JS application is also commented, for now, to avoid issue trying db connection, .../fullstack/api/src/index.js: console . log ( 'Hi, PM2!' ); /* const express = require(\"express\"); const MongoClient = require('mongodb').MongoClient; let clientDb; let count; const MongUrl = process.env.NODE_ENV === 'production' ? `mongodb://${ process.env.MONGO_USERNAME }:${ process.env.MONGO_PWD }@db` : `mongodb://db` console.log(process.env) // to have environnement variables in logs MongoClient.connect(MongUrl, { useUnifiedTopology: true }, (err, client) => { if (err) { console.log(err); } else { console.log('CONNECTION DB OK!'); clientDb = client; count = client.db('test').collection(\"count\"); } }); const app = express(); app.get('/api/count', (req, res) => { console.log('request url: ' + req.url); count.findOneAndUpdate({}, { $inc: { count: 1 } }, { returnNewDocument: true }).then((doc) => { const value = doc.value; res.status(200).json(value.count); }) }); app.all('*', (req, res) => { res.status(404).end(); }); const server = app.listen(80); process.addListener('SIGINT', () => { console.log('Received interruption signal!'); server.close((err) => { if (err) { process.exit(1); } else { if (clientDb) { clientDb.close((err) => process.exit(err ? 1 : 0)); } else { process.exit(0); } } }) }) */ In fullstack folder, build, run production api (maybe dbprod volume is missing, so recreate it): docker volume create dbprod docker-compose -f docker-compose.prod.yml build api docker-compose -f docker-compose.prod.yml run api > api@1.0.0 prod > pm2-runtime ecosystem.config.js 2022-02-11T16:19:52: PM2 log: Launching in no daemon mode 2022-02-11T16:19:52: PM2 log: App [api:0] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:0] online 2022-02-11T16:19:52: PM2 log: App [api:1] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:1] online 2022-02-11T16:19:52: PM2 log: App [api:2] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:2] online 2022-02-11T16:19:52: PM2 log: App [api:3] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:3] online 2022-02-11T16:19:52: PM2 log: App [api:4] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:4] online 2022-02-11T16:19:52: PM2 log: App [api:5] starting in -cluster mode- Hi, PM2! Hi, PM2! 2022-02-11T16:19:52: PM2 log: App [api:5] online 2022-02-11T16:19:52: PM2 log: App [api:6] starting in -cluster mode- Hi, PM2! 2022-02-11T16:19:52: PM2 log: App [api:6] online 2022-02-11T16:19:52: PM2 log: App [api:7] starting in -cluster mode- Hi, PM2! 2022-02-11T16:19:52: PM2 log: App [api:7] online Hi, PM2! Hi, PM2! Hi, PM2! Hi, PM2! Have a look to Quick Start page of PM2 In a second terminal exec a container sh session and list PM2 processes: docker exec -it fullstack_api_run_d97c9b2061d1 sh pm2 ls \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 id \u2502 name \u2502 mode \u2502 \u21ba \u2502 status \u2502 cpu \u2502 memory \u2502 \u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 0 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 44.4mb \u2502 \u2502 1 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 45.0mb \u2502 \u2502 2 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 44.9mb \u2502 \u2502 3 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 44.5mb \u2502 \u2502 4 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 45.0mb \u2502 \u2502 5 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 45.2mb \u2502 \u2502 6 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 45.3mb \u2502 \u2502 7 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 44.7mb \u2502 \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Production Environment Remove references to .env file for api and db service. Values contained in those files (admin and user credentials) will be entered at server start. .../fullstack/docker-compose.prod.yml: version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile.prod environment : - MONGO_USERNAME - MONGO_PWD - NODE_ENV=production restart : unless-stopped #depends_on: # - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true Add a .dockerignore file in .../fullstack/api folder: touch .dockerignore Add in .dockerignore file '.env' entry to not copy it in container. ../fullstack/api/.dockerignore: .env Test api with passing credentials at run from command line, from .../fullstack folder: MONGO_PWD=123 MONGO_USERNAME=paul docker-compose -f docker-compose.prod.yml run api In a second terminal: docker exec -it fullstack_api_run_16a3f881761e sh env NODE_VERSION=16.14.0 HOSTNAME=65742617b66a YARN_VERSION=1.22.17 SHLVL=1 HOME=/root TERM=xterm MONGO_USERNAME=paul PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/app MONGO_PWD=123 NODE_ENV=production Gitlab, Placing the project on Browse to Gitlab and create a new blank project called 'docker-production'. Edit Gitlab user settings to add public SSH Key from local machine, '~/.ssh' folder: cat id_rsa.pub Copy-paste result to \"Add key\" form entry in Gitlab. Back to 'docker-production' project's page, click on clone button and select \"Open in your IDE - Visual Studio Code (SSH)\" and choose a new local folder (e.g. .../docker-production). From new VS Code project (docker-production) root folder, in a terminal, copy all files from '.../fullstack' folder: cp -R ~/.../.../fullstack/* . In .../docker-production folder: touch .gitignore And following entries to .../docker-production/.gitignore file: .env **/node_modules By clicking on 'Source Control' plugin in VS Code, observe that there's no '.env' file in the list. Finalize git folder initialization: git config user.email \"john@doe.com\" git config user.name \"John Doe\" First commit: git add . git commit -m \"first commit\" git push VPS Setup a Virtual Private Server on OVHcloud or on a Raspberry. Connect to remote server through SSH: ssh user@host Create a new user and add it to sudo group and switch to newly created user: adduser jean usermod -aG sudo jean su jean - Back to host machine, set it up to easily connect to remote through SSH: nano ~/.ssh/config config: Host dockerprod Hostname <ip address> User jean Port 22 Add ssh key to remote server to connect to it without the need to enter the password: ssh-copy-id jean@<remote server ip address> Now simply connect to remote server with: ssh dockerprod Secure configuration of SSH on the server Connected on remote server through SSH, edit SSH demon configuration for minimal safety: sudo nano /etc/ssh/sshd_config Edit entries in sshd_config file as follow: PasswordAuthentication no X11Forwarding no PermitRootLogin no Protocol 2 Reload SSH demon: systemctl reload sshd Now the only way to connect on remote server through SSH is with your own computer with the created user. Finally change permission on home folder for more safety: sudo chmod 700 ~ Domain name Order a domain name. In DNS Zone, edit type \"A\" entry with your remote server IP address. TLS Certificate Let's Encrypt is a certification authority that allow to get free X509 certificate for using TLS protocol. They develop ACME protocol to automate certificates management. Certbot is official client from Let's Encrypt using ACME protocol in an automated manner. Certbot Install Connect to remote server through SSH, then: sudo snap install certbot --classic or sudo apt install certbot Create certificate with certbot, \"domain\" should exactly match your root URL (with and/or without \"www\"): sudo certbot certonly -d DOMAIN1 -d DOMAIN2 -d DOMAIN3 Select 'standalone' option, port 80 should be available. Follow instructions to create a letsencrypt account. Now, certificates should be available in folder '/etc/letsencrypt/live/DOMAIN_NAME'. To use TLS certificate edit compose file as follow by opening port 443 for HTTP and HTTP2 for reverse proxy service, docker-compose.prod.yml: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile.prod environment : - MONGO_USERNAME - MONGO_PWD - NODE_ENV=production restart : unless-stopped depends_on : - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 - 443:443 restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true Edit reverse-proxy/conf/prod.conf to use HTTP2 with nginx: server { listen 80; return 301 https://sandbox-dyma.ovh$request_uri; } server { listen 443 ssl http2; ssl_certificate /etc/letsencrypt/live/www.sandbox-dyma.ovh/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/www.sandbox-dyma.ovh/privkey.pem; ssl_trusted_certificate /etc/letsencrypt/live/www.sandbox-dyma.ovh/chain.pem; ssl_protocols TLSv1.2 TLSv1.3; location / { proxy_pass http://client; } location /api { proxy_pass http://api; } } We force the redirection of requests on port 80 (HTTP requests) on port 443 (HTTPS / HTTP2). For the virtual server listening on port 443, we use the certificates created to be able to use HTTP2 (which uses TLS). To allow reverse proxy to access TLS certificate, create a bind mount with modifying docker-compose.prod.yml file as follow: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile.prod environment : - MONGO_USERNAME - MONGO_PWD - NODE_ENV=production restart : unless-stopped depends_on : - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 - 443:443 volumes : - type : bind source : /etc/letsencrypt target : /etc/letsencrypt restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true We simply mount the /etc/letsencrypt folder which contains our letsencrypt certificates. Launch production From local machine. Git add/commit/push all changes: git add * git commit -m \"prod ready\" git push origin Clone from server Connect to your server through SSH. Get link (https) to clone from GitLab repository. Copy paste link to terminal in your server: git clone https://gitlab.com/... Maybe clone PROD project to /root (sudo -i) instead of /home/paul, for cron certbot renew certificate and also ease docker command (no need to sudo each time). Docker installation On Ubuntu server simply type: sudo snap install docker Check installation: sudo docker DB setup On server, create volume: sudo docker volume create dbprod Initialize db service: sudo MONGO_INITDB_ROOT_PASSWORD=password MONGO_INITDB_ROOT_USERNAME=root docker-compose -f docker-compose.prod.yml run -d db Specify user admin root credential, this user is allowed to everything. Connect to MongoDB client in container: sudo docker-compose exec -it db mongo Authenticate in console: use admin db.auth({user: 'root', pwd: 'password'}) Return '1', means it's OK. Create user used to connect from API to DB: db.createUser({user: 'jean', pwd: '123', roles:[{role: 'readWrite', db: 'test'}]}) Initialize collection in test db: use test db.count.insertOne({count: 0}); DB is now ready and we may quit: sudo docker-compose -f docker-compose.prod.yml down Launch application We just have to launch our application: sudo MONGO_USERNAME=jean MONGO_PWD=123 docker-compose -f docker-compose.prod.yml up -d --build We check that everything is well launched: sudo docker-compose -f docker-compose.prod.yml logs -f You can also access the URL in an Internet browser to test the entire application. TLS Certificate, automatic renewal A letsencrypt certificate is valid 90 days. In this chapter we show you how to automate renewal through a cron job. Test renewal command The pre/post hook are there to free port 80, needed by certbot to renew certificate. \"Manual\" with --force-renewal option to check: sudo certbot --standalone renew --force-renewal --pre-hook \"/snap/bin/docker-compose -f ~/docker-production/docker-compose.prod.yml stop reverseproxy\" --post-hook \"/snap/bin/docker-compose -f ~/docker-production/docker-compose.prod.yml restart reverseproxy\" This command should be adapted to your \"/docker-production\" folder and check where is installed Docker Compose with 'which docker-compose'. Add cron task We add cron task: crontab -e Add: 0 0 * * * certbot --standalone renew --pre-hook \"/snap/bin/docker-compose -f ~/docker-production/docker-compose.prod.yml stop reverseproxy\" --post-hook \"/snap/bin/docker-compose -f ~/docker-production/docker-compose.prod.yml restart reverseproxy\" We save and we leave the editor: the certificate will be renewed at the right time by certbot automatically. You will have a downtime of a few seconds one day per month approximately every 3 months. Example code You can also find the project code on Github . Quick Reference PM2 commands PM2 metrics : pm2 ls pm2 monit pm2 monitor // start pm2 unmonitor api // stop MongoDB commands Free online monitoring, from MongoDB console: // enable db.enableFreeMonitoring() // disable db.disableFreeMonitoring() List collection: mongo show dbs use test // db show collections db.count.find()","title":"Compose - 11 - Production"},{"location":"docker/11-compose-production/#compose-11-production","text":"Docker Compose - Production","title":"Compose - 11 - Production"},{"location":"docker/11-compose-production/#introduction","text":"Almost same components as the ones presented/used in preceding chapter Docker Compose - Services . For this note (Compose - 11 - Production), we refer to folder architecture and contained files from chapter Docker Compose - Services . This time all those component are going to be set to work in production, on a real server.","title":"Introduction"},{"location":"docker/11-compose-production/#project-components","text":"Introduction to project components: Certbot , to get your site on https:// (encrypted communications) with TLS certificate (little lock before address in Internet browser). NGINX (Http request and reverse proxy). React (Application), could be Vue.js or Angular, would be the same. PM2 , process manager (load balancer) for Node.js. Node.js (API), to get better performance it's advised to get one running instance of Node.js by CPU core (on host server). MongoDB (database), only one instance here (scale is out of scope here). OVHcloud (VPS, virtual private server and DNS). GitLab , code hosting.","title":"Project components"},{"location":"docker/11-compose-production/#project-architecture","text":"Below is a detailed schema of proposed architecture, obviously, everything in dedicated Docker containers: React (build) / / NGINX / / !=/api / --Certbot--NGINX--/ \\ ==/api \\ \\ PM2 \\ --------------- | --------------------- | | | Node.js Node.js Node.js \u2191 \u2191 \u2191 \u2193 \u2193 \u2193 --------------------- | MongoDB | --------------------- Real scalable and performing architecture for single physical server. For huge application running on multiple physical server, see Docker Swarm .","title":"Project architecture"},{"location":"docker/11-compose-production/#sigint","text":"SIGINT: - A signal sent to a process in order to cause it to be interrupted properly (graceful shutdown). This is a bit off topic, but here we will properly handle the Node.js application (API) interrupt signal. In file '.../fullstack/api/src/index.js': const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; const MongUrl = process . env . NODE_ENV === 'production' ? `mongodb:// ${ process . env . MONGO_USERNAME } : ${ process . env . MONGO_PWD } @db` : `mongodb://db` console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( MongUrl , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/api/count' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . all ( '*' , ( req , res ) => { res . status ( 404 ). end (); }); app . listen ( 80 ); process . addListener ( 'SIGINT' , () => { console . log ( 'Received interruption signal!' ); }) In a terminal: docker-compose -f docker-compose.dev.yml build api docker-compose -f docker-compose.dev.yml run api CTRL + C ^CReceived interruption signal! npm ERR! path /app npm ERR! command failed npm ERR! signal SIGINT npm ERR! command sh -c nodemon ./src/index.js It was just to observe in logs that process correctly receive interruption signal and then there are some red error messages. Below is correct management of process interruption by closing database connection and server that return 1 on exit error. In file '.../fullstack/api/src/index.js': const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let clientDb ; let count ; const MongUrl = process . env . NODE_ENV === 'production' ? `mongodb:// ${ process . env . MONGO_USERNAME } : ${ process . env . MONGO_PWD } @db` : `mongodb://db` console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( MongUrl , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); clientDb = client ; count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/api/count' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . all ( '*' , ( req , res ) => { res . status ( 404 ). end (); }); const server = app . listen ( 80 ); process . addListener ( 'SIGINT' , () => { console . log ( 'Received interruption signal!' ); server . close (( err ) => { if ( err ) { process . exit ( 1 ); } else { if ( clientDb ) { clientDb . close (( err ) => process . exit ( err ? 1 : 0 )); } else { process . exit ( 0 ); } } }) }) In production we also use LTS (Long-term support) version of alpine node image. In '.../fullstack/api/Dockerfile' file: FROM node:lts-alpine WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . EXPOSE 80 CMD [\"npm\", \"start\"] In a terminal (from '.../fullstack' folder), to tests if it's running well. docker-compose -f docker-compose.dev.yml build api docker-compose -f docker-compose.dev.yml run api CTRL + C Still experiment some red error messages in logs after SIGINT received by the process, maybe due to no database connection for now (let's see further on this chapter notes).","title":"SIGINT"},{"location":"docker/11-compose-production/#pm2","text":"One Node.js instance by CPU available threads. Cluster module. In fullstack/api folder, rename Dockerfile as Dockerfile.dev and create new Dockerfile.prod for production: mv Dockerfile Dockerfile.dev touch Dockerfile.prod Edit docker-compose.dev.yml accordingly to use Dockerfile.dev for api service: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.dev volumes : - type : bind source : ./client target : /app - type : volume target : /app/node_modules ports : - 3000:3000 api : build : context : ./api dockerfile : Dockerfile.dev volumes : - type : bind source : ./api/src target : /app/src ports : - 3001:80 db : image : mongo volumes : - type : volume source : dbtest target : /data/db reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.dev ports : - 80:80 depends_on : - api - db volumes : dbtest : Dockerfile.prod: FROM node:lts-alpine WORKDIR /app COPY package.json . RUN npm install RUN npm i pm2 -g # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . EXPOSE 80 CMD [ \"npm\", \"run\", \"prod\" ] package.json: { \"name\" : \"api\" , \"version\" : \"1.0.0\" , \"description\" : \"\" , \"main\" : \"index.js\" , \"scripts\" : { \"start\" : \"nodemon ./src/index.js\" , \"prod\" : \"pm2-runtime ecosystem.config.js\" , \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\" : [], \"author\" : \"\" , \"license\" : \"ISC\" , \"dependencies\" : { \"express\" : \"^4.17.2\" , \"mongodb\" : \"^4.3.0\" , \"nodemon\" : \"^2.0.15\" } } S t ill i n fullsta ck/api f older crea te ecosys te m.co nf ig.js f ile : ```co ns ole t ouch ecosys te m.co nf ig.js ecosystem.config.js: module . exports = [ { script : 'src/index.js' , name : 'api' , exec_mode : 'cluster' , instances : 'max' } ] In fullstack folder, edit docker-compose.prod.yml accordingly (for now, db dependency of api service is commented): version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile.prod env_file : - ./api/.env environment : NODE_ENV : production restart : unless-stopped #depends_on: # - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db env_file : - ./db/.env restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true JS application is also commented, for now, to avoid issue trying db connection, .../fullstack/api/src/index.js: console . log ( 'Hi, PM2!' ); /* const express = require(\"express\"); const MongoClient = require('mongodb').MongoClient; let clientDb; let count; const MongUrl = process.env.NODE_ENV === 'production' ? `mongodb://${ process.env.MONGO_USERNAME }:${ process.env.MONGO_PWD }@db` : `mongodb://db` console.log(process.env) // to have environnement variables in logs MongoClient.connect(MongUrl, { useUnifiedTopology: true }, (err, client) => { if (err) { console.log(err); } else { console.log('CONNECTION DB OK!'); clientDb = client; count = client.db('test').collection(\"count\"); } }); const app = express(); app.get('/api/count', (req, res) => { console.log('request url: ' + req.url); count.findOneAndUpdate({}, { $inc: { count: 1 } }, { returnNewDocument: true }).then((doc) => { const value = doc.value; res.status(200).json(value.count); }) }); app.all('*', (req, res) => { res.status(404).end(); }); const server = app.listen(80); process.addListener('SIGINT', () => { console.log('Received interruption signal!'); server.close((err) => { if (err) { process.exit(1); } else { if (clientDb) { clientDb.close((err) => process.exit(err ? 1 : 0)); } else { process.exit(0); } } }) }) */ In fullstack folder, build, run production api (maybe dbprod volume is missing, so recreate it): docker volume create dbprod docker-compose -f docker-compose.prod.yml build api docker-compose -f docker-compose.prod.yml run api > api@1.0.0 prod > pm2-runtime ecosystem.config.js 2022-02-11T16:19:52: PM2 log: Launching in no daemon mode 2022-02-11T16:19:52: PM2 log: App [api:0] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:0] online 2022-02-11T16:19:52: PM2 log: App [api:1] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:1] online 2022-02-11T16:19:52: PM2 log: App [api:2] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:2] online 2022-02-11T16:19:52: PM2 log: App [api:3] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:3] online 2022-02-11T16:19:52: PM2 log: App [api:4] starting in -cluster mode- 2022-02-11T16:19:52: PM2 log: App [api:4] online 2022-02-11T16:19:52: PM2 log: App [api:5] starting in -cluster mode- Hi, PM2! Hi, PM2! 2022-02-11T16:19:52: PM2 log: App [api:5] online 2022-02-11T16:19:52: PM2 log: App [api:6] starting in -cluster mode- Hi, PM2! 2022-02-11T16:19:52: PM2 log: App [api:6] online 2022-02-11T16:19:52: PM2 log: App [api:7] starting in -cluster mode- Hi, PM2! 2022-02-11T16:19:52: PM2 log: App [api:7] online Hi, PM2! Hi, PM2! Hi, PM2! Hi, PM2! Have a look to Quick Start page of PM2 In a second terminal exec a container sh session and list PM2 processes: docker exec -it fullstack_api_run_d97c9b2061d1 sh pm2 ls \u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 id \u2502 name \u2502 mode \u2502 \u21ba \u2502 status \u2502 cpu \u2502 memory \u2502 \u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502 0 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 44.4mb \u2502 \u2502 1 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 45.0mb \u2502 \u2502 2 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 44.9mb \u2502 \u2502 3 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 44.5mb \u2502 \u2502 4 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 45.0mb \u2502 \u2502 5 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 45.2mb \u2502 \u2502 6 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 45.3mb \u2502 \u2502 7 \u2502 api \u2502 cluster \u2502 0 \u2502 online \u2502 0% \u2502 44.7mb \u2502 \u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"PM2"},{"location":"docker/11-compose-production/#production-environment","text":"Remove references to .env file for api and db service. Values contained in those files (admin and user credentials) will be entered at server start. .../fullstack/docker-compose.prod.yml: version : '3.8' services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile.prod environment : - MONGO_USERNAME - MONGO_PWD - NODE_ENV=production restart : unless-stopped #depends_on: # - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true Add a .dockerignore file in .../fullstack/api folder: touch .dockerignore Add in .dockerignore file '.env' entry to not copy it in container. ../fullstack/api/.dockerignore: .env Test api with passing credentials at run from command line, from .../fullstack folder: MONGO_PWD=123 MONGO_USERNAME=paul docker-compose -f docker-compose.prod.yml run api In a second terminal: docker exec -it fullstack_api_run_16a3f881761e sh env NODE_VERSION=16.14.0 HOSTNAME=65742617b66a YARN_VERSION=1.22.17 SHLVL=1 HOME=/root TERM=xterm MONGO_USERNAME=paul PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/app MONGO_PWD=123 NODE_ENV=production","title":"Production Environment"},{"location":"docker/11-compose-production/#gitlab-placing-the-project-on","text":"Browse to Gitlab and create a new blank project called 'docker-production'. Edit Gitlab user settings to add public SSH Key from local machine, '~/.ssh' folder: cat id_rsa.pub Copy-paste result to \"Add key\" form entry in Gitlab. Back to 'docker-production' project's page, click on clone button and select \"Open in your IDE - Visual Studio Code (SSH)\" and choose a new local folder (e.g. .../docker-production). From new VS Code project (docker-production) root folder, in a terminal, copy all files from '.../fullstack' folder: cp -R ~/.../.../fullstack/* . In .../docker-production folder: touch .gitignore And following entries to .../docker-production/.gitignore file: .env **/node_modules By clicking on 'Source Control' plugin in VS Code, observe that there's no '.env' file in the list. Finalize git folder initialization: git config user.email \"john@doe.com\" git config user.name \"John Doe\" First commit: git add . git commit -m \"first commit\" git push","title":"Gitlab, Placing the project on"},{"location":"docker/11-compose-production/#vps","text":"Setup a Virtual Private Server on OVHcloud or on a Raspberry. Connect to remote server through SSH: ssh user@host Create a new user and add it to sudo group and switch to newly created user: adduser jean usermod -aG sudo jean su jean - Back to host machine, set it up to easily connect to remote through SSH: nano ~/.ssh/config config: Host dockerprod Hostname <ip address> User jean Port 22 Add ssh key to remote server to connect to it without the need to enter the password: ssh-copy-id jean@<remote server ip address> Now simply connect to remote server with: ssh dockerprod","title":"VPS"},{"location":"docker/11-compose-production/#secure-configuration-of-ssh-on-the-server","text":"Connected on remote server through SSH, edit SSH demon configuration for minimal safety: sudo nano /etc/ssh/sshd_config Edit entries in sshd_config file as follow: PasswordAuthentication no X11Forwarding no PermitRootLogin no Protocol 2 Reload SSH demon: systemctl reload sshd Now the only way to connect on remote server through SSH is with your own computer with the created user. Finally change permission on home folder for more safety: sudo chmod 700 ~","title":"Secure configuration of SSH on the server"},{"location":"docker/11-compose-production/#domain-name","text":"Order a domain name. In DNS Zone, edit type \"A\" entry with your remote server IP address.","title":"Domain name"},{"location":"docker/11-compose-production/#tls-certificate","text":"Let's Encrypt is a certification authority that allow to get free X509 certificate for using TLS protocol. They develop ACME protocol to automate certificates management. Certbot is official client from Let's Encrypt using ACME protocol in an automated manner.","title":"TLS Certificate"},{"location":"docker/11-compose-production/#certbot-install","text":"Connect to remote server through SSH, then: sudo snap install certbot --classic or sudo apt install certbot Create certificate with certbot, \"domain\" should exactly match your root URL (with and/or without \"www\"): sudo certbot certonly -d DOMAIN1 -d DOMAIN2 -d DOMAIN3 Select 'standalone' option, port 80 should be available. Follow instructions to create a letsencrypt account. Now, certificates should be available in folder '/etc/letsencrypt/live/DOMAIN_NAME'. To use TLS certificate edit compose file as follow by opening port 443 for HTTP and HTTP2 for reverse proxy service, docker-compose.prod.yml: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile.prod environment : - MONGO_USERNAME - MONGO_PWD - NODE_ENV=production restart : unless-stopped depends_on : - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 - 443:443 restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true Edit reverse-proxy/conf/prod.conf to use HTTP2 with nginx: server { listen 80; return 301 https://sandbox-dyma.ovh$request_uri; } server { listen 443 ssl http2; ssl_certificate /etc/letsencrypt/live/www.sandbox-dyma.ovh/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/www.sandbox-dyma.ovh/privkey.pem; ssl_trusted_certificate /etc/letsencrypt/live/www.sandbox-dyma.ovh/chain.pem; ssl_protocols TLSv1.2 TLSv1.3; location / { proxy_pass http://client; } location /api { proxy_pass http://api; } } We force the redirection of requests on port 80 (HTTP requests) on port 443 (HTTPS / HTTP2). For the virtual server listening on port 443, we use the certificates created to be able to use HTTP2 (which uses TLS). To allow reverse proxy to access TLS certificate, create a bind mount with modifying docker-compose.prod.yml file as follow: version : \"3.8\" services : client : build : context : ./client dockerfile : Dockerfile.prod restart : unless-stopped api : build : context : ./api dockerfile : Dockerfile.prod environment : - MONGO_USERNAME - MONGO_PWD - NODE_ENV=production restart : unless-stopped depends_on : - db db : image : mongo volumes : - type : volume source : dbprod target : /data/db environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD restart : unless-stopped reverse-proxy : build : context : ./reverse-proxy dockerfile : Dockerfile.prod ports : - 80:80 - 443:443 volumes : - type : bind source : /etc/letsencrypt target : /etc/letsencrypt restart : unless-stopped depends_on : - api - db - client volumes : dbprod : external : true We simply mount the /etc/letsencrypt folder which contains our letsencrypt certificates.","title":"Certbot Install"},{"location":"docker/11-compose-production/#launch-production","text":"From local machine. Git add/commit/push all changes: git add * git commit -m \"prod ready\" git push origin","title":"Launch production"},{"location":"docker/11-compose-production/#clone-from-server","text":"Connect to your server through SSH. Get link (https) to clone from GitLab repository. Copy paste link to terminal in your server: git clone https://gitlab.com/... Maybe clone PROD project to /root (sudo -i) instead of /home/paul, for cron certbot renew certificate and also ease docker command (no need to sudo each time).","title":"Clone from server"},{"location":"docker/11-compose-production/#docker-installation","text":"On Ubuntu server simply type: sudo snap install docker Check installation: sudo docker","title":"Docker installation"},{"location":"docker/11-compose-production/#db-setup","text":"On server, create volume: sudo docker volume create dbprod Initialize db service: sudo MONGO_INITDB_ROOT_PASSWORD=password MONGO_INITDB_ROOT_USERNAME=root docker-compose -f docker-compose.prod.yml run -d db Specify user admin root credential, this user is allowed to everything. Connect to MongoDB client in container: sudo docker-compose exec -it db mongo Authenticate in console: use admin db.auth({user: 'root', pwd: 'password'}) Return '1', means it's OK. Create user used to connect from API to DB: db.createUser({user: 'jean', pwd: '123', roles:[{role: 'readWrite', db: 'test'}]}) Initialize collection in test db: use test db.count.insertOne({count: 0}); DB is now ready and we may quit: sudo docker-compose -f docker-compose.prod.yml down","title":"DB setup"},{"location":"docker/11-compose-production/#launch-application","text":"We just have to launch our application: sudo MONGO_USERNAME=jean MONGO_PWD=123 docker-compose -f docker-compose.prod.yml up -d --build We check that everything is well launched: sudo docker-compose -f docker-compose.prod.yml logs -f You can also access the URL in an Internet browser to test the entire application.","title":"Launch application"},{"location":"docker/11-compose-production/#tls-certificate-automatic-renewal","text":"A letsencrypt certificate is valid 90 days. In this chapter we show you how to automate renewal through a cron job.","title":"TLS Certificate, automatic renewal"},{"location":"docker/11-compose-production/#test-renewal-command","text":"The pre/post hook are there to free port 80, needed by certbot to renew certificate. \"Manual\" with --force-renewal option to check: sudo certbot --standalone renew --force-renewal --pre-hook \"/snap/bin/docker-compose -f ~/docker-production/docker-compose.prod.yml stop reverseproxy\" --post-hook \"/snap/bin/docker-compose -f ~/docker-production/docker-compose.prod.yml restart reverseproxy\" This command should be adapted to your \"/docker-production\" folder and check where is installed Docker Compose with 'which docker-compose'.","title":"Test renewal command"},{"location":"docker/11-compose-production/#add-cron-task","text":"We add cron task: crontab -e Add: 0 0 * * * certbot --standalone renew --pre-hook \"/snap/bin/docker-compose -f ~/docker-production/docker-compose.prod.yml stop reverseproxy\" --post-hook \"/snap/bin/docker-compose -f ~/docker-production/docker-compose.prod.yml restart reverseproxy\" We save and we leave the editor: the certificate will be renewed at the right time by certbot automatically. You will have a downtime of a few seconds one day per month approximately every 3 months.","title":"Add cron task"},{"location":"docker/11-compose-production/#example-code","text":"You can also find the project code on Github .","title":"Example code"},{"location":"docker/11-compose-production/#quick-reference","text":"","title":"Quick Reference"},{"location":"docker/11-compose-production/#pm2-commands","text":"PM2 metrics : pm2 ls pm2 monit pm2 monitor // start pm2 unmonitor api // stop","title":"PM2 commands"},{"location":"docker/11-compose-production/#mongodb-commands","text":"Free online monitoring, from MongoDB console: // enable db.enableFreeMonitoring() // disable db.disableFreeMonitoring() List collection: mongo show dbs use test // db show collections db.count.find()","title":"MongoDB commands"},{"location":"docker/2-docker-basis/","text":"Docker - 02 - Basis Node Command to launch node with app.js in folder app CMD [\"node\", \"app/app.js\"] Then from terminal, build command with -t argument to name:tag image . to mention path of Dockerfile in current directory: $ docker build -t node-test-001:latest . To check: $ docker images (or $ docker image ls) To test: $ docker run node-test-001 Hello, world! To auto delete container after execution, use \u2013rm option: $ docker run --rm node-test-001 Hello, world! Check if node is installed on host machine: $ node --version v14.16.1 Test app.js on host machine (if node is installed): $ node app.js Hello, world! VS Code VS Code install Microsoft Docker extension Docker file in short My image -> Docker file =: Base image Modification Action Add Alpine package Add/del Alpine package A base image (e.g. Alpine) is not based on any other image. Add or del package in Alpine $c apk update $c apk add grep $c apk del grep Running container Running container as a running process To demonstrate running container is just a running process on host machine $ docker run -d redis In contrary of a virtual machine (VM) a container is \"just\" a running process sharing Linux kernel on host machine. SRC: - https://stackoverflow.com/questions/64787125/why-doesnt-htop-show-my-docker-processes-using-wsl2 To see running process on WSL use command prompt (would be \"$ sudo ps -ef | grep redis\" on a Linux machine): C:\\> wsl -d docker-desktop top (or C:\\> wsl -d docker-desktop ps -ef) If you want htop, you need to install it first: C:\\> wsl -d docker-desktop apk update C:\\> wsl -d docker-desktop apk add htop ... 0% redis-server *:6379 To kill a process on host machine, WSL: C:\\> wsl -d docker-desktop killall redis-server To kill a process on host machine, Linux: $ sudo killall redis-server Running: C:\\> wsl -d docker-desktop htop See that container isn't running anymore: $ docker container ls Disk usage Show docker disk usage $ docker system df Show detailed information on space usage, -v, --verbose docker system df -v Consumed resource to see live consuming resources of running containers: $ docker container stats Inspect to inspect all configuration of a container $ docker container inspect alpinetest001 Running process Show running process in a container from host $ docker container top alpinetest001 See the difference from inside the container $ docker attach alpine001 1. update $c apk update 2. add bash $c apk add bash 3. test bash $c bash $c echo $0 bash ctrl+p+q $ docker exec -it alpinetest001 bash if ps not present, install it with: $c apk update && apk add procps then: $c ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 16:15 pts/0 00:00:00 /bin/sh root 16 1 0 16:27 pts/0 00:00:00 bash root 17 0 0 16:28 pts/1 00:00:00 bash root 31 17 0 16:33 pts/1 00:00:00 ps -ef sh and bash does not have parent process ID (PPID) = '0' because of container isolation, container does not see running processes of host machine neither the ones belonging to other containers. Modified file Show modified file in a container: A = added, D = deleted, C = modified $ docker container diff alpinetest001 A /test A /test/test1.txt A /test/test2.txt C /root A /root/.ash_history Copy file copy file from host to container, docker cp path container:path docker cp test1.txt alpinetest001:test copy file from container to host, docker cp container:path path docker cp alpinetest001:test/test2.txt . Execute command in container Execute a command in a container without using terminal $ docker container exec alpine001 mkdir testdir $ docker container exec alpine001 touch /testdir/hello.txt other e.g. $ docker run -d --name redis001 redis $ docker exec -it redis001 redis-cli redis command: set cle 42 get cle exit Get shell in container Get a shell, in a no matter which running container it is: $ docker exec -it redis001 bash $c echo $0 bash if bash not installed in container (e.g. with alpine) you may use sh instead: (if both presents, both works (e.g. redis) $ docker exec -it redis001 sh $c echo $0 sh $ docker run -it -d --name alpine001 alpine $ docker exec -it alpine001 bash OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \"bash\": executable file not found in $PATH: unknown $ docker exec -it alpine001 sh $c echo $0 sh Pause/unpause a container $ docker container start -ai alpine001 $ docker container pause alpine001 $ docker container unpause alpine001 Rename a container rename a container named \"beautiful_leakey\" $ docker container rename beautiful_leakey alpine001 not allowed to rename image Postgres with environnement variable $ docker run -d --name mongo mongo $ docker run -d --name redis redis $ docker run -d --name postgres postgres $ docker logs postgres Error: ... $ docker container rm postgres $ docker container run --name postgres -d -e POSTGRES_HOST_AUTH_METHOD=trust postgres Stop container Stop all running container at once docker stop $(docker ps -aq) Suppress all !Suppress all not used (stopped container(s) and not used for the rest)! $ docker system prune -a Remove image $ docker image rm NAME_OR_ID remove unused images (dangling = image is not tagged and no other image depends on it) $ docker image prune -a Remove container try to remove a running container Launch a background test named redis container with: $ docker run --name test -d redis then try to remove it with: $ docker container rm test Error - Stop the container before attempting removal or force remove Force remove running container: $ docker container rm -f test $ docker run --name test1 -d redis $ docker run --name test2 -d redis $ docker run --name test3 -d redis $ docker container rm -f test1 test2 test3 remove all stopped container $ docker container prune Image, images $ docker images $ docker image ls redis Help to get help, simply type: $ docker help on a command: $ docker ps --help Redis $ docker run redis $ docker run -d redis Alpine $ docker run alpine -i, interactive mode $ docker run -i alpine $c ls . . dev etc home . . . exit -t, terminal -> prompt $ docker run -it alpine (= docker run -it alpine sh) $c echo $0 // check which shell (/bin/sh) $c apk update $c apk add bash $c bash $c echo $0 // check which shell (bash) start in foreground mode $ docker run alpine ping google.ch $ docker run alpine echo hello start in background mode (-d = detach (!= daemon)) $ docker run -d alpine ping google.fr 3593... docker logs 3593... $ docker logs 8e86... --follow Available image show available image(s) $ docker images None running container check none running container with (-a show all containers (default shows just running)): $ docker container ls -a Ubuntu $ docker run -it ubuntu bash $c cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"20.04.3 LTS (Focal Fossa)\" ... to exit container and stop it $c ctrl + d then to start/stop it again, e.g container name is 'trusting_yalow' $ docker start trusting_yalow $ docker stop trusting_yalow then to bash into it $ docker attach trusting_yalow to detach from a docker container without stopping it $c ctrl + p + q Hello, world! $ docker run hello-world Info $ docker info","title":"Docker - 02 - Basis"},{"location":"docker/2-docker-basis/#docker-02-basis","text":"","title":"Docker - 02 - Basis"},{"location":"docker/2-docker-basis/#node","text":"Command to launch node with app.js in folder app CMD [\"node\", \"app/app.js\"] Then from terminal, build command with -t argument to name:tag image . to mention path of Dockerfile in current directory: $ docker build -t node-test-001:latest . To check: $ docker images (or $ docker image ls) To test: $ docker run node-test-001 Hello, world! To auto delete container after execution, use \u2013rm option: $ docker run --rm node-test-001 Hello, world! Check if node is installed on host machine: $ node --version v14.16.1 Test app.js on host machine (if node is installed): $ node app.js Hello, world!","title":"Node"},{"location":"docker/2-docker-basis/#vs-code","text":"VS Code install Microsoft Docker extension","title":"VS Code"},{"location":"docker/2-docker-basis/#docker-file-in-short","text":"My image -> Docker file =: Base image Modification Action","title":"Docker file in short"},{"location":"docker/2-docker-basis/#add-alpine-package","text":"Add/del Alpine package A base image (e.g. Alpine) is not based on any other image. Add or del package in Alpine $c apk update $c apk add grep $c apk del grep","title":"Add Alpine package"},{"location":"docker/2-docker-basis/#running-container","text":"Running container as a running process To demonstrate running container is just a running process on host machine $ docker run -d redis In contrary of a virtual machine (VM) a container is \"just\" a running process sharing Linux kernel on host machine. SRC: - https://stackoverflow.com/questions/64787125/why-doesnt-htop-show-my-docker-processes-using-wsl2 To see running process on WSL use command prompt (would be \"$ sudo ps -ef | grep redis\" on a Linux machine): C:\\> wsl -d docker-desktop top (or C:\\> wsl -d docker-desktop ps -ef) If you want htop, you need to install it first: C:\\> wsl -d docker-desktop apk update C:\\> wsl -d docker-desktop apk add htop ... 0% redis-server *:6379 To kill a process on host machine, WSL: C:\\> wsl -d docker-desktop killall redis-server To kill a process on host machine, Linux: $ sudo killall redis-server Running: C:\\> wsl -d docker-desktop htop See that container isn't running anymore: $ docker container ls","title":"Running container"},{"location":"docker/2-docker-basis/#disk-usage","text":"Show docker disk usage $ docker system df Show detailed information on space usage, -v, --verbose docker system df -v","title":"Disk usage"},{"location":"docker/2-docker-basis/#consumed-resource","text":"to see live consuming resources of running containers: $ docker container stats","title":"Consumed resource"},{"location":"docker/2-docker-basis/#inspect","text":"to inspect all configuration of a container $ docker container inspect alpinetest001","title":"Inspect"},{"location":"docker/2-docker-basis/#running-process","text":"Show running process in a container from host $ docker container top alpinetest001 See the difference from inside the container $ docker attach alpine001 1. update $c apk update 2. add bash $c apk add bash 3. test bash $c bash $c echo $0 bash ctrl+p+q $ docker exec -it alpinetest001 bash if ps not present, install it with: $c apk update && apk add procps then: $c ps -ef UID PID PPID C STIME TTY TIME CMD root 1 0 0 16:15 pts/0 00:00:00 /bin/sh root 16 1 0 16:27 pts/0 00:00:00 bash root 17 0 0 16:28 pts/1 00:00:00 bash root 31 17 0 16:33 pts/1 00:00:00 ps -ef sh and bash does not have parent process ID (PPID) = '0' because of container isolation, container does not see running processes of host machine neither the ones belonging to other containers.","title":"Running process"},{"location":"docker/2-docker-basis/#modified-file","text":"Show modified file in a container: A = added, D = deleted, C = modified $ docker container diff alpinetest001 A /test A /test/test1.txt A /test/test2.txt C /root A /root/.ash_history","title":"Modified file"},{"location":"docker/2-docker-basis/#copy-file","text":"copy file from host to container, docker cp path container:path docker cp test1.txt alpinetest001:test copy file from container to host, docker cp container:path path docker cp alpinetest001:test/test2.txt .","title":"Copy file"},{"location":"docker/2-docker-basis/#execute-command-in-container","text":"Execute a command in a container without using terminal $ docker container exec alpine001 mkdir testdir $ docker container exec alpine001 touch /testdir/hello.txt other e.g. $ docker run -d --name redis001 redis $ docker exec -it redis001 redis-cli redis command: set cle 42 get cle exit","title":"Execute command in container"},{"location":"docker/2-docker-basis/#get-shell-in-container","text":"Get a shell, in a no matter which running container it is: $ docker exec -it redis001 bash $c echo $0 bash if bash not installed in container (e.g. with alpine) you may use sh instead: (if both presents, both works (e.g. redis) $ docker exec -it redis001 sh $c echo $0 sh $ docker run -it -d --name alpine001 alpine $ docker exec -it alpine001 bash OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: \"bash\": executable file not found in $PATH: unknown $ docker exec -it alpine001 sh $c echo $0 sh","title":"Get shell in container"},{"location":"docker/2-docker-basis/#pauseunpause-a-container","text":"$ docker container start -ai alpine001 $ docker container pause alpine001 $ docker container unpause alpine001","title":"Pause/unpause a container"},{"location":"docker/2-docker-basis/#rename-a-container","text":"rename a container named \"beautiful_leakey\" $ docker container rename beautiful_leakey alpine001 not allowed to rename image","title":"Rename a container"},{"location":"docker/2-docker-basis/#postgres-with-environnement-variable","text":"$ docker run -d --name mongo mongo $ docker run -d --name redis redis $ docker run -d --name postgres postgres $ docker logs postgres Error: ... $ docker container rm postgres $ docker container run --name postgres -d -e POSTGRES_HOST_AUTH_METHOD=trust postgres","title":"Postgres with environnement variable"},{"location":"docker/2-docker-basis/#stop-container","text":"Stop all running container at once docker stop $(docker ps -aq)","title":"Stop container"},{"location":"docker/2-docker-basis/#suppress-all","text":"!Suppress all not used (stopped container(s) and not used for the rest)! $ docker system prune -a","title":"Suppress all"},{"location":"docker/2-docker-basis/#remove-image","text":"$ docker image rm NAME_OR_ID remove unused images (dangling = image is not tagged and no other image depends on it) $ docker image prune -a","title":"Remove image"},{"location":"docker/2-docker-basis/#remove-container","text":"try to remove a running container Launch a background test named redis container with: $ docker run --name test -d redis then try to remove it with: $ docker container rm test Error - Stop the container before attempting removal or force remove Force remove running container: $ docker container rm -f test $ docker run --name test1 -d redis $ docker run --name test2 -d redis $ docker run --name test3 -d redis $ docker container rm -f test1 test2 test3 remove all stopped container $ docker container prune","title":"Remove container"},{"location":"docker/2-docker-basis/#image-images","text":"$ docker images $ docker image ls redis","title":"Image, images"},{"location":"docker/2-docker-basis/#help","text":"to get help, simply type: $ docker help on a command: $ docker ps --help","title":"Help"},{"location":"docker/2-docker-basis/#redis","text":"$ docker run redis $ docker run -d redis","title":"Redis"},{"location":"docker/2-docker-basis/#alpine","text":"$ docker run alpine -i, interactive mode $ docker run -i alpine $c ls . . dev etc home . . . exit -t, terminal -> prompt $ docker run -it alpine (= docker run -it alpine sh) $c echo $0 // check which shell (/bin/sh) $c apk update $c apk add bash $c bash $c echo $0 // check which shell (bash) start in foreground mode $ docker run alpine ping google.ch $ docker run alpine echo hello start in background mode (-d = detach (!= daemon)) $ docker run -d alpine ping google.fr 3593... docker logs 3593... $ docker logs 8e86... --follow","title":"Alpine"},{"location":"docker/2-docker-basis/#available-image","text":"show available image(s) $ docker images","title":"Available image"},{"location":"docker/2-docker-basis/#none-running-container","text":"check none running container with (-a show all containers (default shows just running)): $ docker container ls -a","title":"None running container"},{"location":"docker/2-docker-basis/#ubuntu","text":"$ docker run -it ubuntu bash $c cat /etc/os-release NAME=\"Ubuntu\" VERSION=\"20.04.3 LTS (Focal Fossa)\" ... to exit container and stop it $c ctrl + d then to start/stop it again, e.g container name is 'trusting_yalow' $ docker start trusting_yalow $ docker stop trusting_yalow then to bash into it $ docker attach trusting_yalow to detach from a docker container without stopping it $c ctrl + p + q","title":"Ubuntu"},{"location":"docker/2-docker-basis/#hello-world","text":"$ docker run hello-world","title":"Hello, world!"},{"location":"docker/2-docker-basis/#info","text":"$ docker info","title":"Info"},{"location":"docker/3-docker-dockerfile/","text":"Docker - 03 - Docker File Image Variants $ docker pull node:slim node:latest = 992MB VS node:slim = 242MB tag tag to tag image after build $ docker image tag mynode:latest mynode:1.0 history Show the history of an image $ docker history mynode:latest IMAGE CREATED CREATED BY SIZE COMMENT 1a8f184ef896 8 minutes ago sh 42.4MB 14119a10abf4 2 months ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 2 months ago /bin/sh -c #(nop) ADD file:aad4290d27580cc1a\u2026 5.6MB logs $ docker container logs redis-4.0-001 -f to follow -t timestamp COMMIT Snapshot a container to image $ docker run -it alpine sh # mkdir app open a new terminal to copy app.js \"manually\" to app folder $ docker container cp app.js ed6:/app/ then from inside container install node.js $c apk add --update nodejs then quit container and from host terminal $ docker container commit -c 'CMD [\"node\", \"/app/app.js\"]' ed6 mynode $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE mynode latest 1a8f184ef896 6 seconds ago 48MB .. $ docker run --rm mynode hello node-test-010 LABEL LABEL to add meta information LABEL MAINTAINER=oldu73@gmail.com LABEL version=1.0 $ docker build -t node10 . $ docker image inspect node10:latest | less /oldu .. \"MAINTAINER\": \"oldu73@gmail.com\", \"version\": \"1.0\" } .. ENV key value Usable in container, available as environment variable: ENV environment=production $ docker build -t node10 . $ docker run -it node10 sh /app $c env HOSTNAME=ac68969910b7 SHLVL=1 HOME=/root environment=production TERM=xterm PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/app ARG Argument available at build time only: ARG folder ARG file then WORKDIR $folder COPY $file . then $ docker build --build-arg folder=/app --build-arg file=app.js -t node-test-008 . then if you try to retriev ARGs by typing env inside the container you do not retrieve it because they are available only at build time: $ docker run --rm -it node-test-008 sh /app # env HOSTNAME=22cc31c49889 SHLVL=1 HOME=/root TERM=xterm PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/app You may also put a default value: ARG folder=/app then $ docker build --build-arg file=app.js -t node-test-008 . Override entry point You can still override the entry point with the --entrypoint option: $ docker run --rm --entrypoint=\"echo\" node:test \"Hi, earth ;)\" or $ docker run -it --entrypoint=\"/bin/sh\" node:test Docker default entry point By default, Docker has a default entry point which is \"/bin/sh -c\" but does not have a default command. $ man sh -> /-c -c Read commands from the command_string operand instead of from the standard input. Special parameter 0 will be set from the command_name operand and the positional parameters ($1, $2, etc.) set from the remaining argument operands. ENTRYPOINT and CMD May have ENTRYPOINT and CMD: ENTRYPOINT [\"echo\"] CMD [\"hello\"] $ docker run --rm node:test hello then you may override hello in run parameter: $ docker run --rm node:test world world exec form [] exec form -> recommended .. shell form, like you would type the command in a terminal ENTRYPOINT instead CMD ENTRYPOINT instead CMD avoid availability for end user to replace Dockerfile CMD by typing one at the end of run terminal command: ENTRYPOINT [\"node\", \"app.js\"] in Dockerfile, then: $ docker run --rm node:test Bonjour or $ docker run --rm node:test echo test Bonjour same same ;-), echo test at the end is not taking under consideration Command at the end of run Typing a command at the end of the run command replace the one in Dockerfile: $ docker run --rm node:test echo test test even if \"CMD [\"node\", \"app.js\"]\" in Dockerfile Remove dangling images Remove dangling ( ) images: $ docker image prune For removing dangling and ununsed images: $ docker image prune -a Docker build no output Docker build not showing any output from commands(Dockerfile RUN): Dockerfile .. RUN echo hello Don't show anything in console at build. Use legacy mode by adding 'DOCKER_BUILDKIT=0' in front of docker build: $ DOCKER_BUILDKIT=0 docker build -t test:latest . .. Step 2/2 : RUN echo hello ---> Running in 3d9c96daa522 hello or (new fashion) with \"--progress=plain --no-cache\" after build command: $ docker build --progress=plain --no-cache -t node-test-007:latest . [3/5] RUN echo \"Hello, world!\" sha256:54040767d950b92027e2e377a0938fd42b89a34fa5d76e3ce281deacda0f1959 0.281 Hello, world! DONE 0.3s List only container names To list only names of all containers: $ docker ps -a --format='{{.Names}}' ENV 'ENV', environment variable: Dockerfile: - Base image - Test environment variable FROM alpine ENV DIR=/app WORKDIR ${DIR}/back then.. $ docker build -t node-test-006:latest . then.. $ docker run -it node-test-006 sh $c pwd /app/back RUN RUN exist in 'exec' and 'shell' mode (which is 'sh' by default). exec: RUN [\"/bin/bash\", \"-c\", \"echo Bonjour !\"] shell: RUN echo \"Bonjour !\" CMD Remove CMD line to test container in interactive mode, then build: $ docker build -t node-test-005:test . Launch a container in interactive mode with sh as shell. Don't forget to mention image tag after ':' as long as it ain't 'latest', and to mention the shell at the end, 'sh': $ docker run -it node-test-005:test sh /app $c As we can see, we are directly in 'app' folder. And by typing 'echo $0' to check shell is indeed, 'sh': $c echo $0 sh And check 'node' version: $c node --version v14.18.1 And test 'app.js' (in app.js -> console.log('Hi test 005');): $c node app.js Hi test 005 WORKDIR WORKDIR define working directory in image: WORKDIR /app Then, for COPY command, no need to specify destination directory: COPY ./app.js . Also for CMD: CMD [\"node\", \"app.js\"] WORKDIR can be changed during the Dockerfile by being filled in again. WORKDIR can create folders if they do not exist (this saves us a mkdir). FROM Only one FROM command by Dockerfile VS Code Dockerfile command VS Code, in a Dockerfile, hit ctrl+space to get a list of available commands. Shortcut available due to Docker Microsoft extension installed in VS Code. ADD source destination ADD source destination, similar to COPY but from URL or compressed file. If it's a compressed file it will be automatically uncompressed. Copy context Dockerfile context is current folder. Could not COPY file from parent folder. Remove image with pattern Remove all images that contain a pattern: $ docker image rm $(docker images --format \"{{.Repository}}\" | grep node-test-00) Optimize cache Optimizing cache. Only the RUN, COPY, and ADD instructions create new layers and increase the size of an image. It is therefore necessary to avoid multiplying the RUN commands, and try to group all the necessary commands in a single RUN instruction (multi-line separator '\\'): FROM ubuntu ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update && apt-get install -y \\ git \\ nodejs \\ && rm -rf /var/lib/apt/lists/* It's recommended to put one installation by line, alphabetically sorted. !! It is mandatory to put apt-get update and apt-get install in the same RUN statement. Otherwise you will have serious cache problems.!! For images using Ubuntu or Debian, it is recommended to remove /var/lib/apt/ lists which contains the APT cache with all available packages in order to reduce the size of the image. ENV DEBIAN_FRONTEND=noninteractive allows us to specify to the Debian Package Manager (APT) that we are in a non-interactive environment for the installation. This avoids the prompts requested by some programs during installation (eg Git). Invalidate cache !Important! Invalidate cache at build. If you have following instruction in Dockerfile it will be run only once at first build then cached: RUN apt update && apt dist-upgrade -y To not use a cache, you have to do: docker build --no-cache -t test . Inspect Go template docker inspect with Go template for format parameter. e.g. to retrieve CMD: $ docker inspect --format='{{.Config.Cmd}}' node-test-001 [node app/app.js] Show the history of an image: $ docker image history node-test-001 Image size node image = 900 MB/3 min VS alpine + node install = 50 MB/30 sec => ??? First build of node based image takes around 3 minutes. Intermediate steps are cached by Docker. Second build of node based image takes now only around 3 seconds. Dockerfile build image Docker file, steps to build an image. Instructions: - FROM // pull image from docker hub \\ = layer - RUN // add node / new image/running intermediate container - COPY / = layer new image/running intermediate container - CMD constructed image resulting from the different images, layers, and intermediate containers You can't run commands in an image, so you need intermediate container. Dockerfile context !! Warning !! Create a Dockerfile then build image in a dedicated folder for your application Otherwise, all files/folders contained in where you build image will be sent to the daemon at build time as the context. It is for this reason that you must create the Dockerfile in the folder of your application, or here to test, in a separate folder. If you create your Dockerfile directly in the root / directory, your entire hard drive is sent as context to the daemon! APK apk = Alpine Package Management It is the equivalent of APT for Debian distributions and therefore in particular for Ubuntu apk add --update actually allows you to do apk update first, then apk add. Debian equivalent of: apt update && apt install && lets you do something based on whether the previous command completed successfully - that's why you tend to see it chained as do_something && do_something_else_that_depended_on_something. Furthermore, you also have || which is the logical or, and also ; which is just a separator which doesn't care what happen to the command before. Dockerfile Create a new folder docker-test. Open it with VS Code. In this example we gonna crate a node image (based on Alpine, not on official node image) to simply test console log in a js file. Create a new file named, with VS Code, 'app.js' and type in it: console.log('Hello, world!'); Then, create a new file, in folder, with VS Code, named: Dockerfile Base image Install node Copy js file from local folder to container. If folder does not exist, it will be created. Type following commands in newly created Dockerfile file (exactly respect the case and do not add any extensions): FROM alpine RUN apk add --update nodejs COPY ./app.js /app/","title":"Docker - 03 - Docker File"},{"location":"docker/3-docker-dockerfile/#docker-03-docker-file","text":"","title":"Docker - 03 - Docker File"},{"location":"docker/3-docker-dockerfile/#image-variants","text":"$ docker pull node:slim node:latest = 992MB VS node:slim = 242MB","title":"Image Variants"},{"location":"docker/3-docker-dockerfile/#tag","text":"tag to tag image after build $ docker image tag mynode:latest mynode:1.0","title":"tag"},{"location":"docker/3-docker-dockerfile/#history","text":"Show the history of an image $ docker history mynode:latest IMAGE CREATED CREATED BY SIZE COMMENT 1a8f184ef896 8 minutes ago sh 42.4MB 14119a10abf4 2 months ago /bin/sh -c #(nop) CMD [\"/bin/sh\"] 0B <missing> 2 months ago /bin/sh -c #(nop) ADD file:aad4290d27580cc1a\u2026 5.6MB","title":"history"},{"location":"docker/3-docker-dockerfile/#logs","text":"$ docker container logs redis-4.0-001 -f to follow -t timestamp","title":"logs"},{"location":"docker/3-docker-dockerfile/#commit","text":"Snapshot a container to image $ docker run -it alpine sh # mkdir app open a new terminal to copy app.js \"manually\" to app folder $ docker container cp app.js ed6:/app/ then from inside container install node.js $c apk add --update nodejs then quit container and from host terminal $ docker container commit -c 'CMD [\"node\", \"/app/app.js\"]' ed6 mynode $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE mynode latest 1a8f184ef896 6 seconds ago 48MB .. $ docker run --rm mynode hello node-test-010","title":"COMMIT"},{"location":"docker/3-docker-dockerfile/#label","text":"LABEL to add meta information LABEL MAINTAINER=oldu73@gmail.com LABEL version=1.0 $ docker build -t node10 . $ docker image inspect node10:latest | less /oldu .. \"MAINTAINER\": \"oldu73@gmail.com\", \"version\": \"1.0\" } ..","title":"LABEL"},{"location":"docker/3-docker-dockerfile/#env","text":"key value Usable in container, available as environment variable: ENV environment=production $ docker build -t node10 . $ docker run -it node10 sh /app $c env HOSTNAME=ac68969910b7 SHLVL=1 HOME=/root environment=production TERM=xterm PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/app","title":"ENV"},{"location":"docker/3-docker-dockerfile/#arg","text":"Argument available at build time only: ARG folder ARG file then WORKDIR $folder COPY $file . then $ docker build --build-arg folder=/app --build-arg file=app.js -t node-test-008 . then if you try to retriev ARGs by typing env inside the container you do not retrieve it because they are available only at build time: $ docker run --rm -it node-test-008 sh /app # env HOSTNAME=22cc31c49889 SHLVL=1 HOME=/root TERM=xterm PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/app You may also put a default value: ARG folder=/app then $ docker build --build-arg file=app.js -t node-test-008 .","title":"ARG"},{"location":"docker/3-docker-dockerfile/#override-entry-point","text":"You can still override the entry point with the --entrypoint option: $ docker run --rm --entrypoint=\"echo\" node:test \"Hi, earth ;)\" or $ docker run -it --entrypoint=\"/bin/sh\" node:test","title":"Override entry point"},{"location":"docker/3-docker-dockerfile/#docker-default-entry-point","text":"By default, Docker has a default entry point which is \"/bin/sh -c\" but does not have a default command. $ man sh -> /-c -c Read commands from the command_string operand instead of from the standard input. Special parameter 0 will be set from the command_name operand and the positional parameters ($1, $2, etc.) set from the remaining argument operands.","title":"Docker default entry point"},{"location":"docker/3-docker-dockerfile/#entrypoint-and-cmd","text":"May have ENTRYPOINT and CMD: ENTRYPOINT [\"echo\"] CMD [\"hello\"] $ docker run --rm node:test hello then you may override hello in run parameter: $ docker run --rm node:test world world","title":"ENTRYPOINT and CMD"},{"location":"docker/3-docker-dockerfile/#exec-form","text":"[] exec form -> recommended .. shell form, like you would type the command in a terminal","title":"exec form"},{"location":"docker/3-docker-dockerfile/#entrypoint-instead-cmd","text":"ENTRYPOINT instead CMD avoid availability for end user to replace Dockerfile CMD by typing one at the end of run terminal command: ENTRYPOINT [\"node\", \"app.js\"] in Dockerfile, then: $ docker run --rm node:test Bonjour or $ docker run --rm node:test echo test Bonjour same same ;-), echo test at the end is not taking under consideration","title":"ENTRYPOINT instead CMD"},{"location":"docker/3-docker-dockerfile/#command-at-the-end-of-run","text":"Typing a command at the end of the run command replace the one in Dockerfile: $ docker run --rm node:test echo test test even if \"CMD [\"node\", \"app.js\"]\" in Dockerfile","title":"Command at the end of run"},{"location":"docker/3-docker-dockerfile/#remove-dangling-images","text":"Remove dangling ( ) images: $ docker image prune For removing dangling and ununsed images: $ docker image prune -a","title":"Remove dangling images"},{"location":"docker/3-docker-dockerfile/#docker-build-no-output","text":"Docker build not showing any output from commands(Dockerfile RUN): Dockerfile .. RUN echo hello Don't show anything in console at build. Use legacy mode by adding 'DOCKER_BUILDKIT=0' in front of docker build: $ DOCKER_BUILDKIT=0 docker build -t test:latest . .. Step 2/2 : RUN echo hello ---> Running in 3d9c96daa522 hello or (new fashion) with \"--progress=plain --no-cache\" after build command: $ docker build --progress=plain --no-cache -t node-test-007:latest . [3/5] RUN echo \"Hello, world!\" sha256:54040767d950b92027e2e377a0938fd42b89a34fa5d76e3ce281deacda0f1959 0.281 Hello, world! DONE 0.3s","title":"Docker build no output"},{"location":"docker/3-docker-dockerfile/#list-only-container-names","text":"To list only names of all containers: $ docker ps -a --format='{{.Names}}'","title":"List only container names"},{"location":"docker/3-docker-dockerfile/#env_1","text":"'ENV', environment variable: Dockerfile: - Base image - Test environment variable FROM alpine ENV DIR=/app WORKDIR ${DIR}/back then.. $ docker build -t node-test-006:latest . then.. $ docker run -it node-test-006 sh $c pwd /app/back","title":"ENV"},{"location":"docker/3-docker-dockerfile/#run","text":"RUN exist in 'exec' and 'shell' mode (which is 'sh' by default). exec: RUN [\"/bin/bash\", \"-c\", \"echo Bonjour !\"] shell: RUN echo \"Bonjour !\"","title":"RUN"},{"location":"docker/3-docker-dockerfile/#cmd","text":"Remove CMD line to test container in interactive mode, then build: $ docker build -t node-test-005:test . Launch a container in interactive mode with sh as shell. Don't forget to mention image tag after ':' as long as it ain't 'latest', and to mention the shell at the end, 'sh': $ docker run -it node-test-005:test sh /app $c As we can see, we are directly in 'app' folder. And by typing 'echo $0' to check shell is indeed, 'sh': $c echo $0 sh And check 'node' version: $c node --version v14.18.1 And test 'app.js' (in app.js -> console.log('Hi test 005');): $c node app.js Hi test 005","title":"CMD"},{"location":"docker/3-docker-dockerfile/#workdir","text":"WORKDIR define working directory in image: WORKDIR /app Then, for COPY command, no need to specify destination directory: COPY ./app.js . Also for CMD: CMD [\"node\", \"app.js\"] WORKDIR can be changed during the Dockerfile by being filled in again. WORKDIR can create folders if they do not exist (this saves us a mkdir).","title":"WORKDIR"},{"location":"docker/3-docker-dockerfile/#from","text":"Only one FROM command by Dockerfile","title":"FROM"},{"location":"docker/3-docker-dockerfile/#vs-code-dockerfile-command","text":"VS Code, in a Dockerfile, hit ctrl+space to get a list of available commands. Shortcut available due to Docker Microsoft extension installed in VS Code.","title":"VS Code Dockerfile command"},{"location":"docker/3-docker-dockerfile/#add-source-destination","text":"ADD source destination, similar to COPY but from URL or compressed file. If it's a compressed file it will be automatically uncompressed.","title":"ADD source destination"},{"location":"docker/3-docker-dockerfile/#copy-context","text":"Dockerfile context is current folder. Could not COPY file from parent folder.","title":"Copy context"},{"location":"docker/3-docker-dockerfile/#remove-image-with-pattern","text":"Remove all images that contain a pattern: $ docker image rm $(docker images --format \"{{.Repository}}\" | grep node-test-00)","title":"Remove image with pattern"},{"location":"docker/3-docker-dockerfile/#optimize-cache","text":"Optimizing cache. Only the RUN, COPY, and ADD instructions create new layers and increase the size of an image. It is therefore necessary to avoid multiplying the RUN commands, and try to group all the necessary commands in a single RUN instruction (multi-line separator '\\'): FROM ubuntu ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update && apt-get install -y \\ git \\ nodejs \\ && rm -rf /var/lib/apt/lists/* It's recommended to put one installation by line, alphabetically sorted. !! It is mandatory to put apt-get update and apt-get install in the same RUN statement. Otherwise you will have serious cache problems.!! For images using Ubuntu or Debian, it is recommended to remove /var/lib/apt/ lists which contains the APT cache with all available packages in order to reduce the size of the image. ENV DEBIAN_FRONTEND=noninteractive allows us to specify to the Debian Package Manager (APT) that we are in a non-interactive environment for the installation. This avoids the prompts requested by some programs during installation (eg Git).","title":"Optimize cache"},{"location":"docker/3-docker-dockerfile/#invalidate-cache","text":"!Important! Invalidate cache at build. If you have following instruction in Dockerfile it will be run only once at first build then cached: RUN apt update && apt dist-upgrade -y To not use a cache, you have to do: docker build --no-cache -t test .","title":"Invalidate cache"},{"location":"docker/3-docker-dockerfile/#inspect-go-template","text":"docker inspect with Go template for format parameter. e.g. to retrieve CMD: $ docker inspect --format='{{.Config.Cmd}}' node-test-001 [node app/app.js]","title":"Inspect Go template"},{"location":"docker/3-docker-dockerfile/#_1","text":"Show the history of an image: $ docker image history node-test-001","title":""},{"location":"docker/3-docker-dockerfile/#image-size","text":"node image = 900 MB/3 min VS alpine + node install = 50 MB/30 sec => ??? First build of node based image takes around 3 minutes. Intermediate steps are cached by Docker. Second build of node based image takes now only around 3 seconds.","title":"Image size"},{"location":"docker/3-docker-dockerfile/#dockerfile-build-image","text":"Docker file, steps to build an image. Instructions: - FROM // pull image from docker hub \\ = layer - RUN // add node / new image/running intermediate container - COPY / = layer new image/running intermediate container - CMD constructed image resulting from the different images, layers, and intermediate containers You can't run commands in an image, so you need intermediate container.","title":"Dockerfile build image"},{"location":"docker/3-docker-dockerfile/#dockerfile-context","text":"!! Warning !! Create a Dockerfile then build image in a dedicated folder for your application Otherwise, all files/folders contained in where you build image will be sent to the daemon at build time as the context. It is for this reason that you must create the Dockerfile in the folder of your application, or here to test, in a separate folder. If you create your Dockerfile directly in the root / directory, your entire hard drive is sent as context to the daemon!","title":"Dockerfile context"},{"location":"docker/3-docker-dockerfile/#apk","text":"apk = Alpine Package Management It is the equivalent of APT for Debian distributions and therefore in particular for Ubuntu apk add --update actually allows you to do apk update first, then apk add. Debian equivalent of: apt update && apt install && lets you do something based on whether the previous command completed successfully - that's why you tend to see it chained as do_something && do_something_else_that_depended_on_something. Furthermore, you also have || which is the logical or, and also ; which is just a separator which doesn't care what happen to the command before.","title":"APK"},{"location":"docker/3-docker-dockerfile/#dockerfile","text":"Create a new folder docker-test. Open it with VS Code. In this example we gonna crate a node image (based on Alpine, not on official node image) to simply test console log in a js file. Create a new file named, with VS Code, 'app.js' and type in it: console.log('Hello, world!'); Then, create a new file, in folder, with VS Code, named: Dockerfile Base image Install node Copy js file from local folder to container. If folder does not exist, it will be created. Type following commands in newly created Dockerfile file (exactly respect the case and do not add any extensions): FROM alpine RUN apk add --update nodejs COPY ./app.js /app/","title":"Dockerfile"},{"location":"docker/4-docker-dockerhub/","text":"Docker - 04 - Docker Hub export/import for container docker container export/import $ docker build -t mynode . $ docker run -it mynode sh $c touch hello.txt $c exit $ docker container ps -a CONTAINER ID IMAGE 3d8e43e502b0 mynode $ docker container export -o mycontainer.tar 3d8 $ tar -tvf mycontainer.tar $ docker container rm 3d8 $ docker image import mycontainer.tar nodetest $ docker images REPOSITORY TAG nodetest latest $ docker image inspect nodetest:latest Only one layer because exporting a container is like creating an image from a file system. It's not possible to relaunch a container directly from another exported container. You must first create an image with import. tar for image docker image save/load <-> tar $ docker build -t mynode:0.1 . $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE mynode 0.1 c68e7a86d468 8 seconds ago 48MB $ docker image save -o monimage.tar mynode (to compress with gzip: docker save mon_image | gzip > mon_image.tar.gz) $ tar -tvf monimage.tar $ docker image prune -a $ docker image load < monimage.tar or $ docker image load -i mon_image.tar $ docker run --rm mynode:0.1 hello node-test-012 Encrypt identifiers Encrypt your identifiers GNU/Linux $ sudo apt install pass $ gpg2 --gen-key Enter your name and email when requested. Then do: $ wget https://github.com/docker/docker-credential-helpers/releases/download/v0.6.3/docker-credential-pass-v0.6.3-amd64.tar.gz && tar -xf docker-credential-pass-v0.6.3-amd64.tar.gz && chmod +x docker-credential-pass && sudo mv docker-credential-pass /usr/local/bin/ $ pass init \"YOUR NAME\" $ nano ~/.docker/config.json then: { \"credsStore\": \"pass\" } $ docker login Push image Push image to docker hub $ docker login $ docker build -t oldu73/mynode . $ docker image push oldu73/mynode $ docker image prune -a $ docker run --rm oldu73/mynode $ docker logout Docker hub docker image pull docker image push docker image pull/push / :[tag] docker search https://hub.docker.com/ $ docker pull node $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE node latest 7220633f01cd 7 days ago 992MB $ docker run -it --rm node sh $c ls $c node -v1 $c mkdir app $c cd app $c echo \"console.log('Hello, world!');\" > app.js $c node app.js","title":"Docker - 04 - Docker Hub"},{"location":"docker/4-docker-dockerhub/#docker-04-docker-hub","text":"","title":"Docker - 04 - Docker Hub"},{"location":"docker/4-docker-dockerhub/#exportimport","text":"for container docker container export/import $ docker build -t mynode . $ docker run -it mynode sh $c touch hello.txt $c exit $ docker container ps -a CONTAINER ID IMAGE 3d8e43e502b0 mynode $ docker container export -o mycontainer.tar 3d8 $ tar -tvf mycontainer.tar $ docker container rm 3d8 $ docker image import mycontainer.tar nodetest $ docker images REPOSITORY TAG nodetest latest $ docker image inspect nodetest:latest Only one layer because exporting a container is like creating an image from a file system. It's not possible to relaunch a container directly from another exported container. You must first create an image with import.","title":"export/import"},{"location":"docker/4-docker-dockerhub/#tar","text":"for image docker image save/load <-> tar $ docker build -t mynode:0.1 . $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE mynode 0.1 c68e7a86d468 8 seconds ago 48MB $ docker image save -o monimage.tar mynode (to compress with gzip: docker save mon_image | gzip > mon_image.tar.gz) $ tar -tvf monimage.tar $ docker image prune -a $ docker image load < monimage.tar or $ docker image load -i mon_image.tar $ docker run --rm mynode:0.1 hello node-test-012","title":"tar"},{"location":"docker/4-docker-dockerhub/#encrypt-identifiers","text":"Encrypt your identifiers GNU/Linux $ sudo apt install pass $ gpg2 --gen-key Enter your name and email when requested. Then do: $ wget https://github.com/docker/docker-credential-helpers/releases/download/v0.6.3/docker-credential-pass-v0.6.3-amd64.tar.gz && tar -xf docker-credential-pass-v0.6.3-amd64.tar.gz && chmod +x docker-credential-pass && sudo mv docker-credential-pass /usr/local/bin/ $ pass init \"YOUR NAME\" $ nano ~/.docker/config.json then: { \"credsStore\": \"pass\" } $ docker login","title":"Encrypt identifiers"},{"location":"docker/4-docker-dockerhub/#push-image","text":"Push image to docker hub $ docker login $ docker build -t oldu73/mynode . $ docker image push oldu73/mynode $ docker image prune -a $ docker run --rm oldu73/mynode $ docker logout","title":"Push image"},{"location":"docker/4-docker-dockerhub/#docker-hub","text":"docker image pull docker image push docker image pull/push / :[tag] docker search https://hub.docker.com/ $ docker pull node $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE node latest 7220633f01cd 7 days ago 992MB $ docker run -it --rm node sh $c ls $c node -v1 $c mkdir app $c cd app $c echo \"console.log('Hello, world!');\" > app.js $c node app.js","title":"Docker hub"},{"location":"docker/5-docker-nodeserver/","text":"Docker - 05 - Node Server Node Server Image Stats Show resource usage statistic Maybe tried with Node server project running $ docker run --rm -d --name appnode -p 80:80 myapp Then $ docker stats After, open a browser at http://localhost/ address Click a lot on refresh button to see CPU % growing in stats terminal's live view. Detach mode -d option $ docker run --rm -d --name appnode -p 80:80 myapp --rm to remove container automatically after stop Then to go back inside running detached container $ docker exec -it appnode sh .dockerignore In current project folder, create a new file named \".dockerignore\" Add also a sample text file \"hello.txt\" file that maybe contain \"to be ignored\" sentence. Re-build image $ docker build -t myapp . Launch it in interactive mode with sh $ docker run --rm -it myapp sh ls/cat hello.txt file in container output respectively presence/content of hello.txt file. Now list hello.txt file in .dockerignore file # for current folder hello.txt # for first level of folder */hello.txt # for everywhere **/hello.txt # exception with ! **/*.txt !README.txt Re-build image Re-launch it in interactive mode ls output does not contain hello.txt file anymore Optimization Let's say we change listening port of Node server project from 80 to 70 file app.js const express = require('express'); const app = express(); app.get('*', (req, res) => { res.status(200).json('Hello, world!'); }) app.listen(70); Re-build image $ docker build -t myapp . => [4/4] RUN npm install 7.1s What we observe here is that NPM install is run again although only listening port in app.js has been modified. To avoid this behavior, Docker file should be modified accordingly. Split COPY instruction in two, before and after \"RUN npm install\" to not invalidate cache for dependencies (package.json). Dockerfile FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"nodemon\", \"app.js\"] And now, on build image after changing listening port in app.js $ docker build -t myapp . => [4/4] RUN npm install 0.0s Redirect port Redirect port from host to container docker run ... -p <hostport>:<containerport> ... -p option: -p, --publish list Publish a container's port(s) to the host To fix Node server project, add missing port redirection to run command: $ docker run --rm -p 80:80 myapp Many containers may listen on same port number because they're isolated. $ docker run --rm -d -p 81:80 myapp $ docker run --rm -d -p 82:80 myapp On the other hand, on the host, only one forwarding on a port is possible. One port = one application! Without opening port, a container may access to Internet. Outgoing traffic is possible. By default, all incoming traffic is blocked, all ports are closed by default. Port http 80 https 443 ssh 22 Node server project Node server project to return a minimal page in a browser at localhost address, with Express (which is a Node.js framework) Our image should have Node.js and its npm package manager. It should have several dependencies: nodemon and express. And it will have to launch the app contained in app.js by default. For the project, we'll create a new Dockerfile in a folder. In the same folder we will also have a package.json file (which allows you to manage the dependencies used) and an app.js file (which will contain our application). In the package.json file therefore have our two dependencies. In the app.js file we just have an Express route which will send in JSON format the character string \"Hello, world!\" for all routes. folder: - /mnt/c/tmp/docker/node-server $ touch package.json edit package.json { \"dependencies\": { \"nodemon\": \"2.0.14\", \"express\": \"4.17.1\" } } $ touch app.js edit app.js const express = require('express'); const app = express(); app.get('*', (req, res) => { res.status(200).json('Hello, world!'); }) app.listen(80); $ touch Dockerfile Browse a bit docker hub and look for alpine tagged image more lighter (169MB) than the official one (latest) (992MB) FROM node:alpine WORKDIR /app COPY . . RUN npm install CMD [\"nodemon\", \"app.js\"] then.. $ docker build -t myapp . $ docker image ls $ docker run --rm myapp path error to debug, run in interactive mode with sh to override Dockerfile CMD $ docker run --rm -it myapp sh /app $c env /app $c nodemon sh: nodemon: not found $c cd node_modules/.bin/ /app/node_modules/.bin # ls is-ci mime nodemon /app $c ./node_modules/.bin/nodemon app.js Fix the Docker file with adding value to image's path .. ENV PATH=$PATH:/app/node_modules/.bin .. $ docker build -t myapp . $ docker run --rm -it myapp sh /app $c env | grep PATH PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/app/node_modules/.bin /app $c nodemon app.js Now it's OK $ docker run --rm myapp Try \"localhost\" in a browser -> does not work and it's normal because default behavior does not allow to communicate with a container.","title":"Docker - 05 - Node Server"},{"location":"docker/5-docker-nodeserver/#docker-05-node-server","text":"Node Server Image","title":"Docker - 05 - Node Server"},{"location":"docker/5-docker-nodeserver/#stats","text":"Show resource usage statistic Maybe tried with Node server project running $ docker run --rm -d --name appnode -p 80:80 myapp Then $ docker stats After, open a browser at http://localhost/ address Click a lot on refresh button to see CPU % growing in stats terminal's live view.","title":"Stats"},{"location":"docker/5-docker-nodeserver/#detach-mode","text":"-d option $ docker run --rm -d --name appnode -p 80:80 myapp --rm to remove container automatically after stop Then to go back inside running detached container $ docker exec -it appnode sh","title":"Detach mode"},{"location":"docker/5-docker-nodeserver/#dockerignore","text":"In current project folder, create a new file named \".dockerignore\" Add also a sample text file \"hello.txt\" file that maybe contain \"to be ignored\" sentence. Re-build image $ docker build -t myapp . Launch it in interactive mode with sh $ docker run --rm -it myapp sh ls/cat hello.txt file in container output respectively presence/content of hello.txt file. Now list hello.txt file in .dockerignore file # for current folder hello.txt # for first level of folder */hello.txt # for everywhere **/hello.txt # exception with ! **/*.txt !README.txt Re-build image Re-launch it in interactive mode ls output does not contain hello.txt file anymore","title":".dockerignore"},{"location":"docker/5-docker-nodeserver/#optimization","text":"Let's say we change listening port of Node server project from 80 to 70 file app.js const express = require('express'); const app = express(); app.get('*', (req, res) => { res.status(200).json('Hello, world!'); }) app.listen(70); Re-build image $ docker build -t myapp . => [4/4] RUN npm install 7.1s What we observe here is that NPM install is run again although only listening port in app.js has been modified. To avoid this behavior, Docker file should be modified accordingly. Split COPY instruction in two, before and after \"RUN npm install\" to not invalidate cache for dependencies (package.json). Dockerfile FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"nodemon\", \"app.js\"] And now, on build image after changing listening port in app.js $ docker build -t myapp . => [4/4] RUN npm install 0.0s","title":"Optimization"},{"location":"docker/5-docker-nodeserver/#redirect-port","text":"Redirect port from host to container docker run ... -p <hostport>:<containerport> ... -p option: -p, --publish list Publish a container's port(s) to the host To fix Node server project, add missing port redirection to run command: $ docker run --rm -p 80:80 myapp Many containers may listen on same port number because they're isolated. $ docker run --rm -d -p 81:80 myapp $ docker run --rm -d -p 82:80 myapp On the other hand, on the host, only one forwarding on a port is possible. One port = one application! Without opening port, a container may access to Internet. Outgoing traffic is possible. By default, all incoming traffic is blocked, all ports are closed by default.","title":"Redirect port"},{"location":"docker/5-docker-nodeserver/#port","text":"http 80 https 443 ssh 22","title":"Port"},{"location":"docker/5-docker-nodeserver/#node-server-project","text":"Node server project to return a minimal page in a browser at localhost address, with Express (which is a Node.js framework) Our image should have Node.js and its npm package manager. It should have several dependencies: nodemon and express. And it will have to launch the app contained in app.js by default. For the project, we'll create a new Dockerfile in a folder. In the same folder we will also have a package.json file (which allows you to manage the dependencies used) and an app.js file (which will contain our application). In the package.json file therefore have our two dependencies. In the app.js file we just have an Express route which will send in JSON format the character string \"Hello, world!\" for all routes. folder: - /mnt/c/tmp/docker/node-server $ touch package.json edit package.json { \"dependencies\": { \"nodemon\": \"2.0.14\", \"express\": \"4.17.1\" } } $ touch app.js edit app.js const express = require('express'); const app = express(); app.get('*', (req, res) => { res.status(200).json('Hello, world!'); }) app.listen(80); $ touch Dockerfile Browse a bit docker hub and look for alpine tagged image more lighter (169MB) than the official one (latest) (992MB) FROM node:alpine WORKDIR /app COPY . . RUN npm install CMD [\"nodemon\", \"app.js\"] then.. $ docker build -t myapp . $ docker image ls $ docker run --rm myapp path error to debug, run in interactive mode with sh to override Dockerfile CMD $ docker run --rm -it myapp sh /app $c env /app $c nodemon sh: nodemon: not found $c cd node_modules/.bin/ /app/node_modules/.bin # ls is-ci mime nodemon /app $c ./node_modules/.bin/nodemon app.js Fix the Docker file with adding value to image's path .. ENV PATH=$PATH:/app/node_modules/.bin .. $ docker build -t myapp . $ docker run --rm -it myapp sh /app $c env | grep PATH PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/app/node_modules/.bin /app $c nodemon app.js Now it's OK $ docker run --rm myapp Try \"localhost\" in a browser -> does not work and it's normal because default behavior does not allow to communicate with a container.","title":"Node server project"},{"location":"docker/6-docker-datapersistence/","text":"Docker - 06 - Data Persistence Introduction Container = - writable layer (this layer is deleted if the container no longer exists) - image layer(s) (read) Writable layers use UnionFS, slow read write performance, not adapted to host database. To persist data, 3 possibilities: Volumes On Filesystem but managed by Docker, accessible through Docker CLI), mostly advised to use (in /var/lib/docker/volumes/ (not on WSL) but never access it directly. Volumes are stored on but isolated from host machine. Create a volume with Docker CLI command docker volume create Bind mount Manged by Filesystem and accessible from outside of Docker, not recommended. Bind mounts are regular folder and files stored on host machine. TMPFS Temporary File System -> RAM. TMPFS are used to store temporary not persisted data, sensitive information like secret. Bind mount Adapted for: - Sharing configuration files between host and container. - Development environment to share source code and alow live reload. Recommend syntax docker run --mount type=bind,source=<url>,target=<url> image $ mkdir data $ cd data $ touch hello.txt $ cd .. $ docker run --mount type=bind,source=\"$(pwd)\"/data,target=/data -it alpine sh $c ls .. data .. $c cd data $c ls hello.txt Search for Mounts section with inspect CLI command to see mounting details: docker container inspect containerName Development environment Set up of a development environnement for a node server application (c.f. Node server project ). The goal here is to have changes available from host machine (IDE) in container and automatically reloaded by nodemon. !!Warning!! A common mistake is to bind the root folder of a project. E.g. in case of node context, if we do so, the node_modules folder created/populated in container image by \"RUN npm install\" instruction from Dockerfile will be erased. To avoid this unwanted behavior, it's mostly advised to move the source code files modified through IDE on host machine in a dedicated folder (e.g. src). On host machine: $ mkdir node-server $ cd node-server $ mkdir src $ cd src $ touch app.js $ cd .. src/app.js: const express = require('express'); const app = express(); app.get('*', (req, res) => { res.status(200).json('Hello, world!'); }) app.listen(80); Dockerfile (in root folder, \"node-server\"): FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"nodemon\", \"-L\", \"src/app.js\"] !! Warning !! In above Dockerfile we use -L option for --legacy-watch because in some containerized environnement, application may not restart automatically after file's change. Build image: $ docker build -t myapp . Start a container with new image version: $ docker run --rm -p 80:80 --mount type=bind,source=\"$(pwd)/src\",target=/app/src myapp Now try to edit 'src/app.js' file on host machine to observe nodemon restart in terminal. You may also observe the changes on your browser at 'localhost' address, after refresh. Volumes docker volume - create - inspect - ls - rm - prune old syntax (not advised, c.f. bind) docker run -v <volume-name>:<container-url> image new syntax docker run --mount type=volume,source=<volume-name>,target=<url> image Create New volume $ docker volume create mydata mydata $ docker volume ls DRIVER VOLUME NAME local mydata $ docker volume inspect mydata [ { \"CreatedAt\": \"2021-12-24T06:15:34Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/mydata/_data\", \"Name\": \"mydata\", \"Options\": {}, \"Scope\": \"local\" } ] WSL2 volumes are not in /var/lib/docker/volumes/mydata/_data You can find WSL2 volumes under a hidden network share. Open Windows Explorer, and type \\\\wsl$ into the location bar. Hit enter, and it should display your WSL volumes, including the ones for Docker for Windows. WSL2 volumes, in Windows Explorer bar \\\\wsl$\\docker-desktop-data\\version-pack-data\\community\\docker\\volumes New container with bind volume $ docker run --mount type = volume,source = mydata,target = /data -it alpine sh $ c cd data $ c touch hello.txt $ c echo 123 > hello.txt ctrl+d $ docker rm containername $ docker run --mount type = volume,source = mydata,target = /data -it alpine sh $ c cd data $ c cat hello.txt 123 Share Backup Restore Share volume between containers $ docker run --mount type = volume,source = mydata,target = /data --name firstcont -it alpine sh $ c1 cd data In another terminal $ docker run --mount type = volume,source = mydata,target = /data --name secondcont -it alpine sh $ c2 cd data $ c2 touch new.txt $ c1 ls hello.txt new.txt Another way is volume from container $ docker run --volumes-from firstcont --name thirdcont -it alpine sh Backup volume Compress a volume to a bind folder with tar $ mkdir backup $ docker run --mount type = volume,source = mydata,target = /data --mount type = bind,source = \" $( pwd ) \" /backup,target = /backup alpine tar -czf /backup/mydata.tar.gz /data Restore volume Extract an archive from a bind folder to a volume $ docker volume create restore $ docker run --mount type = volume,source = restore,target = /data --mount type = bind,source = \" $( pwd ) \" /backup,target = /backup alpine tar -xf /backup/mydata.tar.gz -C /data To not have a folder data in a folder data, use tar with option --strip-components 1 $ docker run --mount type = volume,source = restore,target = /data --mount type = bind,source = \" $( pwd ) \" ,target = /backup -it alpine tar -xf /backup/backup.tar --strip-components 1 -C /data Check volume correctly restored $ docker container run -it --rm --mount source = restore,target = /data alpine sh Volume to persist a database (mongo) For mongo db it consist of mounting a volume with folder /data/db in mongo container $ docker volume create mydb $ docker run --mount type = volume,source = mydb,target = /data/db -d --name mongocontainer1 mongo $ docker exec -it mongocontainer1 sh $ c mongo > use test switched to db test > db.user.insertOne({ name: 'jean' }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"61caeae2df845609f1835264\") } > db.user.findOne() { \"_id\" : ObjectId(\"61caeae2df845609f1835264\"), \"name\" : \"jean\" } > exit bye $ c exit $ docker container stop mongocontainer1 $ docker container rm mongocontainer1 Container is removed but database is persisted in a volume. $ docker run --mount type = volume,source = mydb,target = /data/db -d --name mongocontainer2 mongo $ docker exec -it mongocontainer1 sh $ c mongo > use test switched to db test > db.user.findOne() { \"_id\" : ObjectId(\"61caeae2df845609f1835264\"), \"name\" : \"jean\" } Compass GUI to browse mongo db Should run container with opened port (default 27017). If port is already used on host machine, you may use 27018 for example $ docker run -p 27018 :27017 --mount type = volume,source = mydb,target = /data/db -d --name mongocontainer3 mongo Enter in connection field: mongodb://localhost:27018 TMPFS Rarely used, uniquely to keep data in RAM, e.g. secret or status data, works only on Linux system. $ docker run --mount type = tmpfs,target = /data -it alpine sh $ c cd data $ c touch secret.txt $ c ls secret.txt $ c exit $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 58f104d6a051 alpine \"sh\" 2 minutes ago Exited (0) 5 seconds ago musing_wilbur With 'TMPFS' if container is in an 'Exited' status, data aren't persisted. Relaunch container to observe that data aren't available anymore. $ docker start -ai musing_wilbur $ c cd data $ c ls (empty)","title":"Docker - 06 - Data Persistence"},{"location":"docker/6-docker-datapersistence/#docker-06-data-persistence","text":"","title":"Docker - 06 - Data Persistence"},{"location":"docker/6-docker-datapersistence/#introduction","text":"Container = - writable layer (this layer is deleted if the container no longer exists) - image layer(s) (read) Writable layers use UnionFS, slow read write performance, not adapted to host database. To persist data, 3 possibilities:","title":"Introduction"},{"location":"docker/6-docker-datapersistence/#volumes","text":"On Filesystem but managed by Docker, accessible through Docker CLI), mostly advised to use (in /var/lib/docker/volumes/ (not on WSL) but never access it directly. Volumes are stored on but isolated from host machine. Create a volume with Docker CLI command docker volume create","title":"Volumes"},{"location":"docker/6-docker-datapersistence/#bind-mount","text":"Manged by Filesystem and accessible from outside of Docker, not recommended. Bind mounts are regular folder and files stored on host machine.","title":"Bind mount"},{"location":"docker/6-docker-datapersistence/#tmpfs","text":"Temporary File System -> RAM. TMPFS are used to store temporary not persisted data, sensitive information like secret.","title":"TMPFS"},{"location":"docker/6-docker-datapersistence/#bind-mount_1","text":"Adapted for: - Sharing configuration files between host and container. - Development environment to share source code and alow live reload. Recommend syntax docker run --mount type=bind,source=<url>,target=<url> image $ mkdir data $ cd data $ touch hello.txt $ cd .. $ docker run --mount type=bind,source=\"$(pwd)\"/data,target=/data -it alpine sh $c ls .. data .. $c cd data $c ls hello.txt Search for Mounts section with inspect CLI command to see mounting details: docker container inspect containerName","title":"Bind mount"},{"location":"docker/6-docker-datapersistence/#development-environment","text":"Set up of a development environnement for a node server application (c.f. Node server project ). The goal here is to have changes available from host machine (IDE) in container and automatically reloaded by nodemon. !!Warning!! A common mistake is to bind the root folder of a project. E.g. in case of node context, if we do so, the node_modules folder created/populated in container image by \"RUN npm install\" instruction from Dockerfile will be erased. To avoid this unwanted behavior, it's mostly advised to move the source code files modified through IDE on host machine in a dedicated folder (e.g. src). On host machine: $ mkdir node-server $ cd node-server $ mkdir src $ cd src $ touch app.js $ cd .. src/app.js: const express = require('express'); const app = express(); app.get('*', (req, res) => { res.status(200).json('Hello, world!'); }) app.listen(80); Dockerfile (in root folder, \"node-server\"): FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"nodemon\", \"-L\", \"src/app.js\"] !! Warning !! In above Dockerfile we use -L option for --legacy-watch because in some containerized environnement, application may not restart automatically after file's change. Build image: $ docker build -t myapp . Start a container with new image version: $ docker run --rm -p 80:80 --mount type=bind,source=\"$(pwd)/src\",target=/app/src myapp Now try to edit 'src/app.js' file on host machine to observe nodemon restart in terminal. You may also observe the changes on your browser at 'localhost' address, after refresh.","title":"Development environment"},{"location":"docker/6-docker-datapersistence/#volumes_1","text":"docker volume - create - inspect - ls - rm - prune old syntax (not advised, c.f. bind) docker run -v <volume-name>:<container-url> image new syntax docker run --mount type=volume,source=<volume-name>,target=<url> image","title":"Volumes"},{"location":"docker/6-docker-datapersistence/#create","text":"","title":"Create"},{"location":"docker/6-docker-datapersistence/#new-volume","text":"$ docker volume create mydata mydata $ docker volume ls DRIVER VOLUME NAME local mydata $ docker volume inspect mydata [ { \"CreatedAt\": \"2021-12-24T06:15:34Z\", \"Driver\": \"local\", \"Labels\": {}, \"Mountpoint\": \"/var/lib/docker/volumes/mydata/_data\", \"Name\": \"mydata\", \"Options\": {}, \"Scope\": \"local\" } ] WSL2 volumes are not in /var/lib/docker/volumes/mydata/_data You can find WSL2 volumes under a hidden network share. Open Windows Explorer, and type \\\\wsl$ into the location bar. Hit enter, and it should display your WSL volumes, including the ones for Docker for Windows. WSL2 volumes, in Windows Explorer bar \\\\wsl$\\docker-desktop-data\\version-pack-data\\community\\docker\\volumes","title":"New volume"},{"location":"docker/6-docker-datapersistence/#new-container-with-bind-volume","text":"$ docker run --mount type = volume,source = mydata,target = /data -it alpine sh $ c cd data $ c touch hello.txt $ c echo 123 > hello.txt ctrl+d $ docker rm containername $ docker run --mount type = volume,source = mydata,target = /data -it alpine sh $ c cd data $ c cat hello.txt 123","title":"New container with bind volume"},{"location":"docker/6-docker-datapersistence/#share-backup-restore","text":"","title":"Share Backup Restore"},{"location":"docker/6-docker-datapersistence/#share-volume-between-containers","text":"$ docker run --mount type = volume,source = mydata,target = /data --name firstcont -it alpine sh $ c1 cd data In another terminal $ docker run --mount type = volume,source = mydata,target = /data --name secondcont -it alpine sh $ c2 cd data $ c2 touch new.txt $ c1 ls hello.txt new.txt Another way is volume from container $ docker run --volumes-from firstcont --name thirdcont -it alpine sh","title":"Share volume between containers"},{"location":"docker/6-docker-datapersistence/#backup-volume","text":"Compress a volume to a bind folder with tar $ mkdir backup $ docker run --mount type = volume,source = mydata,target = /data --mount type = bind,source = \" $( pwd ) \" /backup,target = /backup alpine tar -czf /backup/mydata.tar.gz /data","title":"Backup volume"},{"location":"docker/6-docker-datapersistence/#restore-volume","text":"Extract an archive from a bind folder to a volume $ docker volume create restore $ docker run --mount type = volume,source = restore,target = /data --mount type = bind,source = \" $( pwd ) \" /backup,target = /backup alpine tar -xf /backup/mydata.tar.gz -C /data To not have a folder data in a folder data, use tar with option --strip-components 1 $ docker run --mount type = volume,source = restore,target = /data --mount type = bind,source = \" $( pwd ) \" ,target = /backup -it alpine tar -xf /backup/backup.tar --strip-components 1 -C /data Check volume correctly restored $ docker container run -it --rm --mount source = restore,target = /data alpine sh","title":"Restore volume"},{"location":"docker/6-docker-datapersistence/#volume-to-persist-a-database-mongo","text":"For mongo db it consist of mounting a volume with folder /data/db in mongo container $ docker volume create mydb $ docker run --mount type = volume,source = mydb,target = /data/db -d --name mongocontainer1 mongo $ docker exec -it mongocontainer1 sh $ c mongo > use test switched to db test > db.user.insertOne({ name: 'jean' }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"61caeae2df845609f1835264\") } > db.user.findOne() { \"_id\" : ObjectId(\"61caeae2df845609f1835264\"), \"name\" : \"jean\" } > exit bye $ c exit $ docker container stop mongocontainer1 $ docker container rm mongocontainer1 Container is removed but database is persisted in a volume. $ docker run --mount type = volume,source = mydb,target = /data/db -d --name mongocontainer2 mongo $ docker exec -it mongocontainer1 sh $ c mongo > use test switched to db test > db.user.findOne() { \"_id\" : ObjectId(\"61caeae2df845609f1835264\"), \"name\" : \"jean\" }","title":"Volume to persist a database (mongo)"},{"location":"docker/6-docker-datapersistence/#compass-gui-to-browse-mongo-db","text":"Should run container with opened port (default 27017). If port is already used on host machine, you may use 27018 for example $ docker run -p 27018 :27017 --mount type = volume,source = mydb,target = /data/db -d --name mongocontainer3 mongo Enter in connection field: mongodb://localhost:27018","title":"Compass GUI to browse mongo db"},{"location":"docker/6-docker-datapersistence/#tmpfs_1","text":"Rarely used, uniquely to keep data in RAM, e.g. secret or status data, works only on Linux system. $ docker run --mount type = tmpfs,target = /data -it alpine sh $ c cd data $ c touch secret.txt $ c ls secret.txt $ c exit $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 58f104d6a051 alpine \"sh\" 2 minutes ago Exited (0) 5 seconds ago musing_wilbur With 'TMPFS' if container is in an 'Exited' status, data aren't persisted. Relaunch container to observe that data aren't available anymore. $ docker start -ai musing_wilbur $ c cd data $ c ls (empty)","title":"TMPFS"},{"location":"docker/7-docker-network/","text":"Docker - 07 - Network Introduction WAN = Internet LAN = Local docker network: - ls - create - rm - inspect - connect - disconnect - prune - --network | --net 3 methods: - Bridge, sub-segment - Host, merge host machine network - Overlay, Docker Swarm (- MACVLAN) (- Others) Bridge (mainly used) Grouped by sub-segment. Docker as bridge manager. By default a container belongs to named \"bridge\" (Docker0) network. Host (Linux only, rarely used) IP addresses for containers defined by router like for host machine. Containers will be straight forward connected to local network. Overlay (Swarm) To establish communication between Docker Daemons. Bridge $ docker network $ docker network ls NETWORK ID NAME DRIVER SCOPE 266b7ae5e9d7 bridge bridge local ada5f50a5c41 host host local 119b5f46e464 none null local $ docker network inspect bridge ... \"Containers\": {}, ... $ ifconfig ... docker0: ... $ docker run --rm -it alpine sh in a second terminal $ docker network inspect bridge ... \"Containers\": { \"04d3d4540a17d21ea7db83779e8de1716e6e3a4122e1f2c2f66c60d25a094656\": { \"Name\": \"stoic_wozniak\", \"EndpointID\": \"2ed11c55d850ed3cc4eec221f705dad2a9679a016934cd991cd96da86d2dfcbd\", \"MacAddress\": \"02:42:ac:11:00:02\", \"IPv4Address\": \"172.17.0.2/16\", \"IPv6Address\": \"\" } } ... $ docker run --rm -it alpine sh in a third terminal $ docker network inspect bridge \"Containers\": { \"04d3d4540a17d21ea7db83779e8de1716e6e3a4122e1f2c2f66c60d25a094656\": { \"Name\": \"stoic_wozniak\", \"EndpointID\": \"2ed11c55d850ed3cc4eec221f705dad2a9679a016934cd991cd96da86d2dfcbd\", \"MacAddress\": \"02:42:ac:11:00:02\", \"IPv4Address\": \"172.17.0.2/16\", \"IPv6Address\": \"\" }, \"76ff8f56718ae5244eabe03092f7a0227aa2e42249bf2ab1c8f91a9faf76c715\": { \"Name\": \"admiring_driscoll\", \"EndpointID\": \"684e6a570db46dadd9bdf53bf383ea069fa626fc16b7ab26c11e604415c00b25\", \"MacAddress\": \"02:42:ac:11:00:03\", \"IPv4Address\": \"172.17.0.3/16\", \"IPv6Address\": \"\" } } in second terminal $ c ping google.ch $ c ping 172 .17.0.2 Be aware that ip address maybe attributed randomly by Docker. To use name on default network, use --name and --link (deprecated) options on run command then you may ping by name instead of ip address (only for default bridge network). in first terminal $ docker run --rm --name alpine1 -it alpine sh in second terminal $ docker run --rm --link alpine1 -it alpine sh $ c ping alpine1 Create bridge Create a network, make two containers communicate through it and use container name instead of ip addresses. Create network, default driver = bridge $ docker network create mynet $ docker network ls NETWORK ID NAME DRIVER SCOPE 266b7ae5e9d7 bridge bridge local ada5f50a5c41 host host local 0a85e3670d62 mynet bridge local 119b5f46e464 none null local Use network with a named (important for name resolution over the network) container $ docker run --rm --network mynet --name server1 -d alpine ping google.ch $ docker inspect mynet \"Containers\": { \"249b952ab5db2ac4f3077e1a7fb89582eedaa02c236a92a7e15fc5cee73d3292\": { \"Name\": \"server1\", \"EndpointID\": \"48284c2119158a9ebf9d67b3eee0c74c58ede079ba173b769f5b791f2f507abb\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.18.0.2/16\", \"IPv6Address\": \"\" } } $ docker run --rm -it --network mynet --name server2 alpine sh $ c ping server1 PING server1 (172.18.0.2): 56 data bytes 64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.079 ms 64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.166 ms From another terminal $ docker exec -it server1 sh $ c ping server2 PING server2 (172.18.0.3): 56 data bytes 64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.125 ms 64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.496 ms Remove network $ docker network rm mynet Remove all network at once $ docker network prune Connect a Node.js server with MongoDB Goal: - display a counter in a browser at 'localhost' address. Architecture: - Image - MongoDB - Image - Node.js - Volumes - mydb { count: x } - Container - server - Container - db - Network - mynet - Port 80 open to listen to request (count++) from a browser at 'localhost' address MongoDB Volume and container $ docker volume create mydb $ docker run --name db --mount type = volume,source = mydb,target = /data/db -d mongo Network and connect (and disconnect from default bridge) $ docker network create mynet $ docker network connect mynet db $ docker network disconnect bridge db Create db and collection to handle and initialize the counter $ docker exec -it db sh $ c mongo > use test switched to db test > db.count.insertOne({count:0}) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"61cc2517094e32ba7f98bb31\") } > db.count.find() { \"_id\" : ObjectId(\"61cc2517094e32ba7f98bb31\"), \"count\" : 0 } > exit bye $ c exit Node server Development Development Environnement setup First step, application development with bind mount. We should use the mongo javascript driver in our application to allow connection to the db. In 'node-server' folder. File 'package.json' add mongo dependencies (browse for \"npm mongodb\" -> MongoDB NodeJS Driver, to check version) { \"dependencies\" : { \"express\" : \"^4.17.1\" , \"mongodb\" : \"^3.6.2\" , \"nodemon\" : \"^2.0.6\" , \"console-stamp\" : \"^3.0.3\" } } \"console-stamp\" is to add timestamp in logs ^version \u201cCompatible with version\u201d, will update you to all future minor/patch versions, without incrementing the major version. ^2.3.4 will use releases from 2.3.4 to <3.0.0. Dockerfile FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"nodemon\", \"-L\", \"src/app.js\"] Build image in 'node-server' folder $ docker build -t node-server . Application is in js file 'node-server/src/app.js' const express = require ( \"express\" ); const app = express (); app . get ( \"*\" , ( req , res ) => { res . status ( 200 ). json ( \"Hello, world!\" ); }); app . listen ( 80 ); To develop application use a bind mount $ docker run --name server --mount type = bind,source = \" $( pwd ) \" /src,target = /app/src -p 80 :80 --network mynet node-server Browse to 'localhost'. Observe live change availability by editing \"Hello, world!\" response in 'app.js' file and refreshing 'localhost' page in internet browser. Check that port '80' is published for 'server' container $ docker container port server 80/tcp -> 0.0.0.0:80 Development Server configuration Modify 'app.js' file as follow require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; MongoClient . connect ( 'mongodb://db' , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNEXION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); Node server Production Rebuild node server image with released app.js in it (above development has been erased by bind mount) In 'node-server' folder $ docker build -t node-server . If 'node-server' container is still running, remove it and then $ docker run --name server --network mynet -d -p 80 :80 node-server Host C.f. Bridge section for initial setup. Reset 'app.js' to const express = require ( \"express\" ); const app = express (); app . get ( \"*\" , ( req , res ) => { res . status ( 200 ). json ( \"Hello, world!\" ); }); app . listen ( 80 ); Rebuild 'node-server' image $ docker build -t node-server . Relaunch server but on local network, this time, no need to publish port $ docker run --network host node-server Do not work on WSL (Windows Subsystem for Linux), neither MacOs. To not use any network $ docker run --network none node-server","title":"Docker - 07 - Network"},{"location":"docker/7-docker-network/#docker-07-network","text":"","title":"Docker - 07 - Network"},{"location":"docker/7-docker-network/#introduction","text":"WAN = Internet LAN = Local docker network: - ls - create - rm - inspect - connect - disconnect - prune - --network | --net 3 methods: - Bridge, sub-segment - Host, merge host machine network - Overlay, Docker Swarm (- MACVLAN) (- Others)","title":"Introduction"},{"location":"docker/7-docker-network/#bridge-mainly-used","text":"Grouped by sub-segment. Docker as bridge manager. By default a container belongs to named \"bridge\" (Docker0) network.","title":"Bridge (mainly used)"},{"location":"docker/7-docker-network/#host-linux-only-rarely-used","text":"IP addresses for containers defined by router like for host machine. Containers will be straight forward connected to local network.","title":"Host (Linux only, rarely used)"},{"location":"docker/7-docker-network/#overlay-swarm","text":"To establish communication between Docker Daemons.","title":"Overlay (Swarm)"},{"location":"docker/7-docker-network/#bridge","text":"$ docker network $ docker network ls NETWORK ID NAME DRIVER SCOPE 266b7ae5e9d7 bridge bridge local ada5f50a5c41 host host local 119b5f46e464 none null local $ docker network inspect bridge ... \"Containers\": {}, ... $ ifconfig ... docker0: ... $ docker run --rm -it alpine sh in a second terminal $ docker network inspect bridge ... \"Containers\": { \"04d3d4540a17d21ea7db83779e8de1716e6e3a4122e1f2c2f66c60d25a094656\": { \"Name\": \"stoic_wozniak\", \"EndpointID\": \"2ed11c55d850ed3cc4eec221f705dad2a9679a016934cd991cd96da86d2dfcbd\", \"MacAddress\": \"02:42:ac:11:00:02\", \"IPv4Address\": \"172.17.0.2/16\", \"IPv6Address\": \"\" } } ... $ docker run --rm -it alpine sh in a third terminal $ docker network inspect bridge \"Containers\": { \"04d3d4540a17d21ea7db83779e8de1716e6e3a4122e1f2c2f66c60d25a094656\": { \"Name\": \"stoic_wozniak\", \"EndpointID\": \"2ed11c55d850ed3cc4eec221f705dad2a9679a016934cd991cd96da86d2dfcbd\", \"MacAddress\": \"02:42:ac:11:00:02\", \"IPv4Address\": \"172.17.0.2/16\", \"IPv6Address\": \"\" }, \"76ff8f56718ae5244eabe03092f7a0227aa2e42249bf2ab1c8f91a9faf76c715\": { \"Name\": \"admiring_driscoll\", \"EndpointID\": \"684e6a570db46dadd9bdf53bf383ea069fa626fc16b7ab26c11e604415c00b25\", \"MacAddress\": \"02:42:ac:11:00:03\", \"IPv4Address\": \"172.17.0.3/16\", \"IPv6Address\": \"\" } } in second terminal $ c ping google.ch $ c ping 172 .17.0.2 Be aware that ip address maybe attributed randomly by Docker. To use name on default network, use --name and --link (deprecated) options on run command then you may ping by name instead of ip address (only for default bridge network). in first terminal $ docker run --rm --name alpine1 -it alpine sh in second terminal $ docker run --rm --link alpine1 -it alpine sh $ c ping alpine1","title":"Bridge"},{"location":"docker/7-docker-network/#create-bridge","text":"Create a network, make two containers communicate through it and use container name instead of ip addresses. Create network, default driver = bridge $ docker network create mynet $ docker network ls NETWORK ID NAME DRIVER SCOPE 266b7ae5e9d7 bridge bridge local ada5f50a5c41 host host local 0a85e3670d62 mynet bridge local 119b5f46e464 none null local Use network with a named (important for name resolution over the network) container $ docker run --rm --network mynet --name server1 -d alpine ping google.ch $ docker inspect mynet \"Containers\": { \"249b952ab5db2ac4f3077e1a7fb89582eedaa02c236a92a7e15fc5cee73d3292\": { \"Name\": \"server1\", \"EndpointID\": \"48284c2119158a9ebf9d67b3eee0c74c58ede079ba173b769f5b791f2f507abb\", \"MacAddress\": \"02:42:ac:12:00:02\", \"IPv4Address\": \"172.18.0.2/16\", \"IPv6Address\": \"\" } } $ docker run --rm -it --network mynet --name server2 alpine sh $ c ping server1 PING server1 (172.18.0.2): 56 data bytes 64 bytes from 172.18.0.2: seq=0 ttl=64 time=0.079 ms 64 bytes from 172.18.0.2: seq=1 ttl=64 time=0.166 ms From another terminal $ docker exec -it server1 sh $ c ping server2 PING server2 (172.18.0.3): 56 data bytes 64 bytes from 172.18.0.3: seq=0 ttl=64 time=0.125 ms 64 bytes from 172.18.0.3: seq=1 ttl=64 time=0.496 ms Remove network $ docker network rm mynet Remove all network at once $ docker network prune","title":"Create bridge"},{"location":"docker/7-docker-network/#connect-a-nodejs-server-with-mongodb","text":"Goal: - display a counter in a browser at 'localhost' address. Architecture: - Image - MongoDB - Image - Node.js - Volumes - mydb { count: x } - Container - server - Container - db - Network - mynet - Port 80 open to listen to request (count++) from a browser at 'localhost' address","title":"Connect a Node.js server with MongoDB"},{"location":"docker/7-docker-network/#mongodb","text":"Volume and container $ docker volume create mydb $ docker run --name db --mount type = volume,source = mydb,target = /data/db -d mongo Network and connect (and disconnect from default bridge) $ docker network create mynet $ docker network connect mynet db $ docker network disconnect bridge db Create db and collection to handle and initialize the counter $ docker exec -it db sh $ c mongo > use test switched to db test > db.count.insertOne({count:0}) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"61cc2517094e32ba7f98bb31\") } > db.count.find() { \"_id\" : ObjectId(\"61cc2517094e32ba7f98bb31\"), \"count\" : 0 } > exit bye $ c exit","title":"MongoDB"},{"location":"docker/7-docker-network/#node-server-development","text":"","title":"Node server Development"},{"location":"docker/7-docker-network/#development-environnement-setup","text":"First step, application development with bind mount. We should use the mongo javascript driver in our application to allow connection to the db. In 'node-server' folder. File 'package.json' add mongo dependencies (browse for \"npm mongodb\" -> MongoDB NodeJS Driver, to check version) { \"dependencies\" : { \"express\" : \"^4.17.1\" , \"mongodb\" : \"^3.6.2\" , \"nodemon\" : \"^2.0.6\" , \"console-stamp\" : \"^3.0.3\" } } \"console-stamp\" is to add timestamp in logs ^version \u201cCompatible with version\u201d, will update you to all future minor/patch versions, without incrementing the major version. ^2.3.4 will use releases from 2.3.4 to <3.0.0. Dockerfile FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"nodemon\", \"-L\", \"src/app.js\"] Build image in 'node-server' folder $ docker build -t node-server . Application is in js file 'node-server/src/app.js' const express = require ( \"express\" ); const app = express (); app . get ( \"*\" , ( req , res ) => { res . status ( 200 ). json ( \"Hello, world!\" ); }); app . listen ( 80 ); To develop application use a bind mount $ docker run --name server --mount type = bind,source = \" $( pwd ) \" /src,target = /app/src -p 80 :80 --network mynet node-server Browse to 'localhost'. Observe live change availability by editing \"Hello, world!\" response in 'app.js' file and refreshing 'localhost' page in internet browser. Check that port '80' is published for 'server' container $ docker container port server 80/tcp -> 0.0.0.0:80","title":"Development Environnement setup"},{"location":"docker/7-docker-network/#development-server-configuration","text":"Modify 'app.js' file as follow require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; MongoClient . connect ( 'mongodb://db' , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNEXION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 );","title":"Development Server configuration"},{"location":"docker/7-docker-network/#node-server-production","text":"Rebuild node server image with released app.js in it (above development has been erased by bind mount) In 'node-server' folder $ docker build -t node-server . If 'node-server' container is still running, remove it and then $ docker run --name server --network mynet -d -p 80 :80 node-server","title":"Node server Production"},{"location":"docker/7-docker-network/#host","text":"C.f. Bridge section for initial setup. Reset 'app.js' to const express = require ( \"express\" ); const app = express (); app . get ( \"*\" , ( req , res ) => { res . status ( 200 ). json ( \"Hello, world!\" ); }); app . listen ( 80 ); Rebuild 'node-server' image $ docker build -t node-server . Relaunch server but on local network, this time, no need to publish port $ docker run --network host node-server Do not work on WSL (Windows Subsystem for Linux), neither MacOs. To not use any network $ docker run --network none node-server","title":"Host"},{"location":"docker/8-compose-use/","text":"Compose - 08 - Use Docker Compose - Use Introduction Application = Container Web server + Container Database Setup: Ports Volumes Network Environment Docker Compose, talking about service. One Application = (is composed of) Many Services (containers that communicate with each others). Docker Compose is a CLI that read 'docker-compose.yml' file. First, install Docker Compose and check installation and version by typing in a terminal: docker-compose version First use 'docker-compose ..' command(s) always refer to folder from where command is launched and context of 'docker-compose.yml' file contained in folder. Yaml format configuration file. Yaml syntax is based on an indented key value format. touch docker-compose.yml First, mention version to use to ensure retro-compatibility. To determine which version to specify in 'docker-compose.yml' file header, refer to docker engine version that run on your host machine: docker version ... Server: Docker Engine - Community Engine: Version: 20.10.11 ... Then refer to documentation Compose file - Reference and guidelines Second, specify service(s). docker-compose.yml: version : '3.8' services : myalpine : image : alpine docker-compose up Alternative to go straight in service's container: docker-compose run myapline In another console: docker-compose ps docker-compose ps -a docker-compose down Particularity of 'docker-compose down' command is to suppress (don't just stop) all container and network that was launched by previous 'docker-compose up' command. Anonymous volumes are never reused by Docker Compose. It launches new ones each time if declared in configuration. Default command is the one defined in image, for 'alpine' it's '/bin/sh'. To overwrite default command, specify it in 'docker-compose.yml' file: version : '3.8' services : myalpine : image : alpine command : ls Or by adding command directly after service name in run command: docker-compose run myalpine ls Or with entry point in exec form (instead of shell) in 'docker-compose.yml' file: version : '3.8' services : myalpine : image : alpine entrypoint : [ \"ls\" ] Or 'command: [\"ls\"]' instead of 'entrypoint: [\"ls\"]' Custom image touch Dockerfile Dockerfile: FROM alpine CMD [\"/bin/sh\"] docker-compose.yml version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : . docker-compose build Have a look to VS Code Docker plugin to have a synthetic view of all Docker ecosystem components, containers, images, network, etc. Context and Dockerfile Specify a context and Dockerfile: mkdir backend cp Dockerfile backend/DockerfileBackend docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend Arguments Passing arguments, e.g. create a folder at build, 'Dockerfile' receive args from 'docker-compose.yml'. DockerfileBackend: FROM alpine ARG FOLDER RUN mkdir $FOLDER CMD [\"/bin/sh\"] docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : - FOLDER=test Note the 'arg' indentation with '-' for an array of values (yaml syntax). test: docker-compose build docker-compose run b ls .. test .. Instead of list (- FOLDER=), e.g. for 'args' you may also use an object instead (FOLDER:). docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder test: docker-compose build docker-compose run b ls .. myfolder .. Labels docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : - FOLDER=test labels : - EMAIL=toto@test.com test: docker-compose build docker image inspect compose_b:latest | grep EMAIL \"EMAIL\": \"toto@test.com\" Ports docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 Volumes Bind mkdir data touch data/hello.txt DockerfileBackend.yml: FROM alpine ARG FOLDER WORKDIR /app RUN mkdir $FOLDER CMD [\"/bin/sh\"] docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data test: docker-compose build docker-compose run b cd data ls exit Volumes docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume volumes : datavolume : test: docker-compose build docker-compose run b [+] Running 1/0 \u283f Volume \"compose_datavolume\" Created ls data datavolume myfolder exit Volume option external to avoid docker-compose to create volume if it does not exist. docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume volumes : datavolume : external : true Before testing remove previously created volumes. test: docker-compose run b external volume \"\" not found To create anonymous volume, omit source option. docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b ls data datavolume datavolumeanonymous myfolder Docker Compose does not always use the same anonymous volume for a service. Therefore, it is advisable to use: docker-compose down -v to remove it. -v, --volumes volumes, Remove named volumes declared in the volumes section of the Compose file and anonymous volumes attached to containers. Environment Variables from cli docker-compose run b env HOSTNAME=0b9907714155 SHLVL=1 HOME=/root TERM=xterm PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/app Add environnement variable from command line, value from host machine: docker-compose run -e USER b env | grep USER USER=toto By default, by not specifying a value, docker-compose search on host machine environnement variable and if find one that match, pass it (e.g. here with USER that do exist on host machine and has a value). Add environnement variable from command line with specified value: docker-compose run -e USER=tintin b env | grep USER USER=tintin from compose file Without specifying a value (comes from host machine). docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : environment : - USER build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b env | grep USER USER=toto By specifying a value. docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : environment : - USER=tintin build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b env | grep USER USER=tintin from .env file If value of environment variable is not specified, docker compose search for corresponding value in host machine, if not found, search then in '.env' file. .env: NODE_ENV=development docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : environment : - NODE_ENV build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b env | grep NODE_ENV NODE_ENV=development By specifying an environnement file, all variables contained in it will be imported in container. .env: NODE_ENV=development TEST_ENV=test docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : env_file : - .env build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b env | grep _ENV TEST_ENV=test NODE_ENV=development You may have many environnement files. You may specify env file in command line, works only with 'up': docker-compose --env-file ./.env up You may use both 'env_file' and 'environnement' for same service. You may specify compose project name instead of current folder with, e.g. in .env file: COMPOSE_PROJECT_NAME=myproject Network Default By default docker compose create a network with folder name as a prefix, or with value of 'COMPOSE_PROJECT_NAME' key in project's environnement variable: docker-compose up [+] Running 4/4 \u283f Network myproject_default Created .. Note that containers using the network appear in the list (docker network inspect myproject_default) only when they are running. Below, we make a test with ping, note that we use shell form (instead of exec (cause: executable file not found in $PATH: unknown)). docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b b : command : ping a build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose up [+] Running 2/2 \u283f Container myproject_a_1 Recreated 0.2s \u283f Container myproject_b_1 Recreated 0.2s Attaching to a_1, b_1 a_1 | PING b (172.29.0.3): 56 data bytes b_1 | PING a (172.29.0.2): 56 data bytes a_1 | 64 bytes from 172.29.0.3: seq=0 ttl=64 time=188.846 ms b_1 | 64 bytes from 172.29.0.2: seq=0 ttl=64 time=0.152 ms Links Links from a container to another one. docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b b : links : - \"a:containerA\" command : ping containerA build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose up [+] Running 3/2 \u283f Network myproject_default Created 0.0s \u283f Container myproject_a_1 Created 0.8s \u283f Container myproject_b_1 Created 0.1s Attaching to a_1, b_1 b_1 | PING containerA (172.31.0.2): 56 data bytes b_1 | 64 bytes from 172.31.0.2: seq=0 ttl=64 time=0.078 ms b_1 | 64 bytes from 172.31.0.2: seq=1 ttl=64 time=0.190 ms b_1 | 64 bytes from 172.31.0.2: seq=2 ttl=64 time=0.220 ms a_1 | PING b (172.31.0.3): 56 data bytes a_1 | 64 bytes from 172.31.0.3: seq=0 ttl=64 time=0.407 ms b_1 | 64 bytes from 172.31.0.2: seq=3 ttl=64 time=0.305 ms test to ping a and containerA from b: docker-compose up -d docker-compose exec b sh ping a PING a (172.31.0.2): 56 data bytes 64 bytes from 172.31.0.2: seq=0 ttl=64 time=0.181 ms .. ping containerA PING containerA (172.31.0.2): 56 data bytes 64 bytes from 172.31.0.2: seq=0 ttl=64 time=0.287 ms Name Give network a name to replace the default one. docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b b : links : - \"a:containerA\" command : ping containerA build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : networks : default : name : mynetwork test: docker-compose up [+] Running 3/3 \u283f Network mynetwork Created 0.0s \u283f Container myproject_a_1 Created 0.8s \u283f Container myproject_b_1 Created 2.4s Attaching to a_1, b_1 b_1 | PING containerA (192.168.0.2): 56 data bytes b_1 | 64 bytes from 192.168.0.2: seq=0 ttl=64 time=0.081 ms b_1 | 64 bytes from 192.168.0.2: seq=1 ttl=64 time=0.088 ms b_1 | 64 bytes from 192.168.0.2: seq=2 ttl=64 time=0.122 ms a_1 | PING b (192.168.0.3): 56 data bytes a_1 | 64 bytes from 192.168.0.3: seq=0 ttl=64 time=0.339 ms Networks Link container to many networks with adding list in service configuration. docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b b : links : - \"a:containerA\" command : ping containerA build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous networks : - 'othernetwork' volumes : datavolume : networks : default : name : mynetwork test: docker-compose up service \"b\" refers to undefined network othernetwork: invalid compose project Error due to othernetwork missing. We add it to networks section in configuration file and then to services. docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b networks : - 'othernetwork' b : links : - \"a:containerA\" command : ping containerA build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous networks : - 'othernetwork' volumes : datavolume : networks : default : name : mynetwork othernetwork : driver : bridge test: docker-compose up [+] Running 3/3 \u283f Network myproject_othernetwork Created 0.0s \u283f Container myproject_a_1 Created 0.1s \u283f Container myproject_b_1 Created 0.1s Attaching to a_1, b_1 b_1 | PING containerA (192.168.48.2): 56 data bytes b_1 | 64 bytes from 192.168.48.2: seq=0 ttl=64 time=0.122 ms b_1 | 64 bytes from 192.168.48.2: seq=1 ttl=64 time=0.051 ms b_1 | 64 bytes from 192.168.48.2: seq=2 ttl=64 time=0.196 ms a_1 | PING b (192.168.48.3): 56 data bytes a_1 | 64 bytes from 192.168.48.3: seq=0 ttl=64 time=0.252 ms b_1 | 64 bytes from 192.168.48.2: seq=3 ttl=64 time=0.200 ms Sample application Node.js application that increment a counter in a MongoDB. MongoDB We provide volume to handle db data, so, preamble is to \"manually\" create the needed volume: docker volume create mydb docker-compose.yml: version : '3.8' services : db : image : mongo volumes : - type : volume source : mydb target : /data/db volumes : mydb : external : true We run db individually to initialize it: docker-compose run -d db 39cf.. docker container exec -it 39cf sh mongo > use test > db.count.insertOne({ count: 0 }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"61d1a03ac9a303a408034aca\") } > db.count.findOne() { \"_id\" : ObjectId(\"61d1a03ac9a303a408034aca\"), \"count\" : 0 } > exit bye exit docker container stop 39cf In MongoDB, volume that contain the database may not be mounted anywhere. MongoDB will specifically search for database in '/data/db' folder. No need to open specific port(s) for containers that run on same network. By default all ports are available for containers that run on same network. Node.js Dockerfile: FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"nodemon\", \"-L\", \"src/app.js\"] docker-compose.yml: version : '3.8' services : db : image : mongo volumes : - type : volume source : mydb target : /data/db server : build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src volumes : mydb : external : true \\src\\app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; MongoClient . connect ( 'mongodb://db' , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNEXION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); In a terminal: docker-compose up .. server_1 | [02.01.2022 17:10.30.697] [LOG] CONNEXION DB OK! .. In a browser: http://localhost/ Authentication We add authentication through environnement variable to MongoDB. Clear docker environnement and recreate database volume: docker container prune docker volume prune docker volume create mydb Have a look to MongoDB official image on Docker Hub What's interesting us here is to set the two following environnement variables: MONGO_INITDB_ROOT_USERNAME MONGO_INITDB_ROOT_PASSWORD touch .env .env: MONGO_INITDB_ROOT_USERNAME=toto MONGO_INITDB_ROOT_PASSWORD=123 docker-compose.yml: version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src volumes : mydb : external : true In a terminal, set up db with authenticated user and then create a new user 'tintin' with password '456' and role 'readWrite' on db 'test': docker-compose run -d db bad88.. docker exec -it bad88 sh mongo > use test > db.count.insertOne({ count: 0 }) .. error.. command insert requires authentication.. > use admin > db.auth({ user: 'toto', pwd: '123' }) 1 > use test > db.count.insertOne({ count: 0 }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"61d1e40276df2cfd1b903a8f\") } > db.count.findOne() { \"_id\" : ObjectId(\"61d1e40276df2cfd1b903a8f\"), \"count\" : 0 } > use admin switched to db admin > db.createUser({ user: 'tintin', pwd: '456', roles: [{ role: 'readWrite', db: 'test' }] }) Successfully added user: { \"user\" : \"tintin\", \"roles\" : [ { \"role\" : \"readWrite\", \"db\" : \"test\" } ] } > exit bye exit docker stop bad88 Check that connection to db is OK ('CONNECTION DB OK!' in logs), but we cannot access data (trying to refresh 'localhost' in Internet browser), due to unauthenticated connection: docker-compose up .. server_1 | [02.01.2022 19:47.14.606] [LOG] CONNECTION DB OK! .. server_1 | MongoError: command findAndModify requires authentication .. We can authenticate with many different ways. By specifying (hard coded) user password directly in 'app.js' file ('mongodb://tintin:456@db'). We also add a 'console.log(process)' to have environnement variables in logs. app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; console . log ( process ) // to have environnement variables in logs MongoClient . connect ( 'mongodb://tintin:456@db' , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); server_1 logs (to see environnement variables): .. server_1 | env: { server_1 | NODE_VERSION: '17.3.0', server_1 | HOSTNAME: 'fbdb8ee68893', server_1 | YARN_VERSION: '1.22.17', server_1 | SHLVL: '1', server_1 | HOME: '/root', server_1 | PATH: '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/app/node_modules/.bin', server_1 | PWD: '/app' server_1 | } .. By refreshing Internet browser's page at localhost address, we may observer that application is now running fine. Now we stop application by hitting 'Ctrl+c'. Below is described a second manner (more secure, therefor, advised to use) to authenticate to 'db' service from 'sever' service through environnement variables. .env: MONGO_INITDB_ROOT_USERNAME=toto MONGO_INITDB_ROOT_PASSWORD=123 MONGO_USER_NAME=tintin MONGO_USER_PASSWORD=456 docker-compose.yml: version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src volumes : mydb : external : true app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( `mongodb:// ${ process . env . MONGO_USER_NAME } : ${ process . env . MONGO_USER_PASSWORD } @db` , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); ! Be aware of literal evaluation with use of ` character to surround mongodb connection URL instead of ' character like before. To test, type below command in a terminal and refresh Internet browser's page at 'localhost' address: docker-compose up .. server_1 | [02.01.2022 20:32.25.069] [LOG] { server_1 | NODE_VERSION: '17.3.0', server_1 | HOSTNAME: 'e9e18205ee72', server_1 | YARN_VERSION: '1.22.17', server_1 | SHLVL: '1', server_1 | HOME: '/root', server_1 | PATH: '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/app/node_modules/.bin', server_1 | MONGO_USER_PASSWORD: '456', server_1 | PWD: '/app', server_1 | MONGO_USER_NAME: 'tintin' server_1 | } .. Depends and Restart Depends Specify containers up priority order (depends_on). E.g. in our application we want that server (Node.js application) start only once database (MongoDB) container is up. docker-compose.yml: version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db volumes : mydb : external : true In a terminal, type below command and you may observe, in logs, that, first, we have database logs, then server logs: docker-compose up Attaching to db_1, server_1 db_1 | .. db_1 | .. .. server_1 | .. server_1 | .. .. Restart restart: \"no\" (between quotes because without it has a yaml signification), default value, never restart automatically. always restart if container stop from inside or Docker daemon restart, but not with a 'docker-compose stop' command. on-failure restart container only on quit with error code. unless-stopped always restart unless stopped manually with a 'docker-compose stop' command. To test it we add a '/err' route to the server to exit the process (process.exit(errorCode)). app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( `mongodb:// ${ process . env . MONGO_USER_NAME } : ${ process . env . MONGO_USER_PASSWORD } @db` , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/err' , ( req , res ) => { process . exit ( 0 ); }); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); For testing purpose (different restart modes) avoid using nodemon. Dockerfile: FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"node\", \"src/app.js\"] Terminal: docker-compose down docker-compose build --no-cache no docker-compose.yml (restart: \"no\"): version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db restart : \"no\" volumes : mydb : external : true Test: docker-compose down docker-compose up Navigate in Internet browser to: http://localhost/err You may observe in logs: .. server_1 exited with code 0 .. In another terminal: docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS 63c2040a78c6 node-server_server \"docker-entrypoint.s\u2026\" About a minute ago Exited (0) About a minute ago 2d8b795bf1ed mongo \"docker-entrypoint.s\u2026\" 7 minutes ago Up About a minute And server isn't available anymore. always docker-compose.yml (restart: always): version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db restart : always volumes : mydb : external : true Terminal: docker-compose up In an Internet browser, navigate alternatively to following addresses and observe in logs server exit and restart automatically: http://localhost/ http://localhost/err Terminal: .. server_1 | [03.01.2022 16:24.28.780] [LOG] CONNECTION DB OK! .. server_1 exited with code 0 .. server_1 | [03.01.2022 16:27.10.339] [LOG] CONNECTION DB OK! .. In another terminal: docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp If we \"manually\" stop 'server' container from another terminal with below command: docker-compose stop server And then restart the Docker daemon, we may observe the 'server' that has restart automatically due to his 'always' restart policy and 'db' is down because, for now, he hasn't any restart policy defined in docker-compose yaml configuration file. on-failure docker-compose.yml (restart: on-failure): version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db restart : on-failure volumes : mydb : external : true Terminal: docker-compose down docker-compose up -d docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp Browse to: http://localhost/err In app.js exit code is '0' = no error, therefore container does not restart automatically: docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server exited (0) Modify exit code of 'err' route to app.js with '1', app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( `mongodb:// ${ process . env . MONGO_USER_NAME } : ${ process . env . MONGO_USER_PASSWORD } @db` , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/err' , ( req , res ) => { process . exit ( 1 ); }); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); Terminal: docker-compose down docker-compose up -d docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp Browse to: http://localhost/err In app.js exit code is '1' = error, therefore container restart automatically: docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp unless-stopped Will always restart except if stopped \"manually\" with 'docker-compose stop server', then restart Docker daemon, then stopped 'server' container will not restart. docker-compose.yml (restart: unless-stopped): version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db restart : unless-stopped volumes : mydb : external : true Terminal: docker-compose down docker-compose up -d docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp docker-compose stop server docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server exited (137) Restart Docker daemon, then: docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db exited (255) 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server exited (137) We may observe that 'server' container hasn't restarted automatically due to fact it has been \"manually\" stopped before Docker daemon restart. Note that you can change the restart configuration of an already running container by doing: docker container update --restart unless-stopped ID Other commands First: docker-compose up -d logs View output from containers: docker-compose logs .. db_1 | .. server_1 | .. .. To follow -f option, to show timestamps -t options: docker-compose logs -f -t 'Ctrl+c' does not stop containers, stop only logs display: Ctrl+c docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp top Display the running processes: docker-compose top misc Stop a container: docker-compose stop server docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server exited (137) Remove a stopped container: docker-compose rm server docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp Remove a running container (+ -f to avoid confirm's need): docker-compose rm -s server Remove anonym volumes belonging to container: docker-compose rm -v db After removing a container, to get it back (+ -d): docker-compose up Port mapping information and entering ip allowed (0.0.0.0 for all entering ip address allowed): docker-compose port server 80 0.0.0.0:80 Means, outside port 80 is mapped to inside port 80 and all ip addresses allowed. config Let see environnement variables replaced with found values and configuration that will then be used to build the stack: docker-compose config services: db: environment: MONGO_INITDB_ROOT_PASSWORD: \"123\" MONGO_INITDB_ROOT_USERNAME: toto image: mongo restart: unless-stopped volumes: - type: volume source: mydb target: /data/db server: build: context: . depends_on: db: condition: service_started environment: MONGO_USER_NAME: tintin MONGO_USER_PASSWORD: \"456\" ports: - mode: ingress target: 80 published: 80 protocol: tcp restart: unless-stopped volumes: - type: bind source: /mnt/c/git/doc/test/docker/node-server/src target: /app/src volumes: mydb: name: mydb external: true pull push pull: get latest images of containers that compose the stack. push: if image has been modified with custom Dockerfile, let us push it to Docker Hub.","title":"Compose - 08 - Use"},{"location":"docker/8-compose-use/#compose-08-use","text":"Docker Compose - Use","title":"Compose - 08 - Use"},{"location":"docker/8-compose-use/#introduction","text":"Application = Container Web server + Container Database Setup: Ports Volumes Network Environment Docker Compose, talking about service. One Application = (is composed of) Many Services (containers that communicate with each others). Docker Compose is a CLI that read 'docker-compose.yml' file. First, install Docker Compose and check installation and version by typing in a terminal: docker-compose version","title":"Introduction"},{"location":"docker/8-compose-use/#first-use","text":"'docker-compose ..' command(s) always refer to folder from where command is launched and context of 'docker-compose.yml' file contained in folder. Yaml format configuration file. Yaml syntax is based on an indented key value format. touch docker-compose.yml First, mention version to use to ensure retro-compatibility. To determine which version to specify in 'docker-compose.yml' file header, refer to docker engine version that run on your host machine: docker version ... Server: Docker Engine - Community Engine: Version: 20.10.11 ... Then refer to documentation Compose file - Reference and guidelines Second, specify service(s). docker-compose.yml: version : '3.8' services : myalpine : image : alpine docker-compose up Alternative to go straight in service's container: docker-compose run myapline In another console: docker-compose ps docker-compose ps -a docker-compose down Particularity of 'docker-compose down' command is to suppress (don't just stop) all container and network that was launched by previous 'docker-compose up' command. Anonymous volumes are never reused by Docker Compose. It launches new ones each time if declared in configuration. Default command is the one defined in image, for 'alpine' it's '/bin/sh'. To overwrite default command, specify it in 'docker-compose.yml' file: version : '3.8' services : myalpine : image : alpine command : ls Or by adding command directly after service name in run command: docker-compose run myalpine ls Or with entry point in exec form (instead of shell) in 'docker-compose.yml' file: version : '3.8' services : myalpine : image : alpine entrypoint : [ \"ls\" ] Or 'command: [\"ls\"]' instead of 'entrypoint: [\"ls\"]'","title":"First use"},{"location":"docker/8-compose-use/#custom-image","text":"touch Dockerfile Dockerfile: FROM alpine CMD [\"/bin/sh\"] docker-compose.yml version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : . docker-compose build Have a look to VS Code Docker plugin to have a synthetic view of all Docker ecosystem components, containers, images, network, etc.","title":"Custom image"},{"location":"docker/8-compose-use/#context-and-dockerfile","text":"Specify a context and Dockerfile: mkdir backend cp Dockerfile backend/DockerfileBackend docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend","title":"Context and Dockerfile"},{"location":"docker/8-compose-use/#arguments","text":"Passing arguments, e.g. create a folder at build, 'Dockerfile' receive args from 'docker-compose.yml'. DockerfileBackend: FROM alpine ARG FOLDER RUN mkdir $FOLDER CMD [\"/bin/sh\"] docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : - FOLDER=test Note the 'arg' indentation with '-' for an array of values (yaml syntax). test: docker-compose build docker-compose run b ls .. test .. Instead of list (- FOLDER=), e.g. for 'args' you may also use an object instead (FOLDER:). docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder test: docker-compose build docker-compose run b ls .. myfolder ..","title":"Arguments"},{"location":"docker/8-compose-use/#labels","text":"docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : - FOLDER=test labels : - EMAIL=toto@test.com test: docker-compose build docker image inspect compose_b:latest | grep EMAIL \"EMAIL\": \"toto@test.com\"","title":"Labels"},{"location":"docker/8-compose-use/#ports","text":"docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80","title":"Ports"},{"location":"docker/8-compose-use/#volumes","text":"","title":"Volumes"},{"location":"docker/8-compose-use/#bind","text":"mkdir data touch data/hello.txt DockerfileBackend.yml: FROM alpine ARG FOLDER WORKDIR /app RUN mkdir $FOLDER CMD [\"/bin/sh\"] docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data test: docker-compose build docker-compose run b cd data ls exit","title":"Bind"},{"location":"docker/8-compose-use/#volumes_1","text":"docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume volumes : datavolume : test: docker-compose build docker-compose run b [+] Running 1/0 \u283f Volume \"compose_datavolume\" Created ls data datavolume myfolder exit Volume option external to avoid docker-compose to create volume if it does not exist. docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume volumes : datavolume : external : true Before testing remove previously created volumes. test: docker-compose run b external volume \"\" not found To create anonymous volume, omit source option. docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b ls data datavolume datavolumeanonymous myfolder Docker Compose does not always use the same anonymous volume for a service. Therefore, it is advisable to use: docker-compose down -v to remove it. -v, --volumes volumes, Remove named volumes declared in the volumes section of the Compose file and anonymous volumes attached to containers.","title":"Volumes"},{"location":"docker/8-compose-use/#environment-variables","text":"","title":"Environment Variables"},{"location":"docker/8-compose-use/#from-cli","text":"docker-compose run b env HOSTNAME=0b9907714155 SHLVL=1 HOME=/root TERM=xterm PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PWD=/app Add environnement variable from command line, value from host machine: docker-compose run -e USER b env | grep USER USER=toto By default, by not specifying a value, docker-compose search on host machine environnement variable and if find one that match, pass it (e.g. here with USER that do exist on host machine and has a value). Add environnement variable from command line with specified value: docker-compose run -e USER=tintin b env | grep USER USER=tintin","title":"from cli"},{"location":"docker/8-compose-use/#from-compose-file","text":"Without specifying a value (comes from host machine). docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : environment : - USER build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b env | grep USER USER=toto By specifying a value. docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : environment : - USER=tintin build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b env | grep USER USER=tintin","title":"from compose file"},{"location":"docker/8-compose-use/#from-env-file","text":"If value of environment variable is not specified, docker compose search for corresponding value in host machine, if not found, search then in '.env' file. .env: NODE_ENV=development docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : environment : - NODE_ENV build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b env | grep NODE_ENV NODE_ENV=development By specifying an environnement file, all variables contained in it will be imported in container. .env: NODE_ENV=development TEST_ENV=test docker-compose.yml: version : '3.8' services : a : image : alpine command : [ \"ls\" ] b : env_file : - .env build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose build docker-compose run b env | grep _ENV TEST_ENV=test NODE_ENV=development You may have many environnement files. You may specify env file in command line, works only with 'up': docker-compose --env-file ./.env up You may use both 'env_file' and 'environnement' for same service. You may specify compose project name instead of current folder with, e.g. in .env file: COMPOSE_PROJECT_NAME=myproject","title":"from .env file"},{"location":"docker/8-compose-use/#network","text":"","title":"Network"},{"location":"docker/8-compose-use/#default","text":"By default docker compose create a network with folder name as a prefix, or with value of 'COMPOSE_PROJECT_NAME' key in project's environnement variable: docker-compose up [+] Running 4/4 \u283f Network myproject_default Created .. Note that containers using the network appear in the list (docker network inspect myproject_default) only when they are running. Below, we make a test with ping, note that we use shell form (instead of exec (cause: executable file not found in $PATH: unknown)). docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b b : command : ping a build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose up [+] Running 2/2 \u283f Container myproject_a_1 Recreated 0.2s \u283f Container myproject_b_1 Recreated 0.2s Attaching to a_1, b_1 a_1 | PING b (172.29.0.3): 56 data bytes b_1 | PING a (172.29.0.2): 56 data bytes a_1 | 64 bytes from 172.29.0.3: seq=0 ttl=64 time=188.846 ms b_1 | 64 bytes from 172.29.0.2: seq=0 ttl=64 time=0.152 ms","title":"Default"},{"location":"docker/8-compose-use/#links","text":"Links from a container to another one. docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b b : links : - \"a:containerA\" command : ping containerA build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : test: docker-compose up [+] Running 3/2 \u283f Network myproject_default Created 0.0s \u283f Container myproject_a_1 Created 0.8s \u283f Container myproject_b_1 Created 0.1s Attaching to a_1, b_1 b_1 | PING containerA (172.31.0.2): 56 data bytes b_1 | 64 bytes from 172.31.0.2: seq=0 ttl=64 time=0.078 ms b_1 | 64 bytes from 172.31.0.2: seq=1 ttl=64 time=0.190 ms b_1 | 64 bytes from 172.31.0.2: seq=2 ttl=64 time=0.220 ms a_1 | PING b (172.31.0.3): 56 data bytes a_1 | 64 bytes from 172.31.0.3: seq=0 ttl=64 time=0.407 ms b_1 | 64 bytes from 172.31.0.2: seq=3 ttl=64 time=0.305 ms test to ping a and containerA from b: docker-compose up -d docker-compose exec b sh ping a PING a (172.31.0.2): 56 data bytes 64 bytes from 172.31.0.2: seq=0 ttl=64 time=0.181 ms .. ping containerA PING containerA (172.31.0.2): 56 data bytes 64 bytes from 172.31.0.2: seq=0 ttl=64 time=0.287 ms","title":"Links"},{"location":"docker/8-compose-use/#name","text":"Give network a name to replace the default one. docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b b : links : - \"a:containerA\" command : ping containerA build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous volumes : datavolume : networks : default : name : mynetwork test: docker-compose up [+] Running 3/3 \u283f Network mynetwork Created 0.0s \u283f Container myproject_a_1 Created 0.8s \u283f Container myproject_b_1 Created 2.4s Attaching to a_1, b_1 b_1 | PING containerA (192.168.0.2): 56 data bytes b_1 | 64 bytes from 192.168.0.2: seq=0 ttl=64 time=0.081 ms b_1 | 64 bytes from 192.168.0.2: seq=1 ttl=64 time=0.088 ms b_1 | 64 bytes from 192.168.0.2: seq=2 ttl=64 time=0.122 ms a_1 | PING b (192.168.0.3): 56 data bytes a_1 | 64 bytes from 192.168.0.3: seq=0 ttl=64 time=0.339 ms","title":"Name"},{"location":"docker/8-compose-use/#networks","text":"Link container to many networks with adding list in service configuration. docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b b : links : - \"a:containerA\" command : ping containerA build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous networks : - 'othernetwork' volumes : datavolume : networks : default : name : mynetwork test: docker-compose up service \"b\" refers to undefined network othernetwork: invalid compose project Error due to othernetwork missing. We add it to networks section in configuration file and then to services. docker-compose.yml: version : '3.8' services : a : image : alpine command : ping b networks : - 'othernetwork' b : links : - \"a:containerA\" command : ping containerA build : context : ./backend dockerfile : DockerfileBackend args : FOLDER : myfolder labels : - EMAIL=toto@test.com ports : - 80:80 volumes : - type : bind source : ./data target : /app/data - type : volume source : datavolume target : /app/datavolume - type : volume target : /app/datavolumeanonymous networks : - 'othernetwork' volumes : datavolume : networks : default : name : mynetwork othernetwork : driver : bridge test: docker-compose up [+] Running 3/3 \u283f Network myproject_othernetwork Created 0.0s \u283f Container myproject_a_1 Created 0.1s \u283f Container myproject_b_1 Created 0.1s Attaching to a_1, b_1 b_1 | PING containerA (192.168.48.2): 56 data bytes b_1 | 64 bytes from 192.168.48.2: seq=0 ttl=64 time=0.122 ms b_1 | 64 bytes from 192.168.48.2: seq=1 ttl=64 time=0.051 ms b_1 | 64 bytes from 192.168.48.2: seq=2 ttl=64 time=0.196 ms a_1 | PING b (192.168.48.3): 56 data bytes a_1 | 64 bytes from 192.168.48.3: seq=0 ttl=64 time=0.252 ms b_1 | 64 bytes from 192.168.48.2: seq=3 ttl=64 time=0.200 ms","title":"Networks"},{"location":"docker/8-compose-use/#sample-application","text":"Node.js application that increment a counter in a MongoDB.","title":"Sample application"},{"location":"docker/8-compose-use/#mongodb","text":"We provide volume to handle db data, so, preamble is to \"manually\" create the needed volume: docker volume create mydb docker-compose.yml: version : '3.8' services : db : image : mongo volumes : - type : volume source : mydb target : /data/db volumes : mydb : external : true We run db individually to initialize it: docker-compose run -d db 39cf.. docker container exec -it 39cf sh mongo > use test > db.count.insertOne({ count: 0 }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"61d1a03ac9a303a408034aca\") } > db.count.findOne() { \"_id\" : ObjectId(\"61d1a03ac9a303a408034aca\"), \"count\" : 0 } > exit bye exit docker container stop 39cf In MongoDB, volume that contain the database may not be mounted anywhere. MongoDB will specifically search for database in '/data/db' folder. No need to open specific port(s) for containers that run on same network. By default all ports are available for containers that run on same network.","title":"MongoDB"},{"location":"docker/8-compose-use/#nodejs","text":"Dockerfile: FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"nodemon\", \"-L\", \"src/app.js\"] docker-compose.yml: version : '3.8' services : db : image : mongo volumes : - type : volume source : mydb target : /data/db server : build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src volumes : mydb : external : true \\src\\app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; MongoClient . connect ( 'mongodb://db' , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNEXION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); In a terminal: docker-compose up .. server_1 | [02.01.2022 17:10.30.697] [LOG] CONNEXION DB OK! .. In a browser: http://localhost/","title":"Node.js"},{"location":"docker/8-compose-use/#authentication","text":"We add authentication through environnement variable to MongoDB. Clear docker environnement and recreate database volume: docker container prune docker volume prune docker volume create mydb Have a look to MongoDB official image on Docker Hub What's interesting us here is to set the two following environnement variables: MONGO_INITDB_ROOT_USERNAME MONGO_INITDB_ROOT_PASSWORD touch .env .env: MONGO_INITDB_ROOT_USERNAME=toto MONGO_INITDB_ROOT_PASSWORD=123 docker-compose.yml: version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src volumes : mydb : external : true In a terminal, set up db with authenticated user and then create a new user 'tintin' with password '456' and role 'readWrite' on db 'test': docker-compose run -d db bad88.. docker exec -it bad88 sh mongo > use test > db.count.insertOne({ count: 0 }) .. error.. command insert requires authentication.. > use admin > db.auth({ user: 'toto', pwd: '123' }) 1 > use test > db.count.insertOne({ count: 0 }) { \"acknowledged\" : true, \"insertedId\" : ObjectId(\"61d1e40276df2cfd1b903a8f\") } > db.count.findOne() { \"_id\" : ObjectId(\"61d1e40276df2cfd1b903a8f\"), \"count\" : 0 } > use admin switched to db admin > db.createUser({ user: 'tintin', pwd: '456', roles: [{ role: 'readWrite', db: 'test' }] }) Successfully added user: { \"user\" : \"tintin\", \"roles\" : [ { \"role\" : \"readWrite\", \"db\" : \"test\" } ] } > exit bye exit docker stop bad88 Check that connection to db is OK ('CONNECTION DB OK!' in logs), but we cannot access data (trying to refresh 'localhost' in Internet browser), due to unauthenticated connection: docker-compose up .. server_1 | [02.01.2022 19:47.14.606] [LOG] CONNECTION DB OK! .. server_1 | MongoError: command findAndModify requires authentication .. We can authenticate with many different ways. By specifying (hard coded) user password directly in 'app.js' file ('mongodb://tintin:456@db'). We also add a 'console.log(process)' to have environnement variables in logs. app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; console . log ( process ) // to have environnement variables in logs MongoClient . connect ( 'mongodb://tintin:456@db' , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); server_1 logs (to see environnement variables): .. server_1 | env: { server_1 | NODE_VERSION: '17.3.0', server_1 | HOSTNAME: 'fbdb8ee68893', server_1 | YARN_VERSION: '1.22.17', server_1 | SHLVL: '1', server_1 | HOME: '/root', server_1 | PATH: '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/app/node_modules/.bin', server_1 | PWD: '/app' server_1 | } .. By refreshing Internet browser's page at localhost address, we may observer that application is now running fine. Now we stop application by hitting 'Ctrl+c'. Below is described a second manner (more secure, therefor, advised to use) to authenticate to 'db' service from 'sever' service through environnement variables. .env: MONGO_INITDB_ROOT_USERNAME=toto MONGO_INITDB_ROOT_PASSWORD=123 MONGO_USER_NAME=tintin MONGO_USER_PASSWORD=456 docker-compose.yml: version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src volumes : mydb : external : true app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( `mongodb:// ${ process . env . MONGO_USER_NAME } : ${ process . env . MONGO_USER_PASSWORD } @db` , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); ! Be aware of literal evaluation with use of ` character to surround mongodb connection URL instead of ' character like before. To test, type below command in a terminal and refresh Internet browser's page at 'localhost' address: docker-compose up .. server_1 | [02.01.2022 20:32.25.069] [LOG] { server_1 | NODE_VERSION: '17.3.0', server_1 | HOSTNAME: 'e9e18205ee72', server_1 | YARN_VERSION: '1.22.17', server_1 | SHLVL: '1', server_1 | HOME: '/root', server_1 | PATH: '/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/app/node_modules/.bin', server_1 | MONGO_USER_PASSWORD: '456', server_1 | PWD: '/app', server_1 | MONGO_USER_NAME: 'tintin' server_1 | } ..","title":"Authentication"},{"location":"docker/8-compose-use/#depends-and-restart","text":"","title":"Depends and Restart"},{"location":"docker/8-compose-use/#depends","text":"Specify containers up priority order (depends_on). E.g. in our application we want that server (Node.js application) start only once database (MongoDB) container is up. docker-compose.yml: version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db volumes : mydb : external : true In a terminal, type below command and you may observe, in logs, that, first, we have database logs, then server logs: docker-compose up Attaching to db_1, server_1 db_1 | .. db_1 | .. .. server_1 | .. server_1 | .. ..","title":"Depends"},{"location":"docker/8-compose-use/#restart","text":"restart: \"no\" (between quotes because without it has a yaml signification), default value, never restart automatically. always restart if container stop from inside or Docker daemon restart, but not with a 'docker-compose stop' command. on-failure restart container only on quit with error code. unless-stopped always restart unless stopped manually with a 'docker-compose stop' command. To test it we add a '/err' route to the server to exit the process (process.exit(errorCode)). app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( `mongodb:// ${ process . env . MONGO_USER_NAME } : ${ process . env . MONGO_USER_PASSWORD } @db` , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/err' , ( req , res ) => { process . exit ( 0 ); }); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); For testing purpose (different restart modes) avoid using nodemon. Dockerfile: FROM node:alpine WORKDIR /app COPY ./package.json . RUN npm install COPY . . ENV PATH=$PATH:/app/node_modules/.bin CMD [\"node\", \"src/app.js\"] Terminal: docker-compose down docker-compose build --no-cache","title":"Restart"},{"location":"docker/8-compose-use/#no","text":"docker-compose.yml (restart: \"no\"): version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db restart : \"no\" volumes : mydb : external : true Test: docker-compose down docker-compose up Navigate in Internet browser to: http://localhost/err You may observe in logs: .. server_1 exited with code 0 .. In another terminal: docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS 63c2040a78c6 node-server_server \"docker-entrypoint.s\u2026\" About a minute ago Exited (0) About a minute ago 2d8b795bf1ed mongo \"docker-entrypoint.s\u2026\" 7 minutes ago Up About a minute And server isn't available anymore.","title":"no"},{"location":"docker/8-compose-use/#always","text":"docker-compose.yml (restart: always): version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db restart : always volumes : mydb : external : true Terminal: docker-compose up In an Internet browser, navigate alternatively to following addresses and observe in logs server exit and restart automatically: http://localhost/ http://localhost/err Terminal: .. server_1 | [03.01.2022 16:24.28.780] [LOG] CONNECTION DB OK! .. server_1 exited with code 0 .. server_1 | [03.01.2022 16:27.10.339] [LOG] CONNECTION DB OK! .. In another terminal: docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp If we \"manually\" stop 'server' container from another terminal with below command: docker-compose stop server And then restart the Docker daemon, we may observe the 'server' that has restart automatically due to his 'always' restart policy and 'db' is down because, for now, he hasn't any restart policy defined in docker-compose yaml configuration file.","title":"always"},{"location":"docker/8-compose-use/#on-failure","text":"docker-compose.yml (restart: on-failure): version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db restart : on-failure volumes : mydb : external : true Terminal: docker-compose down docker-compose up -d docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp Browse to: http://localhost/err In app.js exit code is '0' = no error, therefore container does not restart automatically: docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server exited (0) Modify exit code of 'err' route to app.js with '1', app.js: require ( 'console-stamp' )( console ); // to add timestamp in logs const express = require ( \"express\" ); const MongoClient = require ( 'mongodb' ). MongoClient ; let count ; console . log ( process . env ) // to have environnement variables in logs MongoClient . connect ( `mongodb:// ${ process . env . MONGO_USER_NAME } : ${ process . env . MONGO_USER_PASSWORD } @db` , { useUnifiedTopology : true }, ( err , client ) => { if ( err ) { console . log ( err ); } else { console . log ( 'CONNECTION DB OK!' ); count = client . db ( 'test' ). collection ( \"count\" ); } }); const app = express (); app . get ( '/err' , ( req , res ) => { process . exit ( 1 ); }); app . get ( '/' , ( req , res ) => { console . log ( 'request url: ' + req . url ); count . findOneAndUpdate ({}, { $inc : { count : 1 } }, { returnNewDocument : true }). then (( doc ) => { const value = doc . value ; res . status ( 200 ). json ( value . count ); }) }); app . get ( '*' , ( req , res ) => { res . end (); }); app . listen ( 80 ); Terminal: docker-compose down docker-compose up -d docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp Browse to: http://localhost/err In app.js exit code is '1' = error, therefore container restart automatically: docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp","title":"on-failure"},{"location":"docker/8-compose-use/#unless-stopped","text":"Will always restart except if stopped \"manually\" with 'docker-compose stop server', then restart Docker daemon, then stopped 'server' container will not restart. docker-compose.yml (restart: unless-stopped): version : '3.8' services : db : environment : - MONGO_INITDB_ROOT_USERNAME - MONGO_INITDB_ROOT_PASSWORD image : mongo volumes : - type : volume source : mydb target : /data/db server : environment : - MONGO_USER_NAME - MONGO_USER_PASSWORD build : . ports : - 80:80 volumes : - type : bind source : ./src target : /app/src depends_on : - db restart : unless-stopped volumes : mydb : external : true Terminal: docker-compose down docker-compose up -d docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp docker-compose stop server docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server exited (137) Restart Docker daemon, then: docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db exited (255) 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server exited (137) We may observe that 'server' container hasn't restarted automatically due to fact it has been \"manually\" stopped before Docker daemon restart. Note that you can change the restart configuration of an already running container by doing: docker container update --restart unless-stopped ID","title":"unless-stopped"},{"location":"docker/8-compose-use/#other-commands","text":"First: docker-compose up -d","title":"Other commands"},{"location":"docker/8-compose-use/#logs","text":"View output from containers: docker-compose logs .. db_1 | .. server_1 | .. .. To follow -f option, to show timestamps -t options: docker-compose logs -f -t 'Ctrl+c' does not stop containers, stop only logs display: Ctrl+c docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server running 0.0.0.0:80->80/tcp","title":"logs"},{"location":"docker/8-compose-use/#top","text":"Display the running processes: docker-compose top","title":"top"},{"location":"docker/8-compose-use/#misc","text":"Stop a container: docker-compose stop server docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp node-server_server_1 \"docker-entrypoint.s\u2026\" server exited (137) Remove a stopped container: docker-compose rm server docker-compose ps NAME COMMAND SERVICE STATUS PORTS node-server_db_1 \"docker-entrypoint.s\u2026\" db running 27017/tcp Remove a running container (+ -f to avoid confirm's need): docker-compose rm -s server Remove anonym volumes belonging to container: docker-compose rm -v db After removing a container, to get it back (+ -d): docker-compose up Port mapping information and entering ip allowed (0.0.0.0 for all entering ip address allowed): docker-compose port server 80 0.0.0.0:80 Means, outside port 80 is mapped to inside port 80 and all ip addresses allowed.","title":"misc"},{"location":"docker/8-compose-use/#config","text":"Let see environnement variables replaced with found values and configuration that will then be used to build the stack: docker-compose config services: db: environment: MONGO_INITDB_ROOT_PASSWORD: \"123\" MONGO_INITDB_ROOT_USERNAME: toto image: mongo restart: unless-stopped volumes: - type: volume source: mydb target: /data/db server: build: context: . depends_on: db: condition: service_started environment: MONGO_USER_NAME: tintin MONGO_USER_PASSWORD: \"456\" ports: - mode: ingress target: 80 published: 80 protocol: tcp restart: unless-stopped volumes: - type: bind source: /mnt/c/git/doc/test/docker/node-server/src target: /app/src volumes: mydb: name: mydb external: true","title":"config"},{"location":"docker/8-compose-use/#pull-push","text":"pull: get latest images of containers that compose the stack. push: if image has been modified with custom Dockerfile, let us push it to Docker Hub.","title":"pull push"},{"location":"docker/9-compose-dockerfile/","text":"Compose - 09 - Dockerfile Docker Compose - Dockerfile !WSL2, advised to not use a mounted volume like '/mnt/c/' for handling a project to avoid slowness and live reload issues. Instead, prefer usage of a \"native\" WSL2 folder like for e.g. '/home/user/react-nginx' Dockerfile and Docker Compose to set up a client application composed of: React NGINX Setup of client application project Node 'create-React-app' is a script to install with 'npm' will pre-configure a 'webpack' environnement and give use access to a development server and a test server and a build command for production. Role of 'NGINX' (http server) is to treat all http(s) request and return response from 'React' application to requester. In this chapter we will see two new features: Dockerfile multi staging, defined with many 'from'. Stdin_open and TTY, two new options of Docker Compose. Prerequisite, install on host machine: 'nvm' - Node Version Manager 'Node.js' Set/check installation with: // Setup project root folder mkdir react-nginx cd react-nginx // To install 'nvm' curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash // Check nvm // Get last version nvm ls-remote | tail . . v17.0.1 v17.1.0 v17.2.0 v17.3.0 // Install last version nvm install 17.3.0 node -v v17.3.0 node Welcome to Node.js v17.3.0. Type \".help\" for more information. > (To exit, press Ctrl+C again or Ctrl+D or type .exit) > React Instal 'React' with 'npx' (like npm but executed once with last script release): npx create-react-app client cd client ls README.md node_modules package-lock.json package.json public src npm start . . Compiled successfully! You can now view client in the browser. Local: http://localhost:3000 Browse to: - http://localhost:3000 Test (launch tests contained in react-nginx/client/src/App.test.js): npm run test !! Fail on WSL2 :( Build for production (add a build folder to the project, this is the folder to return via NGINX for client): npm run build All of this just to initialize the project locally on host machine. Dockerized We may now delete the 'node_modules' folder ('node_modules' folder will then be only in container initialized with dependencies through 'npm install' command in 'Dockerfile'): rm -rf node_modules/ Add a 'Dockerfile' in client project folder '/home/user/react-nginx/client': touch Dockerfile Dockerfile: FROM node:alpine WORKDIR /app COPY package.json . RUN npm install COPY . . CMD [\"npm\", \"start\"] Build docker image: docker build -t myreact . docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE myreact latest c4c1b5bb9d21 45 seconds ago 483MB docker run --rm --name react -p 3000:3000 myreact Browse to: - http://localhost:3000 Live reload It's important to launch from a terminal a based VS Code instance from root client application folder to get live reload effect that works: cd .../react-nginx/client code . For development purpose, to automatically propagate local changes to container. Bind mount project folder: docker run --rm --name react -p 3000:3000 --mount type=bind,src=\"$(pwd)\",target=/app myreact !! => FAIL!! Why? Because when bind mount it crush all what was contained in container '/app' folder with local content and in local there isn't anymore 'node_modules' folder. To avoid this unwanted behavior and keep 'node_modules' in container folder not erased by bind mount (also needed for live reload feature), we bind an anonymous volume targeted on remote container '/app/node_modules' folder. Also, to avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder we modify 'Dockerfile' as follow: FROM node:alpine WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . CMD [\"npm\", \"start\"] Bind mount project folder + anonymous volume with '/app/node_modules' as target: docker run --rm --name react -p 3000:3000 --mount type=bind,src=\"$(pwd)\",target=/app --mount type=volume,target=/app/node_modules myreact It's advised to run with '--rm' option when using anonymous volume to suppress it automatically on stop in addition to container suppression. Test live reload by changing text '.. save to reload.' with e.g. 'Hello, world!\" in 'src/App.js' and then observe live effect at http://localhost:3000 in an Internet browser. Set up Docker Compose docker-compose.yml version : \"3.8\" services : client : build : . ports : - 3000:3000 volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules Reset Docker: docker system prune -a docker volume prune Start service: docker-compose up You may observe live reload working by changing 'App.js' content and 'localhost:3000' changing accordingly. Test during developpement Note: webpack is an open-source JavaScript module bundler (wikipedia). TDD (Test Driven Development). In a devellopement process it's mostly advised to continuously test in order to check that we don't brake anything. To achieve this, we lauch two terminals in prallel, one with webpack for running application and this other one for automatic testing purpose. It's done with same container image and duplicate services in Docker Compose configuration file, only thing that will change for second duplicated service is that we override command from Dockerfile in Docker Compose configuration file to launch test server and remove also port mapping which is useless for tests. docker-compose.yml: version : \"3.8\" services : client : build : . ports : - 3000:3000 volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules test : build : . command : [ \"npm\" , \"run\" , \"test\" ] volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules docker-compose up --build In logs we may observe: . . test_1 | Tests: 1 passed, 1 total . . If we duplicate one exsisting test in '.../src/App.test.js' file, we may observe live change in logs: . . test_1 | Tests: 2 passed, 2 total . . To interact with test server we need a second terminal. If we want to send command in test container when attaching to it we need to add two options to the test service in Docker Compose configuration file, 'stdin_open' and 'tty', both set to 'true'. docker-compose.yml: version : \"3.8\" services : client : build : . ports : - 3000:3000 volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules test : build : . command : [ \"npm\" , \"run\" , \"test\" ] volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules stdin_open : true tty : true In first terminal: docker-compose down -v docker-compose up --build In second terminal: docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ca16b44236ad client_test \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute client_test_1 4633b0999f9c client_client \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute 0.0.0.0:3000->3000/tcp client_client_1 docker attach client_test_1 . . Watch Usage \u203a Press f to run only failed tests. \u203a Press o to only run tests related to changed files. \u203a Press q to quit watch mode. \u203a Press p to filter by a filename regex pattern. \u203a Press t to filter by a test name regex pattern. \u203a Press Enter to trigger a test run. We may now send commands to test server in second terminal. Then, shutdown gracefully: docker-compose down -v Production environnement Introduction NGINX return the build. NGINX intercept http request and return by default an html file. Browse docker hub for nginx on official image page to host some simple static content . Simple test: docker run --rm -p 80:80 nginx Browse to localhost In a second terminal: docker exec -it exciting_elion sh cd usr/share/nginx/html ls 50x.html index.html cat index.html <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> . . It's in this folder we gonna copy our application '/usr/share/nginx/html'. For more complex configuration, have a look in '/etc/nginx/conf.d' folder. Multi stage Dockerfile Reuse a preceding builded image in next build. Goal to reach here is to put React's build folder into NGINX image. ../client/Dockerfile.prod FROM node:alpine as buildstage WORKDIR /app COPY package.json . RUN npm install COPY . . RUN npm run build FROM nginx COPY --from=buildstage /app/build /usr/share/nginx/html EXPOSE 80 After second FROM above, --from=buildstage refer to first stage of image build (npm run build, output is a 'build' folder that containe production application). Build Docker image: docker build -t nginxreact -f Dockerfile.prod . . . => [stage-1 2/2] COPY --from=buildstage /app/build /usr/share/nginx/html . . docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE nginxreact latest ee3388384541 4 minutes ago 141MB . . <none> <none> 1c9dc1e2c948 6 hours ago 483MB . . \"\\<none>\" image are produced by intermediate build stage(s). Launch a container with just built image: docker run --rm -p 80:80 nginxreact Browse to localhost to observe your production application running. Docker Compose A new Docker Compose configuration file dedicated to prodction: touch docker-compose.prod.yml docker-compose.prod.yml version : \"3.8\" services : mynginx : build : context : . dockerfile : Dockerfile.prod ports : - 80:80 Reset Docker content: docker system prune -a docker volume prune Start a fresh new production application: docker-compose -f docker-compose.prod.yml up Browse to localhost to observe your production application running.","title":"Compose - 09 - Dockerfile"},{"location":"docker/9-compose-dockerfile/#compose-09-dockerfile","text":"Docker Compose - Dockerfile !WSL2, advised to not use a mounted volume like '/mnt/c/' for handling a project to avoid slowness and live reload issues. Instead, prefer usage of a \"native\" WSL2 folder like for e.g. '/home/user/react-nginx' Dockerfile and Docker Compose to set up a client application composed of: React NGINX","title":"Compose - 09 - Dockerfile"},{"location":"docker/9-compose-dockerfile/#setup-of-client-application-project","text":"","title":"Setup of client application project"},{"location":"docker/9-compose-dockerfile/#node","text":"'create-React-app' is a script to install with 'npm' will pre-configure a 'webpack' environnement and give use access to a development server and a test server and a build command for production. Role of 'NGINX' (http server) is to treat all http(s) request and return response from 'React' application to requester. In this chapter we will see two new features: Dockerfile multi staging, defined with many 'from'. Stdin_open and TTY, two new options of Docker Compose. Prerequisite, install on host machine: 'nvm' - Node Version Manager 'Node.js' Set/check installation with: // Setup project root folder mkdir react-nginx cd react-nginx // To install 'nvm' curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash // Check nvm // Get last version nvm ls-remote | tail . . v17.0.1 v17.1.0 v17.2.0 v17.3.0 // Install last version nvm install 17.3.0 node -v v17.3.0 node Welcome to Node.js v17.3.0. Type \".help\" for more information. > (To exit, press Ctrl+C again or Ctrl+D or type .exit) >","title":"Node"},{"location":"docker/9-compose-dockerfile/#react","text":"Instal 'React' with 'npx' (like npm but executed once with last script release): npx create-react-app client cd client ls README.md node_modules package-lock.json package.json public src npm start . . Compiled successfully! You can now view client in the browser. Local: http://localhost:3000 Browse to: - http://localhost:3000 Test (launch tests contained in react-nginx/client/src/App.test.js): npm run test !! Fail on WSL2 :( Build for production (add a build folder to the project, this is the folder to return via NGINX for client): npm run build All of this just to initialize the project locally on host machine.","title":"React"},{"location":"docker/9-compose-dockerfile/#dockerized","text":"We may now delete the 'node_modules' folder ('node_modules' folder will then be only in container initialized with dependencies through 'npm install' command in 'Dockerfile'): rm -rf node_modules/ Add a 'Dockerfile' in client project folder '/home/user/react-nginx/client': touch Dockerfile Dockerfile: FROM node:alpine WORKDIR /app COPY package.json . RUN npm install COPY . . CMD [\"npm\", \"start\"] Build docker image: docker build -t myreact . docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE myreact latest c4c1b5bb9d21 45 seconds ago 483MB docker run --rm --name react -p 3000:3000 myreact Browse to: - http://localhost:3000","title":"Dockerized"},{"location":"docker/9-compose-dockerfile/#live-reload","text":"It's important to launch from a terminal a based VS Code instance from root client application folder to get live reload effect that works: cd .../react-nginx/client code . For development purpose, to automatically propagate local changes to container. Bind mount project folder: docker run --rm --name react -p 3000:3000 --mount type=bind,src=\"$(pwd)\",target=/app myreact !! => FAIL!! Why? Because when bind mount it crush all what was contained in container '/app' folder with local content and in local there isn't anymore 'node_modules' folder. To avoid this unwanted behavior and keep 'node_modules' in container folder not erased by bind mount (also needed for live reload feature), we bind an anonymous volume targeted on remote container '/app/node_modules' folder. Also, to avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder we modify 'Dockerfile' as follow: FROM node:alpine WORKDIR /app COPY package.json . RUN npm install # To avoid 'EACCES: permission denied' issue on '/app/node_modules/.cache' folder RUN mkdir -p node_modules/.cache && chmod -R 777 node_modules/.cache COPY . . CMD [\"npm\", \"start\"] Bind mount project folder + anonymous volume with '/app/node_modules' as target: docker run --rm --name react -p 3000:3000 --mount type=bind,src=\"$(pwd)\",target=/app --mount type=volume,target=/app/node_modules myreact It's advised to run with '--rm' option when using anonymous volume to suppress it automatically on stop in addition to container suppression. Test live reload by changing text '.. save to reload.' with e.g. 'Hello, world!\" in 'src/App.js' and then observe live effect at http://localhost:3000 in an Internet browser.","title":"Live reload"},{"location":"docker/9-compose-dockerfile/#set-up-docker-compose","text":"docker-compose.yml version : \"3.8\" services : client : build : . ports : - 3000:3000 volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules Reset Docker: docker system prune -a docker volume prune Start service: docker-compose up You may observe live reload working by changing 'App.js' content and 'localhost:3000' changing accordingly.","title":"Set up Docker Compose"},{"location":"docker/9-compose-dockerfile/#test-during-developpement","text":"Note: webpack is an open-source JavaScript module bundler (wikipedia). TDD (Test Driven Development). In a devellopement process it's mostly advised to continuously test in order to check that we don't brake anything. To achieve this, we lauch two terminals in prallel, one with webpack for running application and this other one for automatic testing purpose. It's done with same container image and duplicate services in Docker Compose configuration file, only thing that will change for second duplicated service is that we override command from Dockerfile in Docker Compose configuration file to launch test server and remove also port mapping which is useless for tests. docker-compose.yml: version : \"3.8\" services : client : build : . ports : - 3000:3000 volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules test : build : . command : [ \"npm\" , \"run\" , \"test\" ] volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules docker-compose up --build In logs we may observe: . . test_1 | Tests: 1 passed, 1 total . . If we duplicate one exsisting test in '.../src/App.test.js' file, we may observe live change in logs: . . test_1 | Tests: 2 passed, 2 total . . To interact with test server we need a second terminal. If we want to send command in test container when attaching to it we need to add two options to the test service in Docker Compose configuration file, 'stdin_open' and 'tty', both set to 'true'. docker-compose.yml: version : \"3.8\" services : client : build : . ports : - 3000:3000 volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules test : build : . command : [ \"npm\" , \"run\" , \"test\" ] volumes : - type : bind source : . target : /app - type : volume target : /app/node_modules stdin_open : true tty : true In first terminal: docker-compose down -v docker-compose up --build In second terminal: docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ca16b44236ad client_test \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute client_test_1 4633b0999f9c client_client \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute 0.0.0.0:3000->3000/tcp client_client_1 docker attach client_test_1 . . Watch Usage \u203a Press f to run only failed tests. \u203a Press o to only run tests related to changed files. \u203a Press q to quit watch mode. \u203a Press p to filter by a filename regex pattern. \u203a Press t to filter by a test name regex pattern. \u203a Press Enter to trigger a test run. We may now send commands to test server in second terminal. Then, shutdown gracefully: docker-compose down -v","title":"Test during developpement"},{"location":"docker/9-compose-dockerfile/#production-environnement","text":"","title":"Production environnement"},{"location":"docker/9-compose-dockerfile/#introduction","text":"NGINX return the build. NGINX intercept http request and return by default an html file. Browse docker hub for nginx on official image page to host some simple static content . Simple test: docker run --rm -p 80:80 nginx Browse to localhost In a second terminal: docker exec -it exciting_elion sh cd usr/share/nginx/html ls 50x.html index.html cat index.html <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> . . It's in this folder we gonna copy our application '/usr/share/nginx/html'. For more complex configuration, have a look in '/etc/nginx/conf.d' folder.","title":"Introduction"},{"location":"docker/9-compose-dockerfile/#multi-stage-dockerfile","text":"Reuse a preceding builded image in next build. Goal to reach here is to put React's build folder into NGINX image. ../client/Dockerfile.prod FROM node:alpine as buildstage WORKDIR /app COPY package.json . RUN npm install COPY . . RUN npm run build FROM nginx COPY --from=buildstage /app/build /usr/share/nginx/html EXPOSE 80 After second FROM above, --from=buildstage refer to first stage of image build (npm run build, output is a 'build' folder that containe production application). Build Docker image: docker build -t nginxreact -f Dockerfile.prod . . . => [stage-1 2/2] COPY --from=buildstage /app/build /usr/share/nginx/html . . docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE nginxreact latest ee3388384541 4 minutes ago 141MB . . <none> <none> 1c9dc1e2c948 6 hours ago 483MB . . \"\\<none>\" image are produced by intermediate build stage(s). Launch a container with just built image: docker run --rm -p 80:80 nginxreact Browse to localhost to observe your production application running.","title":"Multi stage Dockerfile"},{"location":"docker/9-compose-dockerfile/#docker-compose","text":"A new Docker Compose configuration file dedicated to prodction: touch docker-compose.prod.yml docker-compose.prod.yml version : \"3.8\" services : mynginx : build : context : . dockerfile : Dockerfile.prod ports : - 80:80 Reset Docker content: docker system prune -a docker volume prune Start a fresh new production application: docker-compose -f docker-compose.prod.yml up Browse to localhost to observe your production application running.","title":"Docker Compose"},{"location":"elk/elasticsearch/elasticsearch/","text":"ElasticSearch Update field Plugin Head Update boolean field with Chrome plugin ElasticSearch Head local: http://localhost:9200/ Other request tab URL http://localhost:9200/index/doc/id to check empty GET to update request _update POST {\"doc\":{\"deleted\":false}} Application logging ES 7.17, Configure application logging MDC Logback MDC, Mapped Diagnostic Context","title":"ElasticSearch - Misc"},{"location":"elk/elasticsearch/elasticsearch/#elasticsearch","text":"","title":"ElasticSearch"},{"location":"elk/elasticsearch/elasticsearch/#update-field","text":"Plugin Head Update boolean field with Chrome plugin ElasticSearch Head local: http://localhost:9200/ Other request tab URL http://localhost:9200/index/doc/id to check empty GET to update request _update POST {\"doc\":{\"deleted\":false}}","title":"Update field"},{"location":"elk/elasticsearch/elasticsearch/#application-logging","text":"ES 7.17, Configure application logging","title":"Application logging"},{"location":"elk/elasticsearch/elasticsearch/#mdc","text":"Logback MDC, Mapped Diagnostic Context","title":"MDC"},{"location":"elk/kibana/kibana/","text":"Kibana Update field Kibana 5.6 Browse to Dev Tools, Console to get: GET _index/_type/_id to update: POST _index/_type/_id/_update { \"doc\": { \"field\":\"value\" or \"field\":99 or \"field\":false } } Retrieve index Retrieve to which index belongs an id, in Kibana 5.6 Dev Tools: GET /*/_search { \"query\": { \"match\": { \"_id\": \"db37..-..-..-..-..9a117d83\" } } } Filter in query bar Filter in query bar (not in filter-pill) to get all records that id field start with a pattern: id:3* Index pattern scripted field Index pattern scripted field to calculate time difference, in seconds, between two dates, language \"painless\", type \"number\", format pattern \"00.\": (doc['gatewayDate'].value - doc['timestamp'].value)/1000 Delete doc Kibana 7.17 discover to get doc id dev tools e.g. GET/DELETE /index/_doc/<id> List connected agent Kibana 7.17, dev tools: GET _nodes/stats?filter_path=**.clients List queue Kibana 7.17, dev tools: GET _cat/thread_pool/search,write?v","title":"Kibana - Misc"},{"location":"elk/kibana/kibana/#kibana","text":"","title":"Kibana"},{"location":"elk/kibana/kibana/#update-field","text":"Kibana 5.6 Browse to Dev Tools, Console to get: GET _index/_type/_id to update: POST _index/_type/_id/_update { \"doc\": { \"field\":\"value\" or \"field\":99 or \"field\":false } }","title":"Update field"},{"location":"elk/kibana/kibana/#retrieve-index","text":"Retrieve to which index belongs an id, in Kibana 5.6 Dev Tools: GET /*/_search { \"query\": { \"match\": { \"_id\": \"db37..-..-..-..-..9a117d83\" } } }","title":"Retrieve index"},{"location":"elk/kibana/kibana/#filter-in-query-bar","text":"Filter in query bar (not in filter-pill) to get all records that id field start with a pattern: id:3*","title":"Filter in query bar"},{"location":"elk/kibana/kibana/#index-pattern-scripted-field","text":"Index pattern scripted field to calculate time difference, in seconds, between two dates, language \"painless\", type \"number\", format pattern \"00.\": (doc['gatewayDate'].value - doc['timestamp'].value)/1000","title":"Index pattern scripted field"},{"location":"elk/kibana/kibana/#delete-doc","text":"Kibana 7.17 discover to get doc id dev tools e.g. GET/DELETE /index/_doc/<id>","title":"Delete doc"},{"location":"elk/kibana/kibana/#list-connected-agent","text":"Kibana 7.17, dev tools: GET _nodes/stats?filter_path=**.clients","title":"List connected agent"},{"location":"elk/kibana/kibana/#list-queue","text":"Kibana 7.17, dev tools: GET _cat/thread_pool/search,write?v","title":"List queue"},{"location":"git/misc/","text":"misc Initialize Set up a new project. Local, first step mkdir newproject-001 cd newproject-001 git init touch .gitignore git config user.email \"john@doe.com\" git config user.name \"John Doe\" git add . git commit -m \"initial commit\" GitHub Now go to your github account and create a new repository, e.g. \"newproject-001\". Local, second step git branch -M main git remote add origin https://github.com/johndoe/newproject-001.git git push -u origin main WSL Credential Manager setup SRC: microsoft git config credential.helper \"/mnt/c/Program\\ Files/Git/mingw64/libexec/git-core/git-credential-manager-core.exe\" Configuration List configuration parameters: git config --list Edit configuration parameters: git config --global --edit or git config --edit Cache token to cache token (use token instead of password) (for 15 minutes, by default). SRC: github git config credential.helper cache or for 1 hour git config credential.helper 'cache --timeout=3600' SSH To HTTPS Change remote URL from SSH To HTTPS. Source Check existing remote: git remote -v Change your remote's URL from SSH to HTTPS with the git remote set-url command: git remote set-url origin https://github.com/USERNAME/REPOSITORY.git Many GitHub accounts How to manage credentials for many different GitHub accounts. Source Configure credentials to use the full repository path: git config --global credential.useHttpPath true Remove tracked folder For files folder previously tracked and then added to .gitignore: git rm -r --cached <folder> Checkout remote branch Tutorial 1. Fetch all remote branches git fetch origin 2. List the branches available for checkout git branch -a 3. Pull changes from a remote branch git checkout -b local origin/remote Cherry-pick without commit Tutorial git cherry-pick <commit-hash> -n Branch description Tutorial Set git branch --edit-description Get git config branch.<branch>.description Push description into merge commits You can push this description into merge commits if you set: git config merge.branchdesc true This means when you issue git merge --log <branch> , it'll force the branch description into the stock merge commit message. Undo remote change(s) Warning! To use only if you are working alone in repository! git reset --hard HEAD~<number of commit(s) before> git push --force Undo undo git pull","title":"misc"},{"location":"git/misc/#misc","text":"","title":"misc"},{"location":"git/misc/#initialize","text":"Set up a new project.","title":"Initialize"},{"location":"git/misc/#local-first-step","text":"mkdir newproject-001 cd newproject-001 git init touch .gitignore git config user.email \"john@doe.com\" git config user.name \"John Doe\" git add . git commit -m \"initial commit\"","title":"Local, first step"},{"location":"git/misc/#github","text":"Now go to your github account and create a new repository, e.g. \"newproject-001\".","title":"GitHub"},{"location":"git/misc/#local-second-step","text":"git branch -M main git remote add origin https://github.com/johndoe/newproject-001.git git push -u origin main","title":"Local, second step"},{"location":"git/misc/#wsl-credential-manager-setup","text":"SRC: microsoft git config credential.helper \"/mnt/c/Program\\ Files/Git/mingw64/libexec/git-core/git-credential-manager-core.exe\"","title":"WSL Credential Manager setup"},{"location":"git/misc/#configuration","text":"List configuration parameters: git config --list Edit configuration parameters: git config --global --edit or git config --edit","title":"Configuration"},{"location":"git/misc/#cache-token","text":"to cache token (use token instead of password) (for 15 minutes, by default). SRC: github git config credential.helper cache or for 1 hour git config credential.helper 'cache --timeout=3600'","title":"Cache token"},{"location":"git/misc/#ssh-to-https","text":"Change remote URL from SSH To HTTPS. Source Check existing remote: git remote -v Change your remote's URL from SSH to HTTPS with the git remote set-url command: git remote set-url origin https://github.com/USERNAME/REPOSITORY.git","title":"SSH To HTTPS"},{"location":"git/misc/#many-github-accounts","text":"How to manage credentials for many different GitHub accounts. Source Configure credentials to use the full repository path: git config --global credential.useHttpPath true","title":"Many GitHub accounts"},{"location":"git/misc/#remove-tracked-folder","text":"For files folder previously tracked and then added to .gitignore: git rm -r --cached <folder>","title":"Remove tracked folder"},{"location":"git/misc/#checkout-remote-branch","text":"Tutorial","title":"Checkout remote branch"},{"location":"git/misc/#1-fetch-all-remote-branches","text":"git fetch origin","title":"1. Fetch all remote branches"},{"location":"git/misc/#2-list-the-branches-available-for-checkout","text":"git branch -a","title":"2. List the branches available for checkout"},{"location":"git/misc/#3-pull-changes-from-a-remote-branch","text":"git checkout -b local origin/remote","title":"3. Pull changes from a remote branch"},{"location":"git/misc/#cherry-pick-without-commit","text":"Tutorial git cherry-pick <commit-hash> -n","title":"Cherry-pick without commit"},{"location":"git/misc/#branch-description","text":"Tutorial","title":"Branch description"},{"location":"git/misc/#set","text":"git branch --edit-description","title":"Set"},{"location":"git/misc/#get","text":"git config branch.<branch>.description","title":"Get"},{"location":"git/misc/#push-description-into-merge-commits","text":"You can push this description into merge commits if you set: git config merge.branchdesc true This means when you issue git merge --log <branch> , it'll force the branch description into the stock merge commit message.","title":"Push description into merge commits"},{"location":"git/misc/#undo-remote-changes","text":"Warning! To use only if you are working alone in repository! git reset --hard HEAD~<number of commit(s) before> git push --force Undo undo git pull","title":"Undo remote change(s)"},{"location":"google/chrome/0-misc/","text":"0-misc Google Chrome miscellaneous topics Apps icon Apps icon (colorful square dot square), to enable: right click favorite bar and check 'Display applications shortcut'","title":"0-misc"},{"location":"google/chrome/0-misc/#0-misc","text":"Google Chrome miscellaneous topics","title":"0-misc"},{"location":"google/chrome/0-misc/#apps-icon","text":"Apps icon (colorful square dot square), to enable: right click favorite bar and check 'Display applications shortcut'","title":"Apps icon"},{"location":"google/chrome/1-devtool/","text":"1-devtool Google Chrome DevTools (F12 or Inspect) Zoom CTRL + mouse scroll to zoom int/out CTRL + 0 to reset","title":"1-devtool"},{"location":"google/chrome/1-devtool/#1-devtool","text":"Google Chrome DevTools (F12 or Inspect)","title":"1-devtool"},{"location":"google/chrome/1-devtool/#zoom","text":"CTRL + mouse scroll to zoom int/out CTRL + 0 to reset","title":"Zoom"},{"location":"google/chrome/2-shortcut/","text":"2-shortcut Google Chrome miscellaneous shortcuts New tab CTRL + t Reopen closed tab CTRL + SHIFT + t Focus to location bar ALT + d Duplicate tab ALT + d shortcut key sequence to move the focus to the location bar. And keep the Alt key held down. Now simply hit the ENTER key (since you've got the ALT key held down). This will open up the current URL into a new tab, basically duplicating the tab. ALT + d + ENTER","title":"2-shortcut"},{"location":"google/chrome/2-shortcut/#2-shortcut","text":"Google Chrome miscellaneous shortcuts","title":"2-shortcut"},{"location":"google/chrome/2-shortcut/#new-tab","text":"CTRL + t","title":"New tab"},{"location":"google/chrome/2-shortcut/#reopen-closed-tab","text":"CTRL + SHIFT + t","title":"Reopen closed tab"},{"location":"google/chrome/2-shortcut/#focus-to-location-bar","text":"ALT + d","title":"Focus to location bar"},{"location":"google/chrome/2-shortcut/#duplicate-tab","text":"ALT + d shortcut key sequence to move the focus to the location bar. And keep the Alt key held down. Now simply hit the ENTER key (since you've got the ALT key held down). This will open up the current URL into a new tab, basically duplicating the tab. ALT + d + ENTER","title":"Duplicate tab"},{"location":"intellij/1-intellij-misc/","text":"IntelliJ - 01 - Misc Setup project in WSL folder Fix EOL (LF vs CRLF) issue: git clone select root project folder menu File - File Properties - Line Separators - LF Unix git rollback entire project to reset CRLF issue SRC OR Terminal in a new \"native\" WSL folder, clone and fix CRLF issue : git clone project_url git config core.eol lf git config core.autocrlf input Service window For Docker connection, maybe not opened if comes from external project. To open it manually: View | Tool Windows | Services or Alt+8 SRC Search for ; and replace with ;\\r\\n For a file where each values are separated with ';' but need each values on a new line to compare files. replace ; enable regex by ;\\r\\n To revert, do it in Notepad++ with a record sequence Caret Cloning SRC: - IntelliJ IDEA Tips & Tricks: Multiple Cursors Alt + Shift + Insert to switch to column mode Then Shift + Up/Down Arrow(s) Compile and reload changed classes Ctrl + Shift + F9 Surround code fragments SRC: - Surround code fragments Ctrl + Alt + T","title":"IntelliJ - 01 - Misc"},{"location":"intellij/1-intellij-misc/#intellij-01-misc","text":"","title":"IntelliJ - 01 - Misc"},{"location":"intellij/1-intellij-misc/#setup-project-in-wsl-folder","text":"Fix EOL (LF vs CRLF) issue: git clone select root project folder menu File - File Properties - Line Separators - LF Unix git rollback entire project to reset CRLF issue SRC OR Terminal in a new \"native\" WSL folder, clone and fix CRLF issue : git clone project_url git config core.eol lf git config core.autocrlf input","title":"Setup project in WSL folder"},{"location":"intellij/1-intellij-misc/#service-window","text":"For Docker connection, maybe not opened if comes from external project. To open it manually: View | Tool Windows | Services or Alt+8 SRC","title":"Service window"},{"location":"intellij/1-intellij-misc/#search-for-and-replace-with-rn","text":"For a file where each values are separated with ';' but need each values on a new line to compare files. replace ; enable regex by ;\\r\\n To revert, do it in Notepad++ with a record sequence","title":"Search for ; and replace with ;\\r\\n"},{"location":"intellij/1-intellij-misc/#caret-cloning","text":"SRC: - IntelliJ IDEA Tips & Tricks: Multiple Cursors Alt + Shift + Insert to switch to column mode Then Shift + Up/Down Arrow(s)","title":"Caret Cloning"},{"location":"intellij/1-intellij-misc/#compile-and-reload-changed-classes","text":"Ctrl + Shift + F9","title":"Compile and reload changed classes"},{"location":"intellij/1-intellij-misc/#surround-code-fragments","text":"SRC: - Surround code fragments Ctrl + Alt + T","title":"Surround code fragments"},{"location":"intellij/2-intellij-wsl/","text":"IntelliJ - 02 - WSL IntelliJ integration with a project hosted in a WSL2 native folder (not /mnt/c) Test Maven To test that maven is correctly installed and running in Linux distribution. Setup a new maven project with a simple 'hello' class to test debug/run. Firewall rules To allow IntelliJ to communicate with WSL. SRC: - set IntelliJ rules to communicate with WSL PowerShell (admin mode): New-NetFirewallRule -DisplayName \"WSL\" -Direction Inbound -InterfaceAlias \"vEthernet (WSL)\" -Action Allow Get-NetFirewallRule | where DisplayName -ILike \"idea*.exe\" | Remove-NetFirewallRule CRLF issue After cloning a git repository maybe all files are marked as modified due to CRLF miss understanding between Windows and Linux file system. SRC: - fix CRLF issue From a terminal in a native (not /mnt/c) WSL folder: git clone <url> git config core.eol lf git config core.autocrlf input IntelliJ recipe At least release \"IntelliJ IDEA 2021.3.1 (Community Edition)\" Open existing project: - set project SDK with the one from WSL - set Maven home (IDE settings, search for maven) to : \"\\\\wsl$\\<linux distribution>\\usr\\share\\maven\" - invalidate cache and restart Note: - original Maven setting for Windows = C:\\Program Files\\JetBrains\\IntelliJ IDEA Community Edition 2021.3.1\\plugins\\maven\\lib\\maven3 Fix IntelliJ Fix IntelliJ to disable one wsl experimental feature. SRC: - fix IntelliJ - press Shift-Shift to open \"Search everywhere\" - enter \"Experimental features\" and select the action - in the dialog that appears, turn off wsl.fsd.content.loader - restart the IDE Fix Docker For Docker Compose to prepare context with Docker Desktop for Windows. Enable the Docker Compose V2 option under the experimental settings. SRC: - fix Docker Fix Logback In case project fail to start due to Logback issue, \"Failed to create parent directories\" var log. Manually create needed log folder for project and set rights accordingly: sudo mkdir /var/log/<needed project folder name for log> sudo chmod a+rwx /var/log/<needed project folder name for log>","title":"IntelliJ - 02 - WSL"},{"location":"intellij/2-intellij-wsl/#intellij-02-wsl","text":"IntelliJ integration with a project hosted in a WSL2 native folder (not /mnt/c)","title":"IntelliJ - 02 - WSL"},{"location":"intellij/2-intellij-wsl/#test-maven","text":"To test that maven is correctly installed and running in Linux distribution. Setup a new maven project with a simple 'hello' class to test debug/run.","title":"Test Maven"},{"location":"intellij/2-intellij-wsl/#firewall-rules","text":"To allow IntelliJ to communicate with WSL. SRC: - set IntelliJ rules to communicate with WSL PowerShell (admin mode): New-NetFirewallRule -DisplayName \"WSL\" -Direction Inbound -InterfaceAlias \"vEthernet (WSL)\" -Action Allow Get-NetFirewallRule | where DisplayName -ILike \"idea*.exe\" | Remove-NetFirewallRule","title":"Firewall rules"},{"location":"intellij/2-intellij-wsl/#crlf-issue","text":"After cloning a git repository maybe all files are marked as modified due to CRLF miss understanding between Windows and Linux file system. SRC: - fix CRLF issue From a terminal in a native (not /mnt/c) WSL folder: git clone <url> git config core.eol lf git config core.autocrlf input","title":"CRLF issue"},{"location":"intellij/2-intellij-wsl/#intellij-recipe","text":"At least release \"IntelliJ IDEA 2021.3.1 (Community Edition)\" Open existing project: - set project SDK with the one from WSL - set Maven home (IDE settings, search for maven) to : \"\\\\wsl$\\<linux distribution>\\usr\\share\\maven\" - invalidate cache and restart Note: - original Maven setting for Windows = C:\\Program Files\\JetBrains\\IntelliJ IDEA Community Edition 2021.3.1\\plugins\\maven\\lib\\maven3","title":"IntelliJ recipe"},{"location":"intellij/2-intellij-wsl/#fix-intellij","text":"Fix IntelliJ to disable one wsl experimental feature. SRC: - fix IntelliJ - press Shift-Shift to open \"Search everywhere\" - enter \"Experimental features\" and select the action - in the dialog that appears, turn off wsl.fsd.content.loader - restart the IDE","title":"Fix IntelliJ"},{"location":"intellij/2-intellij-wsl/#fix-docker","text":"For Docker Compose to prepare context with Docker Desktop for Windows. Enable the Docker Compose V2 option under the experimental settings. SRC: - fix Docker","title":"Fix Docker"},{"location":"intellij/2-intellij-wsl/#fix-logback","text":"In case project fail to start due to Logback issue, \"Failed to create parent directories\" var log. Manually create needed log folder for project and set rights accordingly: sudo mkdir /var/log/<needed project folder name for log> sudo chmod a+rwx /var/log/<needed project folder name for log>","title":"Fix Logback"},{"location":"java/test/mock/","text":"Mock Mockito in a springboot context. Test that a filter works as expected. Project source on gihub Extract from CustomerService.java: public String onlyCustomersWithPaulAsFirstNameThenLastNameToUpperCase () { List < Customer > customers = customerRepository . findAll (); List < Customer > onlyPauls = customers . stream (). filter ( customer -> \"Paul\" . equals ( customer . getFirstName ())). collect ( Collectors . toList ()); converter . lastNameToUpperCase ( onlyPauls ); return ! onlyPauls . isEmpty () ? String . join ( \", \" , onlyPauls . toString ()) : \"none\" ; } Extract from CustomerServiceTest.java: @InjectMocks private CustomerService customerService ; @Mock private CustomerRepository customerRepository ; @Mock Converter converter ; @Test public void onlyCustomersWithPaulAsFirstNameThenLastNameToUpperCaseTest_WhenNotOnlyPaulsIn_ThenOnlyPaulsOut () { when ( customerRepository . findAll ()). thenReturn ( CUSTOMERS ); customerService . onlyCustomersWithPaulAsFirstNameThenLastNameToUpperCase (); @SuppressWarnings ( \"unchecked\" ) ArgumentCaptor < List < Customer >> customersCaptor = ArgumentCaptor . forClass ( List . class ); verify ( converter , times ( 1 )). lastNameToUpperCase ( customersCaptor . capture ()); // For not captured/tested argument(s) replace with \"any()\" List < Customer > paulsOut = customersCaptor . getValue (); assertEquals ( 4 , paulsOut . size ()); assertTrue ( paulsOut . stream (). allMatch ( customer -> PAUL . equals ( customer . getFirstName ()))); }","title":"mock"},{"location":"java/test/mock/#mock","text":"Mockito in a springboot context. Test that a filter works as expected. Project source on gihub Extract from CustomerService.java: public String onlyCustomersWithPaulAsFirstNameThenLastNameToUpperCase () { List < Customer > customers = customerRepository . findAll (); List < Customer > onlyPauls = customers . stream (). filter ( customer -> \"Paul\" . equals ( customer . getFirstName ())). collect ( Collectors . toList ()); converter . lastNameToUpperCase ( onlyPauls ); return ! onlyPauls . isEmpty () ? String . join ( \", \" , onlyPauls . toString ()) : \"none\" ; } Extract from CustomerServiceTest.java: @InjectMocks private CustomerService customerService ; @Mock private CustomerRepository customerRepository ; @Mock Converter converter ; @Test public void onlyCustomersWithPaulAsFirstNameThenLastNameToUpperCaseTest_WhenNotOnlyPaulsIn_ThenOnlyPaulsOut () { when ( customerRepository . findAll ()). thenReturn ( CUSTOMERS ); customerService . onlyCustomersWithPaulAsFirstNameThenLastNameToUpperCase (); @SuppressWarnings ( \"unchecked\" ) ArgumentCaptor < List < Customer >> customersCaptor = ArgumentCaptor . forClass ( List . class ); verify ( converter , times ( 1 )). lastNameToUpperCase ( customersCaptor . capture ()); // For not captured/tested argument(s) replace with \"any()\" List < Customer > paulsOut = customersCaptor . getValue (); assertEquals ( 4 , paulsOut . size ()); assertTrue ( paulsOut . stream (). allMatch ( customer -> PAUL . equals ( customer . getFirstName ()))); }","title":"Mock"},{"location":"javascript/misc/","text":"misc Spread Vs Rest Same synthax '\u2026' for the Spread operator and the Rest operator. The difference between the two is the context of use, they do not have the same effect. Spread The spread syntax expands iterables into individual elements. In other words, the spread operator unpacks the elements of an iterable object. Spread demo : function sum ( x , y , z ) { return x + y + z ; } const numbers = [ 1 , 2 , 3 ]; console . log ( sum (... numbers )); // expected output: 6 console . log ( sum . apply ( null , numbers )); // expected output: 6 Rest The rest operator puts the rest of some specific user-supplied values into a JavaScript array. In other words, the rest parameter packs the elements into an array. Rest demo : function sum (... theArgs ) { let total = 0 ; for ( const arg of theArgs ) { total += arg ; } return total ; } console . log ( sum ( 1 , 2 , 3 )); // expected output: 6 console . log ( sum ( 1 , 2 , 3 , 4 )); // expected output: 10","title":"misc"},{"location":"javascript/misc/#misc","text":"","title":"misc"},{"location":"javascript/misc/#spread-vs-rest","text":"Same synthax '\u2026' for the Spread operator and the Rest operator. The difference between the two is the context of use, they do not have the same effect.","title":"Spread Vs Rest"},{"location":"javascript/misc/#spread","text":"The spread syntax expands iterables into individual elements. In other words, the spread operator unpacks the elements of an iterable object. Spread demo : function sum ( x , y , z ) { return x + y + z ; } const numbers = [ 1 , 2 , 3 ]; console . log ( sum (... numbers )); // expected output: 6 console . log ( sum . apply ( null , numbers )); // expected output: 6","title":"Spread"},{"location":"javascript/misc/#rest","text":"The rest operator puts the rest of some specific user-supplied values into a JavaScript array. In other words, the rest parameter packs the elements into an array. Rest demo : function sum (... theArgs ) { let total = 0 ; for ( const arg of theArgs ) { total += arg ; } return total ; } console . log ( sum ( 1 , 2 , 3 )); // expected output: 6 console . log ( sum ( 1 , 2 , 3 , 4 )); // expected output: 10","title":"Rest"},{"location":"javascript/training/02-environment/06-install-environnement/","text":"06-install-environnement For WSL2 refer to javascript dev environnement page from Microsoft documentation . VS Code Get latest release of Visual Studio Code from Internet page and install it accordingly to your Operating System (Windows, Linux, Mac). Node.js Get LTS (for Long Term Support) installation package from Internet page . Check install: node -v npm NPM should have installed along with installing Node.js. Check install: npm -v","title":"06-install-environnement"},{"location":"javascript/training/02-environment/06-install-environnement/#06-install-environnement","text":"For WSL2 refer to javascript dev environnement page from Microsoft documentation .","title":"06-install-environnement"},{"location":"javascript/training/02-environment/06-install-environnement/#vs-code","text":"Get latest release of Visual Studio Code from Internet page and install it accordingly to your Operating System (Windows, Linux, Mac).","title":"VS Code"},{"location":"javascript/training/02-environment/06-install-environnement/#nodejs","text":"Get LTS (for Long Term Support) installation package from Internet page . Check install: node -v","title":"Node.js"},{"location":"javascript/training/02-environment/06-install-environnement/#npm","text":"NPM should have installed along with installing Node.js. Check install: npm -v","title":"npm"},{"location":"javascript/training/02-environment/08-install-babel/","text":"08-install-babel Babel is a free and open-source JavaScript transcompiler that is mainly used to convert code into a backwards compatible version of JavaScript. Wiki Website Install In a terminal: mkdir test cd test npm init -y touch index.html touch babel.config.js touch es6.js es6.js: let test = \"123\" ; let keyword only available since es6 and here we want to show that babel do his job by converting to an earlier version understandable by most of the browsers. In a terminal: npm i @babel/core @babel/cli @babel/preset-env Then all dependencies are in created in node_modules folder. If node_modules does not exist or removed, type command below will read package.json file and reinstall everything: npm i First, test babel without preset by adding script below in package.json: \"scripts\" : { ..., \"babel\" : \"babel es6.js\" } Then, in a terminal: npm run babel > test@1.0.0 babel > babel es6.js let test = \"123\"; No difference in output but at least we now that babel core is running well. To have output in a new file, package.json: \"scripts\" : { ..., \"babel\" : \"babel es6.js -o es6after.js\" } Then, again, in a terminal: npm run babel > test@1.0.0 babel > babel es6.js -o es6after.js Now, as expected, we have a new file es6after.js Now we configure babel in babel.config.js to use preset-env: module . exports = { presets : [[ \"@babel/preset-env\" ]] } Then, again, in a terminal: npm run babel > test@1.0.0 babel > babel es6.js -o es6after.js And now, es6after.js has been converted in an earlier version: \"use strict\" ; var test = \"123\" ;","title":"08-install-babel"},{"location":"javascript/training/02-environment/08-install-babel/#08-install-babel","text":"Babel is a free and open-source JavaScript transcompiler that is mainly used to convert code into a backwards compatible version of JavaScript. Wiki Website","title":"08-install-babel"},{"location":"javascript/training/02-environment/08-install-babel/#install","text":"In a terminal: mkdir test cd test npm init -y touch index.html touch babel.config.js touch es6.js es6.js: let test = \"123\" ; let keyword only available since es6 and here we want to show that babel do his job by converting to an earlier version understandable by most of the browsers. In a terminal: npm i @babel/core @babel/cli @babel/preset-env Then all dependencies are in created in node_modules folder. If node_modules does not exist or removed, type command below will read package.json file and reinstall everything: npm i First, test babel without preset by adding script below in package.json: \"scripts\" : { ..., \"babel\" : \"babel es6.js\" } Then, in a terminal: npm run babel > test@1.0.0 babel > babel es6.js let test = \"123\"; No difference in output but at least we now that babel core is running well. To have output in a new file, package.json: \"scripts\" : { ..., \"babel\" : \"babel es6.js -o es6after.js\" } Then, again, in a terminal: npm run babel > test@1.0.0 babel > babel es6.js -o es6after.js Now, as expected, we have a new file es6after.js Now we configure babel in babel.config.js to use preset-env: module . exports = { presets : [[ \"@babel/preset-env\" ]] } Then, again, in a terminal: npm run babel > test@1.0.0 babel > babel es6.js -o es6after.js And now, es6after.js has been converted in an earlier version: \"use strict\" ; var test = \"123\" ;","title":"Install"},{"location":"javascript/training/02-environment/09-javascript-in-html/","text":"09-javascript-in-html Initial setup In root folder project create a src folder that will contain all javascript source code: mkdir src Move file (just a test file) used from previous chapter in src folder: mv es6.js src Edit babel script in package.json to specify input = src folder, output = dist folder: \"scripts\" : { \"babel\" : \"babel src --out-dir dist\" } Test, type below command in a terminal and have a look in dist folder to see the result of babel transpiled output: npm run babel Html In root project folder: touch index.html Open index.html in VS Code, and type \"!\" + tab to get a minimal html file set. Add a h1 section in body and from file explorer drag and drop it in a browser to see the result: < body > < h1 > My app </ h1 > </ body > Source In html Now to incorporate javascript in our html page, add a script section in head: < head > ... < script type = \"text/javascript\" ></ script > </ head > Not really common expect for some dedicated usage like google analytic, but you may insert javascript directly inside script section of an html page (be aware to put only retro compatible code in it as long as it's directly interpreted by the browser, doesn't went through babel transpiler): < head > ... < script type = \"text/javascript\" > console . log ( 'Hi' ); </ script > </ head > Drag and drop index.html file in a browser, right click inspect and then observe message in console. You may incorporate anywhere inside a head or body section of an html document. Out html Separation of concern. Edit head, script section of index.html as follow: < script type = \"text/javascript\" src = \"dist/es6.js\" ></ script > Never edit files in dist folder. Edit src/es6.js by adding a line as follow: console . log ( \"test\" ); Recompile application with babel: npm run babel See the result in dist folder and refresh the page in the browser to observe message in log. From web E.g. jquery CDN = Content Delivery Network Browse for jquery cdn and copy script snippet of last minified version and add it to head section of html file: < head > < script src = \"https://code.jquery.com/jquery-3.6.0.min.js\" integrity = \"sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=\" crossorigin = \"anonymous\" ></ script > < script type = \"text/javascript\" src = \"dist/es6.js\" ></ script > </ head > Test that jquery is working by adding following line in src/es6.js file: console . log ( $ ); npm run babel and refresh page in browser, in console you may observe a message that correspond to a function like: \u0192 (e,t){return new S.fn.init(e,t)} What happen here is that we bring back javascript from internet.","title":"09-javascript-in-html"},{"location":"javascript/training/02-environment/09-javascript-in-html/#09-javascript-in-html","text":"","title":"09-javascript-in-html"},{"location":"javascript/training/02-environment/09-javascript-in-html/#initial-setup","text":"In root folder project create a src folder that will contain all javascript source code: mkdir src Move file (just a test file) used from previous chapter in src folder: mv es6.js src Edit babel script in package.json to specify input = src folder, output = dist folder: \"scripts\" : { \"babel\" : \"babel src --out-dir dist\" } Test, type below command in a terminal and have a look in dist folder to see the result of babel transpiled output: npm run babel","title":"Initial setup"},{"location":"javascript/training/02-environment/09-javascript-in-html/#html","text":"In root project folder: touch index.html Open index.html in VS Code, and type \"!\" + tab to get a minimal html file set. Add a h1 section in body and from file explorer drag and drop it in a browser to see the result: < body > < h1 > My app </ h1 > </ body >","title":"Html"},{"location":"javascript/training/02-environment/09-javascript-in-html/#source","text":"","title":"Source"},{"location":"javascript/training/02-environment/09-javascript-in-html/#in-html","text":"Now to incorporate javascript in our html page, add a script section in head: < head > ... < script type = \"text/javascript\" ></ script > </ head > Not really common expect for some dedicated usage like google analytic, but you may insert javascript directly inside script section of an html page (be aware to put only retro compatible code in it as long as it's directly interpreted by the browser, doesn't went through babel transpiler): < head > ... < script type = \"text/javascript\" > console . log ( 'Hi' ); </ script > </ head > Drag and drop index.html file in a browser, right click inspect and then observe message in console. You may incorporate anywhere inside a head or body section of an html document.","title":"In html"},{"location":"javascript/training/02-environment/09-javascript-in-html/#out-html","text":"Separation of concern. Edit head, script section of index.html as follow: < script type = \"text/javascript\" src = \"dist/es6.js\" ></ script > Never edit files in dist folder. Edit src/es6.js by adding a line as follow: console . log ( \"test\" ); Recompile application with babel: npm run babel See the result in dist folder and refresh the page in the browser to observe message in log.","title":"Out html"},{"location":"javascript/training/02-environment/09-javascript-in-html/#from-web","text":"E.g. jquery CDN = Content Delivery Network Browse for jquery cdn and copy script snippet of last minified version and add it to head section of html file: < head > < script src = \"https://code.jquery.com/jquery-3.6.0.min.js\" integrity = \"sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=\" crossorigin = \"anonymous\" ></ script > < script type = \"text/javascript\" src = \"dist/es6.js\" ></ script > </ head > Test that jquery is working by adding following line in src/es6.js file: console . log ( $ ); npm run babel and refresh page in browser, in console you may observe a message that correspond to a function like: \u0192 (e,t){return new S.fn.init(e,t)} What happen here is that we bring back javascript from internet.","title":"From web"},{"location":"javascript/training/02-environment/10-webpack/","text":"10-webpack Basic principles Webpack is installed through npm. What is installed: webpack webpack-cli webpack-dev-server babel-loader Webpack Bundle creation. Webpack is configured through webpack.config.js configuration file. Webpack use babel-loader and take html + js content to create a dist bundle that contain optimized bundle.js and html content. Webpack-cli Use webpack in a terminal. Webpack-dev-server Local development web server the list at localhost address on dedicated port and return index.html file from dist folder. Installation Prerequisite First, rename used file in previous chapter src/es6.js to src/index.js and remove dist folder if it exist and move index.html in src/: mv src/es6.js src/index.js rm -rf dist mv index.html src/ In src/index.html remove script section. src/index.html: <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > < title > Document </ title > </ head > < body > < h1 > My app </ h1 > </ body > </ html > Edit src/index.js as follow. src/index.js: let test = \"123\" ; console . log ( test ); In package.json remove line in script section that concern babel because we'll let webpack to take it in charge. package.json: { \"name\" : \"test\" , \"version\" : \"1.0.0\" , \"description\" : \"\" , \"main\" : \"index.js\" , \"scripts\" : { \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\" : [], \"author\" : \"\" , \"license\" : \"ISC\" , \"dependencies\" : { \"@babel/cli\" : \"^7.17.6\" , \"@babel/core\" : \"^7.17.5\" , \"@babel/preset-env\" : \"^7.16.11\" } } Launch npm i webpack webpack-cli webpack-dev-server babel-loader Add new file at the project root level called webpack.config.js: touch webpack.config.js webpack.config.js: const path = require ( \"path\" ); const HtmlWebpackPlugin = require ( \"html-webpack-plugin\" ); module . exports = { entry : path . resolve ( __dirname , \"src/index.js\" ), output : { path : path . resolve ( __dirname , \"dist\" ), filename : \"[name].bundle.js\" }, module : { rules : [ { test : /\\.js$/ , exclude : /node_modules/ , use : { loader : \"babel-loader\" } } ] }, plugins : [ new HtmlWebpackPlugin ({ template : path . resolve ( __dirname , \"src/index.html\" ) }) ], devtool : \"source-map\" , mode : \"development\" , devServer : { static : path . resolve ( __dirname , './dist' ), open : true , port : 4000 } }; Html webpack plugin automatically inject output js bundle to html file. Install: npm i html-webpack-plugin Edit package.json to use webpack and webpack server in script section. package.json ... \"scripts\" : { \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" , \"webpack\" : \"webpack\" , \"start\" : \"webpack serve\" }, ... Run npm run webpack Now a dist folder appear with an index.html file that does include call to output result of bundled js, main.bundle.js. dist/index.html: ... < head > ... < script defer src = \"main.bundle.js\" ></ script > ... </ head > ... In dist/main.bundle.js we may observe that output code went through babel because let has been replaced by var. dist/main.bundle.js: ... var test = \"123\" ; ... Start npm start Browse to http://localhost:4000/ and hit F12 to open browser development tools and observe console. Retrieve code for this chapter on github","title":"10-webpack"},{"location":"javascript/training/02-environment/10-webpack/#10-webpack","text":"","title":"10-webpack"},{"location":"javascript/training/02-environment/10-webpack/#basic-principles","text":"Webpack is installed through npm. What is installed: webpack webpack-cli webpack-dev-server babel-loader","title":"Basic principles"},{"location":"javascript/training/02-environment/10-webpack/#webpack","text":"Bundle creation. Webpack is configured through webpack.config.js configuration file. Webpack use babel-loader and take html + js content to create a dist bundle that contain optimized bundle.js and html content.","title":"Webpack"},{"location":"javascript/training/02-environment/10-webpack/#webpack-cli","text":"Use webpack in a terminal.","title":"Webpack-cli"},{"location":"javascript/training/02-environment/10-webpack/#webpack-dev-server","text":"Local development web server the list at localhost address on dedicated port and return index.html file from dist folder.","title":"Webpack-dev-server"},{"location":"javascript/training/02-environment/10-webpack/#installation","text":"","title":"Installation"},{"location":"javascript/training/02-environment/10-webpack/#prerequisite","text":"First, rename used file in previous chapter src/es6.js to src/index.js and remove dist folder if it exist and move index.html in src/: mv src/es6.js src/index.js rm -rf dist mv index.html src/ In src/index.html remove script section. src/index.html: <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > < title > Document </ title > </ head > < body > < h1 > My app </ h1 > </ body > </ html > Edit src/index.js as follow. src/index.js: let test = \"123\" ; console . log ( test ); In package.json remove line in script section that concern babel because we'll let webpack to take it in charge. package.json: { \"name\" : \"test\" , \"version\" : \"1.0.0\" , \"description\" : \"\" , \"main\" : \"index.js\" , \"scripts\" : { \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"keywords\" : [], \"author\" : \"\" , \"license\" : \"ISC\" , \"dependencies\" : { \"@babel/cli\" : \"^7.17.6\" , \"@babel/core\" : \"^7.17.5\" , \"@babel/preset-env\" : \"^7.16.11\" } }","title":"Prerequisite"},{"location":"javascript/training/02-environment/10-webpack/#launch","text":"npm i webpack webpack-cli webpack-dev-server babel-loader Add new file at the project root level called webpack.config.js: touch webpack.config.js webpack.config.js: const path = require ( \"path\" ); const HtmlWebpackPlugin = require ( \"html-webpack-plugin\" ); module . exports = { entry : path . resolve ( __dirname , \"src/index.js\" ), output : { path : path . resolve ( __dirname , \"dist\" ), filename : \"[name].bundle.js\" }, module : { rules : [ { test : /\\.js$/ , exclude : /node_modules/ , use : { loader : \"babel-loader\" } } ] }, plugins : [ new HtmlWebpackPlugin ({ template : path . resolve ( __dirname , \"src/index.html\" ) }) ], devtool : \"source-map\" , mode : \"development\" , devServer : { static : path . resolve ( __dirname , './dist' ), open : true , port : 4000 } }; Html webpack plugin automatically inject output js bundle to html file. Install: npm i html-webpack-plugin Edit package.json to use webpack and webpack server in script section. package.json ... \"scripts\" : { \"test\" : \"echo \\\"Error: no test specified\\\" && exit 1\" , \"webpack\" : \"webpack\" , \"start\" : \"webpack serve\" }, ...","title":"Launch"},{"location":"javascript/training/02-environment/10-webpack/#run","text":"npm run webpack Now a dist folder appear with an index.html file that does include call to output result of bundled js, main.bundle.js. dist/index.html: ... < head > ... < script defer src = \"main.bundle.js\" ></ script > ... </ head > ... In dist/main.bundle.js we may observe that output code went through babel because let has been replaced by var. dist/main.bundle.js: ... var test = \"123\" ; ...","title":"Run"},{"location":"javascript/training/02-environment/10-webpack/#start","text":"npm start Browse to http://localhost:4000/ and hit F12 to open browser development tools and observe console. Retrieve code for this chapter on github","title":"Start"},{"location":"javascript/training/02-environment/11-console/","text":"11-console Introduction console is an object which is part of javascript API. console.log is used to display information in browser console. console.error display output in red as an error. Error In a terminal from project's root folder start webpack: npm start Edit src/index.js as follow: let test = \"123\" ; console . log ( tost ); tost variable does not exist and it will through an error in browser's console. Open a browser and DevTools window (F12 in Google Chrome), click on console tab. You can click on link to browse in file that trigger the error, index.js:2:13 You are now in original source code even he supposed to have been optimized by webpack. It's thanks to dist/main.bundle.js.map file that ease debugging stage with showing us original source code.","title":"11-console"},{"location":"javascript/training/02-environment/11-console/#11-console","text":"","title":"11-console"},{"location":"javascript/training/02-environment/11-console/#introduction","text":"console is an object which is part of javascript API. console.log is used to display information in browser console. console.error display output in red as an error.","title":"Introduction"},{"location":"javascript/training/02-environment/11-console/#error","text":"In a terminal from project's root folder start webpack: npm start Edit src/index.js as follow: let test = \"123\" ; console . log ( tost ); tost variable does not exist and it will through an error in browser's console. Open a browser and DevTools window (F12 in Google Chrome), click on console tab. You can click on link to browse in file that trigger the error, index.js:2:13 You are now in original source code even he supposed to have been optimized by webpack. It's thanks to dist/main.bundle.js.map file that ease debugging stage with showing us original source code.","title":"Error"},{"location":"javascript/training/03-basis/12-intro-and-var/","text":"12-intro-and-var Introduction Chapter (03) objectives summary: Variables (var, let and const) Hoisting Types Operators Coercion Scope, Execution context Reference and value Environment is inherited from previous chapter and without particular mention, thereafter we will assume that the webpack environment has been launched with following command through a terminal at the root level of the project: npm start var src/index.js // declaration var firstName ; // undefined console . log ( firstName ); // initialisation firstName = 'Olivier' ; console . log ( firstName ); // reference error, does not exist // and stop execution if not commmented // console.log(lastName); // declare many variables at the same time var myvar1 , myvar2 ; myvar1 = 123 ; myvar2 = 55 ; console . log ( myvar1 , myvar2 );","title":"12-intro-and-var"},{"location":"javascript/training/03-basis/12-intro-and-var/#12-intro-and-var","text":"","title":"12-intro-and-var"},{"location":"javascript/training/03-basis/12-intro-and-var/#introduction","text":"Chapter (03) objectives summary: Variables (var, let and const) Hoisting Types Operators Coercion Scope, Execution context Reference and value Environment is inherited from previous chapter and without particular mention, thereafter we will assume that the webpack environment has been launched with following command through a terminal at the root level of the project: npm start","title":"Introduction"},{"location":"javascript/training/03-basis/12-intro-and-var/#var","text":"src/index.js // declaration var firstName ; // undefined console . log ( firstName ); // initialisation firstName = 'Olivier' ; console . log ( firstName ); // reference error, does not exist // and stop execution if not commmented // console.log(lastName); // declare many variables at the same time var myvar1 , myvar2 ; myvar1 = 123 ; myvar2 = 55 ; console . log ( myvar1 , myvar2 );","title":"var"},{"location":"javascript/training/03-basis/13-let-and-const/","text":"13-let-and-const var problematic let has been introduced to fix some var's problem. var allow to redeclare a variable that has already been declared with the same name, which could lead in very problematic issues in huge js file when you don't remember that a variable with a particular name has already been declared and you redeclare it with the same name for a different usage: var test = 123 ; var test = 456 ; Curly brace delimitate a code block but does not limit variable scope: { var test = 123 ; } console . log ( test ); let Reuse a name will raise a \"Identifier 'test' has already been declared.\" error in browser and webpack consoles: let test = 123 ; let test = 456 ; Scope of let is limited to code block, below example will raise a \"test not defined\" error: { let test = 123 ; } console . log ( test ); Use let instead of var is mostly advised. const const should be initialized on line where she's declared: const test = 123 ; Like expected const cannot be reassigned and trying to will raise a read only error. For an object reference it's possible to reassign a key inside a const object but not the reference himself: const simpleObject = { test : 123 }; console . log ( simpleObject . test ); // OK simpleObject . test = 456 ; console . log ( simpleObject . test ); // KO simpleObject = 456 Priority order of usage const let var","title":"13-let-and-const"},{"location":"javascript/training/03-basis/13-let-and-const/#13-let-and-const","text":"","title":"13-let-and-const"},{"location":"javascript/training/03-basis/13-let-and-const/#var-problematic","text":"let has been introduced to fix some var's problem. var allow to redeclare a variable that has already been declared with the same name, which could lead in very problematic issues in huge js file when you don't remember that a variable with a particular name has already been declared and you redeclare it with the same name for a different usage: var test = 123 ; var test = 456 ; Curly brace delimitate a code block but does not limit variable scope: { var test = 123 ; } console . log ( test );","title":"var problematic"},{"location":"javascript/training/03-basis/13-let-and-const/#let","text":"Reuse a name will raise a \"Identifier 'test' has already been declared.\" error in browser and webpack consoles: let test = 123 ; let test = 456 ; Scope of let is limited to code block, below example will raise a \"test not defined\" error: { let test = 123 ; } console . log ( test ); Use let instead of var is mostly advised.","title":"let"},{"location":"javascript/training/03-basis/13-let-and-const/#const","text":"const should be initialized on line where she's declared: const test = 123 ; Like expected const cannot be reassigned and trying to will raise a read only error. For an object reference it's possible to reassign a key inside a const object but not the reference himself: const simpleObject = { test : 123 }; console . log ( simpleObject . test ); // OK simpleObject . test = 456 ; console . log ( simpleObject . test ); // KO simpleObject = 456","title":"const"},{"location":"javascript/training/03-basis/13-let-and-const/#priority-order-of-usage","text":"const let var","title":"Priority order of usage"},{"location":"javascript/training/03-basis/14-hoisting/","text":"14-hoisting Introduction At first when a browser receive a javascript file, he read the file content entirely and initialize a stack. This behavior is due to the fact that Javascript is an interpreted language. This stack is a reserved memory place to store needed things (var, function, etc.) for running the script. This process of preparing the stack for running the script is called \"Hoisting\". Initialization var Initialized during the hoisting phase to \"undefined\". let and const They are hoisted and NOT , initialized to \"undefined\". Initialized during the execution phase.","title":"14-hoisting"},{"location":"javascript/training/03-basis/14-hoisting/#14-hoisting","text":"","title":"14-hoisting"},{"location":"javascript/training/03-basis/14-hoisting/#introduction","text":"At first when a browser receive a javascript file, he read the file content entirely and initialize a stack. This behavior is due to the fact that Javascript is an interpreted language. This stack is a reserved memory place to store needed things (var, function, etc.) for running the script. This process of preparing the stack for running the script is called \"Hoisting\".","title":"Introduction"},{"location":"javascript/training/03-basis/14-hoisting/#initialization","text":"","title":"Initialization"},{"location":"javascript/training/03-basis/14-hoisting/#var","text":"Initialized during the hoisting phase to \"undefined\".","title":"var"},{"location":"javascript/training/03-basis/14-hoisting/#let-and-const","text":"They are hoisted and NOT , initialized to \"undefined\". Initialized during the execution phase.","title":"let and const"},{"location":"javascript/training/03-basis/15-type/","text":"15-type Introduction Javascript is a dynamically typed language. Statically typed languages types are checked during the compilation phase: bool test = '123' // error Dynamic types, in Javascript, may change during the execution and is inferred from the type of the value assigned to the variable: let test = '123' // ok test = true // ok test = 55 // ok In Javascript there's two main family of types: primitive object Everything that's ain't an object is a primitive. Primitive boolean number // may contain any type of number, integer, float, etc. string of characters Undefined // not yet assigned, automatic, implicit Null // like 'Undefined' but set manually, explicit Symbol // since ES6 BigInt // since ES2020 Primitives are immutable which mean on a reassignment the variable name point to a new value, but the previous value is still held in memory. Hence the need for garbage collection. Object Literal object Array Function Date .. all the rest // all what is not a primitive, is an object Objects and arrays are mutable. A mutable object is an object whose state can be modified after it is created. Review src/index.js: // string, console -> string const test01 = \"jean\" ; console . log ( typeof test01 ); // number, console -> number const test02 = 123.4 ; console . log ( typeof test02 ); // null, console -> object = bug! const test03 = null ; console . log ( typeof test03 ); // undefined, console -> undefined // could not be a const like above // because const should be initialized // on the same line where declared let test04 ; console . log ( typeof test04 ); // Symbol, console -> symbol let test05 = Symbol (); console . log ( typeof test05 ); // Boolean, console -> boolean let test06 = true ; console . log ( typeof test06 ); // Literal object, console -> object const test07 = { test71 : \"Hello\" , test72 : \"World\" } console . log ( typeof test07 ); // Function object, console -> function, although it's an object const test08 = function () { console . log ( \"Hello\" ); }; console . log ( typeof test08 ); // Date object, console -> object const test09 = new Date (); console . log ( typeof test09 ); // Array object, console -> object const test10 = [ 1 , 2 , 3 ]; console . log ( typeof test10 );","title":"15-type"},{"location":"javascript/training/03-basis/15-type/#15-type","text":"","title":"15-type"},{"location":"javascript/training/03-basis/15-type/#introduction","text":"Javascript is a dynamically typed language. Statically typed languages types are checked during the compilation phase: bool test = '123' // error Dynamic types, in Javascript, may change during the execution and is inferred from the type of the value assigned to the variable: let test = '123' // ok test = true // ok test = 55 // ok In Javascript there's two main family of types: primitive object Everything that's ain't an object is a primitive.","title":"Introduction"},{"location":"javascript/training/03-basis/15-type/#primitive","text":"boolean number // may contain any type of number, integer, float, etc. string of characters Undefined // not yet assigned, automatic, implicit Null // like 'Undefined' but set manually, explicit Symbol // since ES6 BigInt // since ES2020 Primitives are immutable which mean on a reassignment the variable name point to a new value, but the previous value is still held in memory. Hence the need for garbage collection.","title":"Primitive"},{"location":"javascript/training/03-basis/15-type/#object","text":"Literal object Array Function Date .. all the rest // all what is not a primitive, is an object Objects and arrays are mutable. A mutable object is an object whose state can be modified after it is created.","title":"Object"},{"location":"javascript/training/03-basis/15-type/#review","text":"src/index.js: // string, console -> string const test01 = \"jean\" ; console . log ( typeof test01 ); // number, console -> number const test02 = 123.4 ; console . log ( typeof test02 ); // null, console -> object = bug! const test03 = null ; console . log ( typeof test03 ); // undefined, console -> undefined // could not be a const like above // because const should be initialized // on the same line where declared let test04 ; console . log ( typeof test04 ); // Symbol, console -> symbol let test05 = Symbol (); console . log ( typeof test05 ); // Boolean, console -> boolean let test06 = true ; console . log ( typeof test06 ); // Literal object, console -> object const test07 = { test71 : \"Hello\" , test72 : \"World\" } console . log ( typeof test07 ); // Function object, console -> function, although it's an object const test08 = function () { console . log ( \"Hello\" ); }; console . log ( typeof test08 ); // Date object, console -> object const test09 = new Date (); console . log ( typeof test09 ); // Array object, console -> object const test10 = [ 1 , 2 , 3 ]; console . log ( typeof test10 );","title":"Review"},{"location":"javascript/training/03-basis/16-operator-1/","text":"16-operator-1 Introduction Operators and the notions of precedence and associativity. Assignation let a = 1 ; console . log ( a ); Addition let a = 1 + 2 ; console . log ( a ); let b = 'hello, ' + 'world' ; console . log ( b ); Precedence Operator precedence Precedence determine in which order operators are executed. For, let's say: let a = 1 + 2 ; // 3 We have two operators, '=' and '+'. By clicking one link above you may find Table precedence section with \"weight\" of each operators that will determine execution order: for '+' it's 12 for '=' it's 2 Means that '+' will be executed before '='. For: let a = 2 + 2 * 5 ; // 12 for '*' it's 13 for '+' it's 12 for '=' it's 2 Means that first '*' then '+' and finally '='. Associativity Associativity rules apply when there's many operator with same precedence weight. Even though the calculation below gives the same result no matter which way you apply the operators, we still use it as an example. Multiplication and division operator have same precedence weight of 13. let a = 2 + 2 * 5 / 3 ; // 5.333333333333334 But then what matter here is what 'Associativity' column contain (in above mentioned table) which is, in our case, 'left-to-right'. Means, calculation start by multiplication and then division. Note that parenthesis is also an operator with maximal precedence weight. Conditional (ternary) operator (?) Source let a = 1 ; // a += 2; let b = a == 3 ? 5 : 6 ; console . log ( b ); // 6","title":"16-operator-1"},{"location":"javascript/training/03-basis/16-operator-1/#16-operator-1","text":"","title":"16-operator-1"},{"location":"javascript/training/03-basis/16-operator-1/#introduction","text":"Operators and the notions of precedence and associativity.","title":"Introduction"},{"location":"javascript/training/03-basis/16-operator-1/#assignation","text":"let a = 1 ; console . log ( a );","title":"Assignation"},{"location":"javascript/training/03-basis/16-operator-1/#addition","text":"let a = 1 + 2 ; console . log ( a ); let b = 'hello, ' + 'world' ; console . log ( b );","title":"Addition"},{"location":"javascript/training/03-basis/16-operator-1/#precedence","text":"Operator precedence Precedence determine in which order operators are executed. For, let's say: let a = 1 + 2 ; // 3 We have two operators, '=' and '+'. By clicking one link above you may find Table precedence section with \"weight\" of each operators that will determine execution order: for '+' it's 12 for '=' it's 2 Means that '+' will be executed before '='. For: let a = 2 + 2 * 5 ; // 12 for '*' it's 13 for '+' it's 12 for '=' it's 2 Means that first '*' then '+' and finally '='.","title":"Precedence"},{"location":"javascript/training/03-basis/16-operator-1/#associativity","text":"Associativity rules apply when there's many operator with same precedence weight. Even though the calculation below gives the same result no matter which way you apply the operators, we still use it as an example. Multiplication and division operator have same precedence weight of 13. let a = 2 + 2 * 5 / 3 ; // 5.333333333333334 But then what matter here is what 'Associativity' column contain (in above mentioned table) which is, in our case, 'left-to-right'. Means, calculation start by multiplication and then division. Note that parenthesis is also an operator with maximal precedence weight.","title":"Associativity"},{"location":"javascript/training/03-basis/16-operator-1/#conditional-ternary-operator","text":"Source let a = 1 ; // a += 2; let b = a == 3 ? 5 : 6 ; console . log ( b ); // 6","title":"Conditional (ternary) operator (?)"},{"location":"javascript/training/03-basis/17-coercion/","text":"17-coercion Introduction Due to the dynamic nature of JavaScript's typing. Type coercion Implicit coercion Implicit or automatic conversion. let a = 1 ; let b = 'hello' ; console . log ( a + b ); // 1hello Unable to convert hello to a number but 1 is easily converted to a string and then the result of the addition is the concatenated string. Another example when adding a number to a true boolean value (converted to 1 (false would be converted to zero)): let a = 1 ; let b = true ; console . log ( a + b ); // 2 Another example with string and boolean value, here converted to a string: let a = 'test' ; let b = false ; console . log ( a + b ); // testfalse Constructor conversion Types's constructor call with a value trying to convert, e.g. Number('hello'). console . log ( Number ( 'hello, world' )); // NaN = not a number let a = 1 ; let b = undefined ; console . log ( a + b ); // NaN console . log ( Number ( undefined )); // NaN console . log ( String ( undefined )); // undefined console . log ( String ( 1 )); // 1 console . log ( String ( null )); // null NaN, not a number, coercion not possible!","title":"17-coercion"},{"location":"javascript/training/03-basis/17-coercion/#17-coercion","text":"","title":"17-coercion"},{"location":"javascript/training/03-basis/17-coercion/#introduction","text":"Due to the dynamic nature of JavaScript's typing. Type coercion","title":"Introduction"},{"location":"javascript/training/03-basis/17-coercion/#implicit-coercion","text":"Implicit or automatic conversion. let a = 1 ; let b = 'hello' ; console . log ( a + b ); // 1hello Unable to convert hello to a number but 1 is easily converted to a string and then the result of the addition is the concatenated string. Another example when adding a number to a true boolean value (converted to 1 (false would be converted to zero)): let a = 1 ; let b = true ; console . log ( a + b ); // 2 Another example with string and boolean value, here converted to a string: let a = 'test' ; let b = false ; console . log ( a + b ); // testfalse","title":"Implicit coercion"},{"location":"javascript/training/03-basis/17-coercion/#constructor-conversion","text":"Types's constructor call with a value trying to convert, e.g. Number('hello'). console . log ( Number ( 'hello, world' )); // NaN = not a number let a = 1 ; let b = undefined ; console . log ( a + b ); // NaN console . log ( Number ( undefined )); // NaN console . log ( String ( undefined )); // undefined console . log ( String ( 1 )); // 1 console . log ( String ( null )); // null NaN, not a number, coercion not possible!","title":"Constructor conversion"},{"location":"javascript/training/03-basis/18-operator-2/","text":"18-operator-2 Introduction = VS == VS === and others logical operators Assignment (=) = assignment Set a variable value Equality (==) Not recommended! == equality Automatic (implicit) coercion, enabled! If compared values are of different type, coercion happen before comparison. A bit tricky to use/understand, usage of '===' is mostly advised. let a = 1 ; let b = 1 ; let c = 2 ; console . log ( a == b ); // true console . log ( a == c ); // false console . log ( a == true ); // true console . log ( 1 == '1' ); // true console . log ( 1 == '2' ); // false console . log ( true == \"true\" ); // false console . log ( undefined == false ); // false Strict equality (===) Recommended! === strict equality Automatic (implicit) coercion, disabled! Compared values should be same type and value to return true. let a = 1 ; let b = 1 ; let c = 2 ; console . log ( a === true ); // false console . log ( 1 === '1' ); // false console . log ( undefined === false ); // false Less than (<) console . log ( 1 < 2 < 3 ); // true = OK! console . log ( 3 < 2 < 1 ); // true = WHAT?? Explanation is associativity from left to right, so first 3 < 2 is executed which return false and then false < 1 equivalent to 0 < 1, which is true! Logical NOT (!) Invert logical value. let a = 0 ; console . log ( ! a ); // true console . log ( ! true ); // false console . log ( !! true ); // true Logical AND (&&) console . log ( true && true ); // true console . log ( true && false ); // false console . log ( false && true ); // false console . log ( false && false ); // false But when the two elements do not evaluate to boolean (true or false) the following rule must be used: - If the left element can be converted to false then it will be returned. In all other cases the second element will be returned: console . log ( '1' && '2' ); // 2 Other usage sample: let a = 1 , b = 1 ; if ( a && b ) { console . log ( 'ok' ); // ok } Logical OR (||) console . log ( true || true ); // true console . log ( true || false ); // true console . log ( false || true ); // true console . log ( false || false ); // false But when the two elements do not evaluate to boolean (true or false) the following rule must be used: - If the left element can be converted to true then it will be returned. In all other cases the second element will be returned: console . log ( false || undefined ); // undefined Other usage sample: let a = 1 , b = 0 ; if ( a || b ) { console . log ( 'ok' ); // ok } Summary console . log ( ! 2 || 'test' && 42 ); // 42","title":"18-operator-2"},{"location":"javascript/training/03-basis/18-operator-2/#18-operator-2","text":"","title":"18-operator-2"},{"location":"javascript/training/03-basis/18-operator-2/#introduction","text":"= VS == VS === and others logical operators","title":"Introduction"},{"location":"javascript/training/03-basis/18-operator-2/#assignment","text":"= assignment Set a variable value","title":"Assignment (=)"},{"location":"javascript/training/03-basis/18-operator-2/#equality","text":"Not recommended! == equality Automatic (implicit) coercion, enabled! If compared values are of different type, coercion happen before comparison. A bit tricky to use/understand, usage of '===' is mostly advised. let a = 1 ; let b = 1 ; let c = 2 ; console . log ( a == b ); // true console . log ( a == c ); // false console . log ( a == true ); // true console . log ( 1 == '1' ); // true console . log ( 1 == '2' ); // false console . log ( true == \"true\" ); // false console . log ( undefined == false ); // false","title":"Equality (==)"},{"location":"javascript/training/03-basis/18-operator-2/#strict-equality","text":"Recommended! === strict equality Automatic (implicit) coercion, disabled! Compared values should be same type and value to return true. let a = 1 ; let b = 1 ; let c = 2 ; console . log ( a === true ); // false console . log ( 1 === '1' ); // false console . log ( undefined === false ); // false","title":"Strict equality (===)"},{"location":"javascript/training/03-basis/18-operator-2/#less-than","text":"console . log ( 1 < 2 < 3 ); // true = OK! console . log ( 3 < 2 < 1 ); // true = WHAT?? Explanation is associativity from left to right, so first 3 < 2 is executed which return false and then false < 1 equivalent to 0 < 1, which is true!","title":"Less than (&lt;)"},{"location":"javascript/training/03-basis/18-operator-2/#logical-not","text":"Invert logical value. let a = 0 ; console . log ( ! a ); // true console . log ( ! true ); // false console . log ( !! true ); // true","title":"Logical NOT (!)"},{"location":"javascript/training/03-basis/18-operator-2/#logical-and","text":"console . log ( true && true ); // true console . log ( true && false ); // false console . log ( false && true ); // false console . log ( false && false ); // false But when the two elements do not evaluate to boolean (true or false) the following rule must be used: - If the left element can be converted to false then it will be returned. In all other cases the second element will be returned: console . log ( '1' && '2' ); // 2 Other usage sample: let a = 1 , b = 1 ; if ( a && b ) { console . log ( 'ok' ); // ok }","title":"Logical AND (&amp;&amp;)"},{"location":"javascript/training/03-basis/18-operator-2/#logical-or","text":"console . log ( true || true ); // true console . log ( true || false ); // true console . log ( false || true ); // true console . log ( false || false ); // false But when the two elements do not evaluate to boolean (true or false) the following rule must be used: - If the left element can be converted to true then it will be returned. In all other cases the second element will be returned: console . log ( false || undefined ); // undefined Other usage sample: let a = 1 , b = 0 ; if ( a || b ) { console . log ( 'ok' ); // ok }","title":"Logical OR (||)"},{"location":"javascript/training/03-basis/18-operator-2/#summary","text":"console . log ( ! 2 || 'test' && 42 ); // 42","title":"Summary"},{"location":"javascript/training/03-basis/19-value-and-reference/","text":"19-value-and-reference Introduction Variable (let, const, var) Address + value In a browser: Web API (e.g. XMLHttpRequest ) Event loop in charge to intercept event triggered by user interaction V8 JavaScript engine, execute but do not store anything Stack : Static memory allocation for storing primitives, small and fast. Heap : Dynamic memory allocation for storing objects, huge and slow. Value VS Reference const a = 'hello' // string -> stack(a => address1 | 'hello') const b = { c : 1 } // literal object -> stack(b => address2 | address3), heap(address3 | { c: 1 }) const c = a ; // assignment by values copied in new memory address stack(c => address4 | 'hello') const d = b ; // assignment by reference copied in new memory address stack that point on same value in heap -> stack(d => address5 | address3), heap(address3 | { c: 1 }) By Value let a = 1 ; let b = a ; console . log ({ a , b }); // { \"a\": 1, \"b\": 1 } b += 3 ; console . log ({ a , b }); // { \"a\": 1, \"b\": 4 } By Reference let a = { b : 1 }; let c = a ; c . b = 2 ; console . log ( a ); // { \"b\": 2 } console . log ( c ); // { \"b\": 2 }","title":"19-value-and-reference"},{"location":"javascript/training/03-basis/19-value-and-reference/#19-value-and-reference","text":"","title":"19-value-and-reference"},{"location":"javascript/training/03-basis/19-value-and-reference/#introduction","text":"Variable (let, const, var) Address + value In a browser: Web API (e.g. XMLHttpRequest ) Event loop in charge to intercept event triggered by user interaction V8 JavaScript engine, execute but do not store anything Stack : Static memory allocation for storing primitives, small and fast. Heap : Dynamic memory allocation for storing objects, huge and slow.","title":"Introduction"},{"location":"javascript/training/03-basis/19-value-and-reference/#value-vs-reference","text":"const a = 'hello' // string -> stack(a => address1 | 'hello') const b = { c : 1 } // literal object -> stack(b => address2 | address3), heap(address3 | { c: 1 }) const c = a ; // assignment by values copied in new memory address stack(c => address4 | 'hello') const d = b ; // assignment by reference copied in new memory address stack that point on same value in heap -> stack(d => address5 | address3), heap(address3 | { c: 1 })","title":"Value VS Reference"},{"location":"javascript/training/03-basis/19-value-and-reference/#by-value","text":"let a = 1 ; let b = a ; console . log ({ a , b }); // { \"a\": 1, \"b\": 1 } b += 3 ; console . log ({ a , b }); // { \"a\": 1, \"b\": 4 }","title":"By Value"},{"location":"javascript/training/03-basis/19-value-and-reference/#by-reference","text":"let a = { b : 1 }; let c = a ; c . b = 2 ; console . log ( a ); // { \"b\": 2 } console . log ( c ); // { \"b\": 2 }","title":"By Reference"},{"location":"javascript/training/04-control/20-condition/","text":"20-condition Introduction if, else, else if If const condition = true ; if ( condition === true ) { console . log ( 'here!' ); // here! } By omitting strict comparison in if condition, implicit coercion to boolean happen: const condition = true ; if ( condition ) { console . log ( 'here!' ); // here! } Even if 'condition' here in example is not a boolean, implicit coercion happen as well: With a number const condition = 0 ; if ( ! condition ) { console . log ( 'here!' ); // here! } With a string const condition1 = \"john\" ; // true const condition2 = \"\" ; // false if ( condition1 ) { console . log ( 'condition1' ); // condition1 } if ( ! condition2 ) { console . log ( 'not condition2' ); // not condition2 } else if the condition is not met, an \"else\" branch is provided to allow this case to be processed: //const test = \"John\"; const test = \"\" ; if ( test ) { console . log ( \"hello \" + test ); // hello John } else { console . log ( \"hello !\" ); // hello ! } else if Treat more than two cases (if/else) with \"else if\": //const test = \"John\"; const test = \"Paul\" ; //const test = \"\"; if ( test === \"John\" ) { console . log ( \"hello John\" ); // hello John } else if ( test === \"Paul\" ) { console . log ( \"hello Paul\" ); // hello Paul } else { console . log ( \"hello !\" ); // hello ! } Not recommended! But if only one instruction, not forced to delimit instruction block with curly braces: //const test = \"John\"; const test = \"Paul\" ; //const test = \"\"; if ( test === \"John\" ) console . log ( \"hello John\" ); // hello John else if ( test === \"Paul\" ) console . log ( \"hello Paul\" ); // hello Paul else console . log ( \"hello !\" ); // hello !","title":"20-condition"},{"location":"javascript/training/04-control/20-condition/#20-condition","text":"","title":"20-condition"},{"location":"javascript/training/04-control/20-condition/#introduction","text":"if, else, else if","title":"Introduction"},{"location":"javascript/training/04-control/20-condition/#if","text":"const condition = true ; if ( condition === true ) { console . log ( 'here!' ); // here! } By omitting strict comparison in if condition, implicit coercion to boolean happen: const condition = true ; if ( condition ) { console . log ( 'here!' ); // here! } Even if 'condition' here in example is not a boolean, implicit coercion happen as well: With a number const condition = 0 ; if ( ! condition ) { console . log ( 'here!' ); // here! } With a string const condition1 = \"john\" ; // true const condition2 = \"\" ; // false if ( condition1 ) { console . log ( 'condition1' ); // condition1 } if ( ! condition2 ) { console . log ( 'not condition2' ); // not condition2 }","title":"If"},{"location":"javascript/training/04-control/20-condition/#else","text":"if the condition is not met, an \"else\" branch is provided to allow this case to be processed: //const test = \"John\"; const test = \"\" ; if ( test ) { console . log ( \"hello \" + test ); // hello John } else { console . log ( \"hello !\" ); // hello ! }","title":"else"},{"location":"javascript/training/04-control/20-condition/#else-if","text":"Treat more than two cases (if/else) with \"else if\": //const test = \"John\"; const test = \"Paul\" ; //const test = \"\"; if ( test === \"John\" ) { console . log ( \"hello John\" ); // hello John } else if ( test === \"Paul\" ) { console . log ( \"hello Paul\" ); // hello Paul } else { console . log ( \"hello !\" ); // hello ! } Not recommended! But if only one instruction, not forced to delimit instruction block with curly braces: //const test = \"John\"; const test = \"Paul\" ; //const test = \"\"; if ( test === \"John\" ) console . log ( \"hello John\" ); // hello John else if ( test === \"Paul\" ) console . log ( \"hello Paul\" ); // hello Paul else console . log ( \"hello !\" ); // hello !","title":"else if"},{"location":"javascript/training/04-control/21-ternary-operator/","text":"21-ternary-operator Introduction Ternary operator: - '?' Syntax ( SRC ): condition ? exprIfTrue : exprIfFalse Simplify below syntax with ternary operator: const hour = 21 ; let isBeforeNoon ; if ( hour <= 12 ) { isBeforeNoon = true ; } else { isBeforeNoon = false ; } console . log ( isBeforeNoon ); // false With ternary operator: const hour = 10 ; const isBeforeNoon = hour <= 12 ? true : false ; console . log ( isBeforeNoon ); // true Nested ternary: const hour = 19 ; const test = hour >= 6 && hour < 12 ? 'morning' : hour >= 12 && hour < 18 ? 'afternoon' : 'night' ; console . log ( test ); // night","title":"21-ternary-operator"},{"location":"javascript/training/04-control/21-ternary-operator/#21-ternary-operator","text":"","title":"21-ternary-operator"},{"location":"javascript/training/04-control/21-ternary-operator/#introduction","text":"Ternary operator: - '?' Syntax ( SRC ): condition ? exprIfTrue : exprIfFalse Simplify below syntax with ternary operator: const hour = 21 ; let isBeforeNoon ; if ( hour <= 12 ) { isBeforeNoon = true ; } else { isBeforeNoon = false ; } console . log ( isBeforeNoon ); // false With ternary operator: const hour = 10 ; const isBeforeNoon = hour <= 12 ? true : false ; console . log ( isBeforeNoon ); // true Nested ternary: const hour = 19 ; const test = hour >= 6 && hour < 12 ? 'morning' : hour >= 12 && hour < 18 ? 'afternoon' : 'night' ; console . log ( test ); // night","title":"Introduction"},{"location":"javascript/training/04-control/22-switch/","text":"22-switch Introduction const wheel = 4 ; let vehicleType ; switch ( wheel ) { case 2 : { vehicleType = 'motorbike' ; break ; } case 4 : { let test = 123 ; console . log ( test ); // 123 vehicleType = 'car' ; break ; } default : { console . log ( test ); // e.g. with wheel = 6 -> error test is not defined vehicleType = 'truck' ; break ; } } console . log ( vehicleType ); // car break is needed otherwise all code below matching condition is also executed. Advantage of using curly brace for each case block is that make possible to declare let variable in it with limited scope. String It's not limited to number, it works with string as well: const wheel = \"4\" ; let vehicleType ; switch ( wheel ) { case 2 : { vehicleType = 'motorbike' ; break ; } case 4 : { let test = 123 ; console . log ( test ); vehicleType = 'car' ; break ; } case \"4\" : { vehicleType = 'car2' ; break ; } default : { console . log ( test ); vehicleType = 'truck' ; break ; } } console . log ( vehicleType ); // car2 Many cases Many cases with same treatment: const wheel = \"4\" ; let vehicleType ; switch ( wheel ) { case 2 : { vehicleType = 'motorbike' ; break ; } case 4 : case \"4\" : { let test = 123 ; console . log ( test ); // 123 vehicleType = 'car' ; break ; } default : { console . log ( test ); vehicleType = 'truck' ; break ; } } console . log ( vehicleType ); // car","title":"22-switch"},{"location":"javascript/training/04-control/22-switch/#22-switch","text":"","title":"22-switch"},{"location":"javascript/training/04-control/22-switch/#introduction","text":"const wheel = 4 ; let vehicleType ; switch ( wheel ) { case 2 : { vehicleType = 'motorbike' ; break ; } case 4 : { let test = 123 ; console . log ( test ); // 123 vehicleType = 'car' ; break ; } default : { console . log ( test ); // e.g. with wheel = 6 -> error test is not defined vehicleType = 'truck' ; break ; } } console . log ( vehicleType ); // car break is needed otherwise all code below matching condition is also executed. Advantage of using curly brace for each case block is that make possible to declare let variable in it with limited scope.","title":"Introduction"},{"location":"javascript/training/04-control/22-switch/#string","text":"It's not limited to number, it works with string as well: const wheel = \"4\" ; let vehicleType ; switch ( wheel ) { case 2 : { vehicleType = 'motorbike' ; break ; } case 4 : { let test = 123 ; console . log ( test ); vehicleType = 'car' ; break ; } case \"4\" : { vehicleType = 'car2' ; break ; } default : { console . log ( test ); vehicleType = 'truck' ; break ; } } console . log ( vehicleType ); // car2","title":"String"},{"location":"javascript/training/04-control/22-switch/#many-cases","text":"Many cases with same treatment: const wheel = \"4\" ; let vehicleType ; switch ( wheel ) { case 2 : { vehicleType = 'motorbike' ; break ; } case 4 : case \"4\" : { let test = 123 ; console . log ( test ); // 123 vehicleType = 'car' ; break ; } default : { console . log ( test ); vehicleType = 'truck' ; break ; } } console . log ( vehicleType ); // car","title":"Many cases"},{"location":"javascript/training/04-control/23-for/","text":"23-for Introduction For loop Increasing for ( let i = 0 ; i < 3 ; i ++ ) { console . log ( i ); } Output: 0 1 2 Decreasing for ( let i = 2 ; i >= 0 ; i -- ) { console . log ( i ); } Output: 2 1 0 Infinite for ( let i = 2 ; i >= 0 ; i ++ ) { console . log ( i ); } Output: 2 3 4 ... infinite loop -> crash browser! In case of crash (e.g. above code) open a new tab in Chrome hit shift + esc to open task manager identify the process that use the more CPU and kill him, then go back to previous crashed tab and reload page (after having changing the faulty code, obviously) Break for ( let i = 0 ; i < 3 ; i ++ ) { console . log ( i ); if ( i === 1 ) { break ; } } Output: 0 1 Continue Skip treatment after instruction to next iteration: for ( let i = 0 ; i < 3 ; i ++ ) { if ( i === 1 ) { continue ; } console . log ( i ); } Output: 0 2","title":"23-for"},{"location":"javascript/training/04-control/23-for/#23-for","text":"","title":"23-for"},{"location":"javascript/training/04-control/23-for/#introduction","text":"For loop","title":"Introduction"},{"location":"javascript/training/04-control/23-for/#increasing","text":"for ( let i = 0 ; i < 3 ; i ++ ) { console . log ( i ); } Output: 0 1 2","title":"Increasing"},{"location":"javascript/training/04-control/23-for/#decreasing","text":"for ( let i = 2 ; i >= 0 ; i -- ) { console . log ( i ); } Output: 2 1 0","title":"Decreasing"},{"location":"javascript/training/04-control/23-for/#infinite","text":"for ( let i = 2 ; i >= 0 ; i ++ ) { console . log ( i ); } Output: 2 3 4 ... infinite loop -> crash browser! In case of crash (e.g. above code) open a new tab in Chrome hit shift + esc to open task manager identify the process that use the more CPU and kill him, then go back to previous crashed tab and reload page (after having changing the faulty code, obviously)","title":"Infinite"},{"location":"javascript/training/04-control/23-for/#break","text":"for ( let i = 0 ; i < 3 ; i ++ ) { console . log ( i ); if ( i === 1 ) { break ; } } Output: 0 1","title":"Break"},{"location":"javascript/training/04-control/23-for/#continue","text":"Skip treatment after instruction to next iteration: for ( let i = 0 ; i < 3 ; i ++ ) { if ( i === 1 ) { continue ; } console . log ( i ); } Output: 0 2","title":"Continue"},{"location":"javascript/training/04-control/24-while/","text":"24-while Introduction While and do while loops While Enter in loop only if condition is true: let i = 0 ; while ( i < 10 ) { console . log ( i ); i ++ ; } console . log ( \"out\" ); Output: 0 1 2 3 4 5 6 7 8 9 out Do while Run at least once: let i = 0 ; do { console . log ( i ); i ++ ; } while ( i < 10 ); console . log ( \"out\" ); Output: 0 1 2 3 4 5 6 7 8 9 out","title":"24-while"},{"location":"javascript/training/04-control/24-while/#24-while","text":"","title":"24-while"},{"location":"javascript/training/04-control/24-while/#introduction","text":"While and do while loops","title":"Introduction"},{"location":"javascript/training/04-control/24-while/#while","text":"Enter in loop only if condition is true: let i = 0 ; while ( i < 10 ) { console . log ( i ); i ++ ; } console . log ( \"out\" ); Output: 0 1 2 3 4 5 6 7 8 9 out","title":"While"},{"location":"javascript/training/04-control/24-while/#do-while","text":"Run at least once: let i = 0 ; do { console . log ( i ); i ++ ; } while ( i < 10 ); console . log ( \"out\" ); Output: 0 1 2 3 4 5 6 7 8 9 out","title":"Do while"},{"location":"javascript/training/05-number/25-introduction/","text":"25-introduction Introduction Math. The Math object is a native object whose methods and properties allow the use of mathematical constants and functions. This object is not a function. SRC Declaration 3 ways to declare a number: Literal With Number() function With a constructor (new) const myNumber = 45 ; // literal declaration console . log ( myNumber ); // 45 console . log ( Number ( 85 )); // 85 console . log ( Number ( \"test\" )); // NaN, Not a Number console . log ( new Number ( 10 )); // Number {10}","title":"25-introduction"},{"location":"javascript/training/05-number/25-introduction/#25-introduction","text":"","title":"25-introduction"},{"location":"javascript/training/05-number/25-introduction/#introduction","text":"Math. The Math object is a native object whose methods and properties allow the use of mathematical constants and functions. This object is not a function. SRC","title":"Introduction"},{"location":"javascript/training/05-number/25-introduction/#declaration","text":"3 ways to declare a number: Literal With Number() function With a constructor (new) const myNumber = 45 ; // literal declaration console . log ( myNumber ); // 45 console . log ( Number ( 85 )); // 85 console . log ( Number ( \"test\" )); // NaN, Not a Number console . log ( new Number ( 10 )); // Number {10}","title":"Declaration"},{"location":"javascript/training/05-number/26-decimal/","text":"26-decimal Introduction Decimals, exponent and scientific notation const dec = 15.45 ; console . log ( dec ); // 15.45 Common case Due to how decimal number are stored as fraction, sometimes there's a little approximate: console . log ( 0.1 + 0.2 ); // 0.30000000000000004 console . log ( 0.1 + 0.1 ); // 0.2 console . log ( 5 / 3 ); // 1.6666666666666667 console . log ( 1 / 3 ); // 0.3333333333333333 A strategy used by some libraries (e.g. stripe for financial transaction) and to get correct result is to multiply each operand by 10 before main arithmetical operation and the to divide the final result by 10: console . log ( 0.1 + 0.2 ); // 0.30000000000000004 console . log (( 0.1 * 10 + 0.2 * 10 ) / 10 ); // 0.3 Scientific notation Exponent (10^n), useful to manipulate big number: const price = 1e6 ; console . log ( price ); // 1000000 Exponent operator Exponentiation: ** console . log ( 2 ** 3 ); // 8 console . log ( 3 ** 2 ); // 9","title":"26-decimal"},{"location":"javascript/training/05-number/26-decimal/#26-decimal","text":"","title":"26-decimal"},{"location":"javascript/training/05-number/26-decimal/#introduction","text":"Decimals, exponent and scientific notation const dec = 15.45 ; console . log ( dec ); // 15.45","title":"Introduction"},{"location":"javascript/training/05-number/26-decimal/#common-case","text":"Due to how decimal number are stored as fraction, sometimes there's a little approximate: console . log ( 0.1 + 0.2 ); // 0.30000000000000004 console . log ( 0.1 + 0.1 ); // 0.2 console . log ( 5 / 3 ); // 1.6666666666666667 console . log ( 1 / 3 ); // 0.3333333333333333 A strategy used by some libraries (e.g. stripe for financial transaction) and to get correct result is to multiply each operand by 10 before main arithmetical operation and the to divide the final result by 10: console . log ( 0.1 + 0.2 ); // 0.30000000000000004 console . log (( 0.1 * 10 + 0.2 * 10 ) / 10 ); // 0.3","title":"Common case"},{"location":"javascript/training/05-number/26-decimal/#scientific-notation","text":"Exponent (10^n), useful to manipulate big number: const price = 1e6 ; console . log ( price ); // 1000000","title":"Scientific notation"},{"location":"javascript/training/05-number/26-decimal/#exponent-operator","text":"Exponentiation: ** console . log ( 2 ** 3 ); // 8 console . log ( 3 ** 2 ); // 9","title":"Exponent operator"},{"location":"javascript/training/05-number/27-global-method/","text":"27-global-method Introduction We already have seen that we can convert a string to a number when possible: console . log ( Number ( \"38\" )); // 38 console . log ( Number ( \"38test\" )); // NaN parseInt() The parseInt() function is a little bit more power full than Number(): console . log ( parseInt ( \"38test\" )); // 38 console . log ( parseInt ( \"101\" , 2 )); // 5, 101 = 5 in binary console . log ( parseInt ( \"1a\" , 16 )); // 1a = 26 in hexadecimal console . log ( parseInt ( \"11.5\" )); // 11, because integer It convert input in specified base (default = 10, 2, 8, 16) output is in base 10. It convert all convertible character and stop when reach an none convertible one. parseFloat() console . log ( parseFloat ( \"11.5\" )); // 11.5 console . log ( parseFloat ( \"11\" )); // 11 + Another way to convert string to number is to add '+' sign before string: console . log ( + \"44\" ); // 44 console . log ( 1 + \"43\" ); // 143 console . log ( 1 + + \"43\" ); // 44 Test isFinite(): console . log ( isFinite ( 1 / 0 )); // false console . log ( isFinite ( 1 / 3 )); // true isNaN(): console . log ( isNaN ( \"test\" )); // true console . log ( isNaN ( \"44\" )); // false Undefined console . log ( Number . isNaN ( undefined )); // false console . log ( isNaN ( undefined )); // true, not advised, always use Number.isNaN()","title":"27-global-method"},{"location":"javascript/training/05-number/27-global-method/#27-global-method","text":"","title":"27-global-method"},{"location":"javascript/training/05-number/27-global-method/#introduction","text":"We already have seen that we can convert a string to a number when possible: console . log ( Number ( \"38\" )); // 38 console . log ( Number ( \"38test\" )); // NaN","title":"Introduction"},{"location":"javascript/training/05-number/27-global-method/#parseint","text":"The parseInt() function is a little bit more power full than Number(): console . log ( parseInt ( \"38test\" )); // 38 console . log ( parseInt ( \"101\" , 2 )); // 5, 101 = 5 in binary console . log ( parseInt ( \"1a\" , 16 )); // 1a = 26 in hexadecimal console . log ( parseInt ( \"11.5\" )); // 11, because integer It convert input in specified base (default = 10, 2, 8, 16) output is in base 10. It convert all convertible character and stop when reach an none convertible one.","title":"parseInt()"},{"location":"javascript/training/05-number/27-global-method/#parsefloat","text":"console . log ( parseFloat ( \"11.5\" )); // 11.5 console . log ( parseFloat ( \"11\" )); // 11","title":"parseFloat()"},{"location":"javascript/training/05-number/27-global-method/#_1","text":"Another way to convert string to number is to add '+' sign before string: console . log ( + \"44\" ); // 44 console . log ( 1 + \"43\" ); // 143 console . log ( 1 + + \"43\" ); // 44","title":"+"},{"location":"javascript/training/05-number/27-global-method/#test","text":"isFinite(): console . log ( isFinite ( 1 / 0 )); // false console . log ( isFinite ( 1 / 3 )); // true isNaN(): console . log ( isNaN ( \"test\" )); // true console . log ( isNaN ( \"44\" )); // false","title":"Test"},{"location":"javascript/training/05-number/27-global-method/#undefined","text":"console . log ( Number . isNaN ( undefined )); // false console . log ( isNaN ( undefined )); // true, not advised, always use Number.isNaN()","title":"Undefined"},{"location":"javascript/training/05-number/28-number-method/","text":"28-number-method Introduction Number object give access to methods for manipulating them. const a = 1 ; const b = new Number ( 1 ); console . log ( a ); // 1, primitive console . log ( b ); // Number {1}, object Primitive When invoking dot on a primitive the JS engine convert it first to a Number object with constructor, then apply called function and convert it back to primitive: const a = 1.555555555555555555 ; console . log ( a . toFixed ( 2 )); // 1.56, primitive string (black in console) (not a Number object) // to get a float back console . log ( parseFloat ( a . toFixed ( 2 ))); // 1.56, primitive number (blue in console) (not a Number object) Each primitive has his object equivalent. Number.isNaN() Without instantiating a number object we may invoke static method of JS native object Number, e.g. Number.isNanN(): const a = Number ( \"123\" ); const b = Number ( \"123test\" ); console . log ( Number . isNaN ( a )); // false, a string has been converted to number console . log ( Number . isNaN ( b )); // true, b string has not been converted to number Number.isInteger() console . log ( Number . isInteger ( 55 )); // true console . log ( Number . isInteger ( 55.0 )); // true, 55.0 equivalent to 55 so integer console . log ( Number . isInteger ( 55.1 )); // false ().toExponential() Convert a number to scientific notation (exponential): console . log (( 1577 ). toExponential ()); // 1.577e+3 console . log (( 1577 ). toExponential ( 2 )); // 1.58e+3 const b = 400000000000 ; console . log ( b . toExponential ()); // 4e+11 ().toFixed() Round to specified number of decimal: console . log (( 1.45e2 ). toFixed ( 2 )); // \"145.00\" (string) console . log (( 57.654981 ). toFixed ( 1 )); // \"57.7\" (string) console . log (( 53 ). toFixed ( 1 )); // \"53.0\" (string) ().toPrecision() Keep given precision without changing notation expect for integer of precision is lower than the needed numbers of digit to write it: console . log (( 1.79e2 ). toPrecision ( 1 )); // \"2e+2\" (string) console . log (( 795 ). toPrecision ( 2 )); // \"8.0e+2\" (string) console . log (( 795 ). toPrecision ( 3 )); // \"795\" (string) console . log (( 1.65498416 ). toPrecision ( 3 )); // \"1.65\" (string)","title":"28-number-method"},{"location":"javascript/training/05-number/28-number-method/#28-number-method","text":"","title":"28-number-method"},{"location":"javascript/training/05-number/28-number-method/#introduction","text":"Number object give access to methods for manipulating them. const a = 1 ; const b = new Number ( 1 ); console . log ( a ); // 1, primitive console . log ( b ); // Number {1}, object","title":"Introduction"},{"location":"javascript/training/05-number/28-number-method/#primitive","text":"When invoking dot on a primitive the JS engine convert it first to a Number object with constructor, then apply called function and convert it back to primitive: const a = 1.555555555555555555 ; console . log ( a . toFixed ( 2 )); // 1.56, primitive string (black in console) (not a Number object) // to get a float back console . log ( parseFloat ( a . toFixed ( 2 ))); // 1.56, primitive number (blue in console) (not a Number object) Each primitive has his object equivalent.","title":"Primitive"},{"location":"javascript/training/05-number/28-number-method/#numberisnan","text":"Without instantiating a number object we may invoke static method of JS native object Number, e.g. Number.isNanN(): const a = Number ( \"123\" ); const b = Number ( \"123test\" ); console . log ( Number . isNaN ( a )); // false, a string has been converted to number console . log ( Number . isNaN ( b )); // true, b string has not been converted to number","title":"Number.isNaN()"},{"location":"javascript/training/05-number/28-number-method/#numberisinteger","text":"console . log ( Number . isInteger ( 55 )); // true console . log ( Number . isInteger ( 55.0 )); // true, 55.0 equivalent to 55 so integer console . log ( Number . isInteger ( 55.1 )); // false","title":"Number.isInteger()"},{"location":"javascript/training/05-number/28-number-method/#toexponential","text":"Convert a number to scientific notation (exponential): console . log (( 1577 ). toExponential ()); // 1.577e+3 console . log (( 1577 ). toExponential ( 2 )); // 1.58e+3 const b = 400000000000 ; console . log ( b . toExponential ()); // 4e+11","title":"().toExponential()"},{"location":"javascript/training/05-number/28-number-method/#tofixed","text":"Round to specified number of decimal: console . log (( 1.45e2 ). toFixed ( 2 )); // \"145.00\" (string) console . log (( 57.654981 ). toFixed ( 1 )); // \"57.7\" (string) console . log (( 53 ). toFixed ( 1 )); // \"53.0\" (string)","title":"().toFixed()"},{"location":"javascript/training/05-number/28-number-method/#toprecision","text":"Keep given precision without changing notation expect for integer of precision is lower than the needed numbers of digit to write it: console . log (( 1.79e2 ). toPrecision ( 1 )); // \"2e+2\" (string) console . log (( 795 ). toPrecision ( 2 )); // \"8.0e+2\" (string) console . log (( 795 ). toPrecision ( 3 )); // \"795\" (string) console . log (( 1.65498416 ). toPrecision ( 3 )); // \"1.65\" (string)","title":"().toPrecision()"},{"location":"javascript/training/05-number/29-math-object/","text":"29-math-object Introduction Math is a native JavaScript object to handle math easily: const a = Math . pow ( 2 , 3 ); console . log ( a ); // 8 console . log ( Math . PI ); // 3.14.. console . log ( Math . ceil ( 2.5 )) // 3, up to next integer console . log ( Math . round ( 1.4 )); // 1 console . log ( Math . max ( 1 , 4 , 7 , 3 , 10 , 9 )); // 10 console . log ( Math . random ()); // random between 0 and 1 console . log ( Math . round ( Math . random () * 100 )); // random number between 0 and 100 console . log ( Math . round (( Math . random () * 10 )) % 5 ); // random number between 0 and 5","title":"29-math-object"},{"location":"javascript/training/05-number/29-math-object/#29-math-object","text":"","title":"29-math-object"},{"location":"javascript/training/05-number/29-math-object/#introduction","text":"Math is a native JavaScript object to handle math easily: const a = Math . pow ( 2 , 3 ); console . log ( a ); // 8 console . log ( Math . PI ); // 3.14.. console . log ( Math . ceil ( 2.5 )) // 3, up to next integer console . log ( Math . round ( 1.4 )); // 1 console . log ( Math . max ( 1 , 4 , 7 , 3 , 10 , 9 )); // 10 console . log ( Math . random ()); // random between 0 and 1 console . log ( Math . round ( Math . random () * 100 )); // random number between 0 and 100 console . log ( Math . round (( Math . random () * 10 )) % 5 ); // random number between 0 and 5","title":"Introduction"},{"location":"javascript/training/06-string/30-introduction/","text":"30-introduction Introduction Introduction to strings, JS primitive type. Course on Unicode text encoding (french) const a = 'hello' ; // Literal console . log ( a ); // hello const b = String ( 'hello' ); // Through special constructor function String() // Convert anything to a string console . log ( b ); // hello const c = new String ( 'hello' ); // Constructor function with new operator // Create an object that contain a string primitive console . log ( c ); // String {'hello'} const d = \"hello 'world'\" ; console . log ( d ); // hello 'world' const e = 'hello \\'world\\'' ; console . log ( d ); // hello 'world' // \\n = new line, \\t = tab const f = 'hello\\n' + '\\tworld' ; console . log ( f ); // hello // world","title":"30-introduction"},{"location":"javascript/training/06-string/30-introduction/#30-introduction","text":"","title":"30-introduction"},{"location":"javascript/training/06-string/30-introduction/#introduction","text":"Introduction to strings, JS primitive type. Course on Unicode text encoding (french) const a = 'hello' ; // Literal console . log ( a ); // hello const b = String ( 'hello' ); // Through special constructor function String() // Convert anything to a string console . log ( b ); // hello const c = new String ( 'hello' ); // Constructor function with new operator // Create an object that contain a string primitive console . log ( c ); // String {'hello'} const d = \"hello 'world'\" ; console . log ( d ); // hello 'world' const e = 'hello \\'world\\'' ; console . log ( d ); // hello 'world' // \\n = new line, \\t = tab const f = 'hello\\n' + '\\tworld' ; console . log ( f ); // hello // world","title":"Introduction"},{"location":"javascript/training/06-string/31-template-literal/","text":"31-template-literal Introduction Use of Backtick (grave accent) ` to declare a template literal: const a = `I am a string` ; console . log ( a ); Output: I am a string Interpolation Interpolation inside a string: const a = 'BIG' ; const b = `I am ${ a } a string` ; console . log ( b ); Output: I am BIG a string Anything in between ${} will be interpolated as a string: const a = `I am ${ 1 + 1 } a string` ; console . log ( a ); Output: I am 2 a string Double interpolation const tired = true ; const action = 'sleep' ; const stateOfTiredness = `Paul is ${ tired ? `tired and would like to ${ action } !` : `in fine fettle!` } ` ; console . log ( stateOfTiredness ); // Paul is tired and would like to sleep!","title":"31-template-literal"},{"location":"javascript/training/06-string/31-template-literal/#31-template-literal","text":"","title":"31-template-literal"},{"location":"javascript/training/06-string/31-template-literal/#introduction","text":"Use of Backtick (grave accent) ` to declare a template literal: const a = `I am a string` ; console . log ( a ); Output: I am a string","title":"Introduction"},{"location":"javascript/training/06-string/31-template-literal/#interpolation","text":"Interpolation inside a string: const a = 'BIG' ; const b = `I am ${ a } a string` ; console . log ( b ); Output: I am BIG a string Anything in between ${} will be interpolated as a string: const a = `I am ${ 1 + 1 } a string` ; console . log ( a ); Output: I am 2 a string","title":"Interpolation"},{"location":"javascript/training/06-string/31-template-literal/#double-interpolation","text":"const tired = true ; const action = 'sleep' ; const stateOfTiredness = `Paul is ${ tired ? `tired and would like to ${ action } !` : `in fine fettle!` } ` ; console . log ( stateOfTiredness ); // Paul is tired and would like to sleep!","title":"Double interpolation"},{"location":"javascript/training/06-string/32-index-length/","text":"32-index-length Introduction Index and length properties on string. const a = 'test' ; console . log ( a . length ); // 4 console . log ( a [ 1 ]); // e Index [#] retrieve character at given index like for an array: const a = 'test' ; console . log ( a [ 0 ]); // t console . log ( a [ 1 ]); // e console . log ( a [ 2 ]); // s console . log ( a [ 3 ]); // t Another sample: const a = 'Hello ! \ud83d\ude00' ; console . log ( a [ 0 ]); // H console . log ( a [ 6 ]); // ! console . log ( a [ 8 ]); // \ufffd For emoji at index 8 output = '\ufffd' due to 32 bits encoded char and a[8] represent the 16 first bits of the character which do not have an Unicode significant value. Length Return string length: const a = 'test' ; console . log ( a . length ); // 4 Last char of a sring const a = 'test' ; console . log ( a [ a . length - 1 ]); // t String object With a string object we may retrieve each characters with their respective index and also the length property. const b = new String ( 'test' ); console . log ( b ); Output: String {'test'} 0: \"t\" 1: \"e\" 2: \"s\" 3: \"t\" length: 4","title":"32-index-length"},{"location":"javascript/training/06-string/32-index-length/#32-index-length","text":"","title":"32-index-length"},{"location":"javascript/training/06-string/32-index-length/#introduction","text":"Index and length properties on string. const a = 'test' ; console . log ( a . length ); // 4 console . log ( a [ 1 ]); // e","title":"Introduction"},{"location":"javascript/training/06-string/32-index-length/#index","text":"[#] retrieve character at given index like for an array: const a = 'test' ; console . log ( a [ 0 ]); // t console . log ( a [ 1 ]); // e console . log ( a [ 2 ]); // s console . log ( a [ 3 ]); // t Another sample: const a = 'Hello ! \ud83d\ude00' ; console . log ( a [ 0 ]); // H console . log ( a [ 6 ]); // ! console . log ( a [ 8 ]); // \ufffd For emoji at index 8 output = '\ufffd' due to 32 bits encoded char and a[8] represent the 16 first bits of the character which do not have an Unicode significant value.","title":"Index"},{"location":"javascript/training/06-string/32-index-length/#length","text":"Return string length: const a = 'test' ; console . log ( a . length ); // 4","title":"Length"},{"location":"javascript/training/06-string/32-index-length/#last-char-of-a-sring","text":"const a = 'test' ; console . log ( a [ a . length - 1 ]); // t","title":"Last char of a sring"},{"location":"javascript/training/06-string/32-index-length/#string-object","text":"With a string object we may retrieve each characters with their respective index and also the length property. const b = new String ( 'test' ); console . log ( b ); Output: String {'test'} 0: \"t\" 1: \"e\" 2: \"s\" 3: \"t\" length: 4","title":"String object"},{"location":"javascript/training/06-string/33-method/","text":"33-method Introduction Different methods available for string treatment. All those methods are case sensitive and return a new string because primitive are immutable. Calling a method on a character string results in the creation of a temporary String object by the JavaScript interpreter on which the method is located because a primitive has, by definition, no method! Not all string methods are available on the 2015 version of JavaScript. It is by using Babel and Webpack that we can access all the features! charAt Return char at given position: console . log ( 'I am the sun' . charAt ( 0 )); // I endsWith Return true if string end with given parameter: console . log ( 'I am the sun' . endsWith ( 'sun' )); // true startWith Return true if string start with given parameter: console . log ( 'I am the sun' . startsWith ( 'You' )); // false indexOf Return position of given parameter: console . log ( 'I am the sun' . indexOf ( 'the' )); // 5 console . log ( 'I am the sun' . indexOf ( 'the' , 6 )); // start at 6, not found = -1 If not found return -1. replace Replace portion of string with given parameter: console . log ( 'I am the sun' . replace ( 'I am' , 'You are' )); // You are the sun a = 'I am the sun' ; a . replace ( 'I am' , 'You are' ); console . log ( a ); // I am the sun, because string is a primitive immutable search Returns the first iteration of the given parameter found: console . log ( 'I am the sun' . search ( 'am' )); // 2 slice Returns cut portion of string between two given indexes as parameters: const a = 'I am the sun' ; const b = a . slice ( 1 , a . length ); const c = a . slice ( 1 , - 1 ); console . log ( b ); // am the sun console . log ( c ); // am the su trim Remove all spaces before/after a string: console . log ( ' I am the sun ' . trim ()); // I am the sun split Returns an array of values separated by the separator given as parameter: console . log ( 'sun,earth,moon' . split ( ',' )); // ['sun', 'earth', 'moon'] charCodeAt Get UTF-16 character code at given position: console . log ( 'Hello \ud83d\udc4d' . charCodeAt ( 0 )); // 72 console . log ( 'Hello \ud83d\udc4d' . charCodeAt ( 6 )); // 55357 console . log ( 'Hello \ud83d\udc4d' . charCodeAt ( 7 )); // 56397 codePointAt Get Unicode code of character at given position: console . log ( 'Hello \ud83d\udc4d' . codePointAt ( 6 )); // 128077 fromCharCode Create a string from UTF-16 code: console . log ( String . fromCharCode ( 72 )); // H fromCodePoint Create a string from Unicode code: console . log ( String . fromCodePoint ( 128077 )); // \ud83d\udc4d","title":"33-method"},{"location":"javascript/training/06-string/33-method/#33-method","text":"","title":"33-method"},{"location":"javascript/training/06-string/33-method/#introduction","text":"Different methods available for string treatment. All those methods are case sensitive and return a new string because primitive are immutable. Calling a method on a character string results in the creation of a temporary String object by the JavaScript interpreter on which the method is located because a primitive has, by definition, no method! Not all string methods are available on the 2015 version of JavaScript. It is by using Babel and Webpack that we can access all the features!","title":"Introduction"},{"location":"javascript/training/06-string/33-method/#charat","text":"Return char at given position: console . log ( 'I am the sun' . charAt ( 0 )); // I","title":"charAt"},{"location":"javascript/training/06-string/33-method/#endswith","text":"Return true if string end with given parameter: console . log ( 'I am the sun' . endsWith ( 'sun' )); // true","title":"endsWith"},{"location":"javascript/training/06-string/33-method/#startwith","text":"Return true if string start with given parameter: console . log ( 'I am the sun' . startsWith ( 'You' )); // false","title":"startWith"},{"location":"javascript/training/06-string/33-method/#indexof","text":"Return position of given parameter: console . log ( 'I am the sun' . indexOf ( 'the' )); // 5 console . log ( 'I am the sun' . indexOf ( 'the' , 6 )); // start at 6, not found = -1 If not found return -1.","title":"indexOf"},{"location":"javascript/training/06-string/33-method/#replace","text":"Replace portion of string with given parameter: console . log ( 'I am the sun' . replace ( 'I am' , 'You are' )); // You are the sun a = 'I am the sun' ; a . replace ( 'I am' , 'You are' ); console . log ( a ); // I am the sun, because string is a primitive immutable","title":"replace"},{"location":"javascript/training/06-string/33-method/#search","text":"Returns the first iteration of the given parameter found: console . log ( 'I am the sun' . search ( 'am' )); // 2","title":"search"},{"location":"javascript/training/06-string/33-method/#slice","text":"Returns cut portion of string between two given indexes as parameters: const a = 'I am the sun' ; const b = a . slice ( 1 , a . length ); const c = a . slice ( 1 , - 1 ); console . log ( b ); // am the sun console . log ( c ); // am the su","title":"slice"},{"location":"javascript/training/06-string/33-method/#trim","text":"Remove all spaces before/after a string: console . log ( ' I am the sun ' . trim ()); // I am the sun","title":"trim"},{"location":"javascript/training/06-string/33-method/#split","text":"Returns an array of values separated by the separator given as parameter: console . log ( 'sun,earth,moon' . split ( ',' )); // ['sun', 'earth', 'moon']","title":"split"},{"location":"javascript/training/06-string/33-method/#charcodeat","text":"Get UTF-16 character code at given position: console . log ( 'Hello \ud83d\udc4d' . charCodeAt ( 0 )); // 72 console . log ( 'Hello \ud83d\udc4d' . charCodeAt ( 6 )); // 55357 console . log ( 'Hello \ud83d\udc4d' . charCodeAt ( 7 )); // 56397","title":"charCodeAt"},{"location":"javascript/training/06-string/33-method/#codepointat","text":"Get Unicode code of character at given position: console . log ( 'Hello \ud83d\udc4d' . codePointAt ( 6 )); // 128077","title":"codePointAt"},{"location":"javascript/training/06-string/33-method/#fromcharcode","text":"Create a string from UTF-16 code: console . log ( String . fromCharCode ( 72 )); // H","title":"fromCharCode"},{"location":"javascript/training/06-string/33-method/#fromcodepoint","text":"Create a string from Unicode code: console . log ( String . fromCodePoint ( 128077 )); // \ud83d\udc4d","title":"fromCodePoint"},{"location":"javascript/training/06-string/34-regex/","text":"34-regex Introduction Regular expression are available with string methods. Two ways for declaring regex: Literal With constructor, new RegExp() test Test for matching pattern: const a = 'I am 123 the sun' ; const evaluate1 = /am/ . test ( a ); console . log ( evaluate1 ); // true const evaluate2 = /\\d{1,3}/ . test ( a ); console . log ( evaluate2 ); // true exec Extract part of sting: // e.g. extract a file name without extension from a string const a = \"picture_12345.jpg\" const evaluate = /(.*)\\./ . exec ( a ); console . log ( evaluate ); // ['picture_12345.', 'picture_12345', ..] match Return an array that contain result of search: const a = 'I am 123 the sun' ; console . log ( a . match ( /am/ )); // ['am', index: 2, input: 'I am 123 the sun', ..] split Split between matching pattern: const a = 'I am 123 the sun' ; console . log ( a . split ( /am/ )); // ['I ', ' 123 the sun'] replace Replace matching pattern: const a = 'I am 123 the sun' ; console . log ( a . replace ( /am/ , 'hello' )); // I hello 123 the sun","title":"34-regex"},{"location":"javascript/training/06-string/34-regex/#34-regex","text":"","title":"34-regex"},{"location":"javascript/training/06-string/34-regex/#introduction","text":"Regular expression are available with string methods. Two ways for declaring regex: Literal With constructor, new RegExp()","title":"Introduction"},{"location":"javascript/training/06-string/34-regex/#test","text":"Test for matching pattern: const a = 'I am 123 the sun' ; const evaluate1 = /am/ . test ( a ); console . log ( evaluate1 ); // true const evaluate2 = /\\d{1,3}/ . test ( a ); console . log ( evaluate2 ); // true","title":"test"},{"location":"javascript/training/06-string/34-regex/#exec","text":"Extract part of sting: // e.g. extract a file name without extension from a string const a = \"picture_12345.jpg\" const evaluate = /(.*)\\./ . exec ( a ); console . log ( evaluate ); // ['picture_12345.', 'picture_12345', ..]","title":"exec"},{"location":"javascript/training/06-string/34-regex/#match","text":"Return an array that contain result of search: const a = 'I am 123 the sun' ; console . log ( a . match ( /am/ )); // ['am', index: 2, input: 'I am 123 the sun', ..]","title":"match"},{"location":"javascript/training/06-string/34-regex/#split","text":"Split between matching pattern: const a = 'I am 123 the sun' ; console . log ( a . split ( /am/ )); // ['I ', ' 123 the sun']","title":"split"},{"location":"javascript/training/06-string/34-regex/#replace","text":"Replace matching pattern: const a = 'I am 123 the sun' ; console . log ( a . replace ( /am/ , 'hello' )); // I hello 123 the sun","title":"replace"},{"location":"javascript/training/07-object/35-introduction/","text":"35-introduction Introduction Object is everything that's ain't a primitive. Object is a unordered collection of key value. Key is a String which serves as an identifier. Value maybe a primitive or an object, nested, stored in heap (not stack). Object maybe literal or special object like function (function are executable object that maybe stored in variable). Literal The literal manner of declaring an object: const a = {}; console . log ( a ); // {} Object Using Object() function to declare an object: const a = Object (); console . log ( a ); // {} New Using new operator constructor function on Object(): const a = new Object (); console . log ( a ); // {}","title":"35-introduction"},{"location":"javascript/training/07-object/35-introduction/#35-introduction","text":"","title":"35-introduction"},{"location":"javascript/training/07-object/35-introduction/#introduction","text":"Object is everything that's ain't a primitive. Object is a unordered collection of key value. Key is a String which serves as an identifier. Value maybe a primitive or an object, nested, stored in heap (not stack). Object maybe literal or special object like function (function are executable object that maybe stored in variable).","title":"Introduction"},{"location":"javascript/training/07-object/35-introduction/#literal","text":"The literal manner of declaring an object: const a = {}; console . log ( a ); // {}","title":"Literal"},{"location":"javascript/training/07-object/35-introduction/#object","text":"Using Object() function to declare an object: const a = Object (); console . log ( a ); // {}","title":"Object"},{"location":"javascript/training/07-object/35-introduction/#new","text":"Using new operator constructor function on Object(): const a = new Object (); console . log ( a ); // {}","title":"New"},{"location":"javascript/training/07-object/36-property/","text":"36-property Introduction Declaration of different properties and different manner to access them: const hearth = { population : 7e9 , satellite : 'Moon' , temperature : { min : - 70 , max : 60 }, isOld : false , // function in an object = method getTemperature : function () { console . log ( \"14.5\" ); }, // Since ES6 function isn't needed anymore to declare a function getHumidity () { console . log ( '98' ); } } console . log ( hearth ); // {population: 70000000, satellite: 'Moon', temperature: {\u2026}, isOld: false} console . log ( hearth . population ); 7000000000 console . log ( hearth [ \"population\" ]); 7000000000 const populationProperty = 'population' ; console . log ( hearth [ populationProperty ]); 7000000000 // copy object reference const copy = hearth ; // modifying copy modify original because both reference same object copy . population = 7.5e9 console . log ( hearth [ populationProperty ]); 7500000000 // Invoke method = function that belong to an object copy . getTemperature (); // 14.5 hearth . getHumidity (); // 98","title":"36-property"},{"location":"javascript/training/07-object/36-property/#36-property","text":"","title":"36-property"},{"location":"javascript/training/07-object/36-property/#introduction","text":"Declaration of different properties and different manner to access them: const hearth = { population : 7e9 , satellite : 'Moon' , temperature : { min : - 70 , max : 60 }, isOld : false , // function in an object = method getTemperature : function () { console . log ( \"14.5\" ); }, // Since ES6 function isn't needed anymore to declare a function getHumidity () { console . log ( '98' ); } } console . log ( hearth ); // {population: 70000000, satellite: 'Moon', temperature: {\u2026}, isOld: false} console . log ( hearth . population ); 7000000000 console . log ( hearth [ \"population\" ]); 7000000000 const populationProperty = 'population' ; console . log ( hearth [ populationProperty ]); 7000000000 // copy object reference const copy = hearth ; // modifying copy modify original because both reference same object copy . population = 7.5e9 console . log ( hearth [ populationProperty ]); 7500000000 // Invoke method = function that belong to an object copy . getTemperature (); // 14.5 hearth . getHumidity (); // 98","title":"Introduction"},{"location":"javascript/training/07-object/37-syntax-shortcut/","text":"37-syntax-shortcut Introduction Since ES6, in an object, if a key is not initialized with a value, it presupposes that a key of the same name has already been initialized before: const population = 7e9 ; const satellite = \"Moon\" ; const temperature = { min : - 70 , max : 60 }; // Before ES6 const hearth1 = { population : population , satellite : satellite , temperature : temperature , isOld : false } console . log ( hearth1 ); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false} // Since ES6 const hearth2 = { population , satellite , temperature , isOld : false } console . log ( hearth2 ); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false} // Object's key from a const, at declaration const pop = \"population\" ; const hearth3 = { [ pop ] : population , satellite , temperature , isOld : false } console . log ( hearth3 ); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false} // Object's key from a const, after declaration const hearth4 = { satellite , temperature , isOld : false } hearth4 [ pop ] = population ; console . log ( hearth4 ); // {satellite: 'Moon', temperature: {\u2026}, isOld: false, population: 7000000000}","title":"37-syntax-shortcut"},{"location":"javascript/training/07-object/37-syntax-shortcut/#37-syntax-shortcut","text":"","title":"37-syntax-shortcut"},{"location":"javascript/training/07-object/37-syntax-shortcut/#introduction","text":"Since ES6, in an object, if a key is not initialized with a value, it presupposes that a key of the same name has already been initialized before: const population = 7e9 ; const satellite = \"Moon\" ; const temperature = { min : - 70 , max : 60 }; // Before ES6 const hearth1 = { population : population , satellite : satellite , temperature : temperature , isOld : false } console . log ( hearth1 ); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false} // Since ES6 const hearth2 = { population , satellite , temperature , isOld : false } console . log ( hearth2 ); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false} // Object's key from a const, at declaration const pop = \"population\" ; const hearth3 = { [ pop ] : population , satellite , temperature , isOld : false } console . log ( hearth3 ); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false} // Object's key from a const, after declaration const hearth4 = { satellite , temperature , isOld : false } hearth4 [ pop ] = population ; console . log ( hearth4 ); // {satellite: 'Moon', temperature: {\u2026}, isOld: false, population: 7000000000}","title":"Introduction"},{"location":"javascript/training/07-object/38-decomposition/","text":"38-decomposition Introduction Extract key value from an object: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { population , satellite , temperature } = hearth ; console . log ( population , satellite , temperature ); // 7000000000 'Moon' {min: -70, max: 40} Alias If identifier has already been declared: const population = 50 ; // first declaration of population const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { population : population2 , satellite , temperature } = hearth ; // alias console . log ( population2 , satellite , temperature ); // 7000000000 'Moon' {min: -70, max: 40} Rest operator 3 dots '...' (Rest operator) to get the rest of decomposed object: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { temperature , ... restArgs } = hearth ; // spread operator to get the rest console . log ( temperature , restArgs ); // {min: -70, max: 40} // {population: 7000000000, satellite: 'Moon', isOld: false} Rest operator should always be at the end of the decomposition. Default value const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { temperature , isOld , isYoung = true } = hearth ; // default value for isYoung which is not defined in hearth object console . log ( temperature , isOld , isYoung ); // {min: -70, max: 40} false true Nested const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { temperature : { min } } = hearth ; console . log ( min ); // -70 console . log ( temperature ); // Error: temperature is not defined","title":"38-decomposition"},{"location":"javascript/training/07-object/38-decomposition/#38-decomposition","text":"","title":"38-decomposition"},{"location":"javascript/training/07-object/38-decomposition/#introduction","text":"Extract key value from an object: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { population , satellite , temperature } = hearth ; console . log ( population , satellite , temperature ); // 7000000000 'Moon' {min: -70, max: 40}","title":"Introduction"},{"location":"javascript/training/07-object/38-decomposition/#alias","text":"If identifier has already been declared: const population = 50 ; // first declaration of population const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { population : population2 , satellite , temperature } = hearth ; // alias console . log ( population2 , satellite , temperature ); // 7000000000 'Moon' {min: -70, max: 40}","title":"Alias"},{"location":"javascript/training/07-object/38-decomposition/#rest-operator","text":"3 dots '...' (Rest operator) to get the rest of decomposed object: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { temperature , ... restArgs } = hearth ; // spread operator to get the rest console . log ( temperature , restArgs ); // {min: -70, max: 40} // {population: 7000000000, satellite: 'Moon', isOld: false} Rest operator should always be at the end of the decomposition.","title":"Rest operator"},{"location":"javascript/training/07-object/38-decomposition/#default-value","text":"const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { temperature , isOld , isYoung = true } = hearth ; // default value for isYoung which is not defined in hearth object console . log ( temperature , isOld , isYoung ); // {min: -70, max: 40} false true","title":"Default value"},{"location":"javascript/training/07-object/38-decomposition/#nested","text":"const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; const { temperature : { min } } = hearth ; console . log ( min ); // -70 console . log ( temperature ); // Error: temperature is not defined","title":"Nested"},{"location":"javascript/training/07-object/39-test-existence/","text":"39-test-existence Introduction Test the existence and value of a property Test for value If property contain a value, then it's converted to true: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; // test for value if ( hearth . population ) { console . log ( 'OK' ); // OK } else { console . log ( 'KO' ); } Test for key in operator: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; // test for key if ( 'population' in hearth ) { console . log ( 'OK' ); // OK } else { console . log ( 'KO' ); } hasOwnProperty Native method available on all object: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; // hasOwnProperty if ( hearth . hasOwnProperty ( 'population' )) { console . log ( 'OK' ); // OK } else { console . log ( 'KO' ); } falsy values As a reminder, falsy values in JavaScript are false, null, undefined, 0, NaN, '', \"\" and ``. Also as a reminder, a value is said to be false because it is automatically converted to false, especially in if \u2026 else.","title":"39-test-existence"},{"location":"javascript/training/07-object/39-test-existence/#39-test-existence","text":"","title":"39-test-existence"},{"location":"javascript/training/07-object/39-test-existence/#introduction","text":"Test the existence and value of a property","title":"Introduction"},{"location":"javascript/training/07-object/39-test-existence/#test-for-value","text":"If property contain a value, then it's converted to true: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; // test for value if ( hearth . population ) { console . log ( 'OK' ); // OK } else { console . log ( 'KO' ); }","title":"Test for value"},{"location":"javascript/training/07-object/39-test-existence/#test-for-key","text":"in operator: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; // test for key if ( 'population' in hearth ) { console . log ( 'OK' ); // OK } else { console . log ( 'KO' ); }","title":"Test for key"},{"location":"javascript/training/07-object/39-test-existence/#hasownproperty","text":"Native method available on all object: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; // hasOwnProperty if ( hearth . hasOwnProperty ( 'population' )) { console . log ( 'OK' ); // OK } else { console . log ( 'KO' ); }","title":"hasOwnProperty"},{"location":"javascript/training/07-object/39-test-existence/#falsy-values","text":"As a reminder, falsy values in JavaScript are false, null, undefined, 0, NaN, '', \"\" and ``. Also as a reminder, a value is said to be false because it is automatically converted to false, especially in if \u2026 else.","title":"falsy values"},{"location":"javascript/training/07-object/40-discard-property/","text":"40-discard-property Introduction Delete or discard properties. First, as a recap, we add three new properties in three different ways: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; // add properties, 3 ways hearth . liveable1 = true ; hearth [ \"liveable2\" ] = true ; const liveable3 = \"liveable3\" ; hearth [ liveable3 ] = true ; console . log ( hearth ); // {..., liveable1: true, liveable2: true, liveable3: true, ...} Remove property Operator delete: delete hearth . liveable1 ; console . log ( hearth ); // {..., liveable2: true, liveable3: true, ...} Set property to no value Set to 'null', not 'undefined' as it was never assigned: hearth . liveable2 = null ; console . log ( hearth ); // {..., liveable2: null, liveable3: true, ...} Copy without property Decomposition by specifiying which key(s) to not copy, then Rest ... operator: const { population , satellite , ... copyHearth } = hearth ; console . log ( copyHearth ); // {temperature: {\u2026}, isOld: false, liveable2: null, liveable3: true}","title":"40-discard-property"},{"location":"javascript/training/07-object/40-discard-property/#40-discard-property","text":"","title":"40-discard-property"},{"location":"javascript/training/07-object/40-discard-property/#introduction","text":"Delete or discard properties. First, as a recap, we add three new properties in three different ways: const hearth = { population : 7e9 , satellite : \"Moon\" , temperature : { min : - 70 , max : 40 }, isOld : false }; // add properties, 3 ways hearth . liveable1 = true ; hearth [ \"liveable2\" ] = true ; const liveable3 = \"liveable3\" ; hearth [ liveable3 ] = true ; console . log ( hearth ); // {..., liveable1: true, liveable2: true, liveable3: true, ...}","title":"Introduction"},{"location":"javascript/training/07-object/40-discard-property/#remove-property","text":"Operator delete: delete hearth . liveable1 ; console . log ( hearth ); // {..., liveable2: true, liveable3: true, ...}","title":"Remove property"},{"location":"javascript/training/07-object/40-discard-property/#set-property-to-no-value","text":"Set to 'null', not 'undefined' as it was never assigned: hearth . liveable2 = null ; console . log ( hearth ); // {..., liveable2: null, liveable3: true, ...}","title":"Set property to no value"},{"location":"javascript/training/07-object/40-discard-property/#copy-without-property","text":"Decomposition by specifiying which key(s) to not copy, then Rest ... operator: const { population , satellite , ... copyHearth } = hearth ; console . log ( copyHearth ); // {temperature: {\u2026}, isOld: false, liveable2: null, liveable3: true}","title":"Copy without property"},{"location":"javascript/training/07-object/41-merge/","text":"41-merge Introduction The goal is to combine two objects: const hearth1 = { population : 7e9 , satellite : \"Moon\" , }; const hearth2 = { temperature : { min : - 70 , max : 40 }, isOld : false }; Assign The 'assign()' native method of 'Object': Start with an empty object (the one to be assigned, a new reference), then merged from left to rigth: console . log ( Object . assign ({}, hearth1 , hearth2 )); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false} Spread With '...' spread operator: const hearth = { ... hearth1 , ... hearth2 }; console . log ( hearth ); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false}","title":"41-merge"},{"location":"javascript/training/07-object/41-merge/#41-merge","text":"","title":"41-merge"},{"location":"javascript/training/07-object/41-merge/#introduction","text":"The goal is to combine two objects: const hearth1 = { population : 7e9 , satellite : \"Moon\" , }; const hearth2 = { temperature : { min : - 70 , max : 40 }, isOld : false };","title":"Introduction"},{"location":"javascript/training/07-object/41-merge/#assign","text":"The 'assign()' native method of 'Object': Start with an empty object (the one to be assigned, a new reference), then merged from left to rigth: console . log ( Object . assign ({}, hearth1 , hearth2 )); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false}","title":"Assign"},{"location":"javascript/training/07-object/41-merge/#spread","text":"With '...' spread operator: const hearth = { ... hearth1 , ... hearth2 }; console . log ( hearth ); // {population: 7000000000, satellite: 'Moon', temperature: {\u2026}, isOld: false}","title":"Spread"},{"location":"javascript/training/07-object/42-compare/","text":"42-compare Introduction Compare object: const a = {}; const b = {}; console . log ( a == b ); // false console . log ( a === b ); // false const c = b ; console . log ( c === b ); // true Even they've same value, 'a' and 'b', their references are different, so not equal. For 'b' and 'c', they reference same object (same stack address), so, equal. It doesn't matter that the two objects have the same properties because it is the references that are compared, never the contents of the objects.","title":"42-compare"},{"location":"javascript/training/07-object/42-compare/#42-compare","text":"","title":"42-compare"},{"location":"javascript/training/07-object/42-compare/#introduction","text":"Compare object: const a = {}; const b = {}; console . log ( a == b ); // false console . log ( a === b ); // false const c = b ; console . log ( c === b ); // true Even they've same value, 'a' and 'b', their references are different, so not equal. For 'b' and 'c', they reference same object (same stack address), so, equal. It doesn't matter that the two objects have the same properties because it is the references that are compared, never the contents of the objects.","title":"Introduction"},{"location":"javascript/training/07-object/43-iterate/","text":"43-iterate Introduction Iterate over objects. for in const obj = { a : 'one' , b : 'two' , c : 'three' } for ( prop in obj ) { console . log ( prop ); // keys console . log ( obj [ prop ]); // values } Ouput: a one b two c three Object.keys() console . log ( Object . keys ( obj )); // (3) ['a', 'b', 'c'] Output an array of object's keys. Object.values() console . log ( Object . values ( obj )); // (3) ['one', 'two', 'three'] Output an array of object's values. Object.entries() console . log ( Object . entries ( obj )); Output: (3) [Array(2), Array(2), Array(2)] 0: (2) ['a', 'one'] 1: (2) ['b', 'two'] 2: (2) ['c', 'three'] Output an array of arrays of object's keys, values.","title":"43-iterate"},{"location":"javascript/training/07-object/43-iterate/#43-iterate","text":"","title":"43-iterate"},{"location":"javascript/training/07-object/43-iterate/#introduction","text":"Iterate over objects.","title":"Introduction"},{"location":"javascript/training/07-object/43-iterate/#for-in","text":"const obj = { a : 'one' , b : 'two' , c : 'three' } for ( prop in obj ) { console . log ( prop ); // keys console . log ( obj [ prop ]); // values } Ouput: a one b two c three","title":"for in"},{"location":"javascript/training/07-object/43-iterate/#objectkeys","text":"console . log ( Object . keys ( obj )); // (3) ['a', 'b', 'c'] Output an array of object's keys.","title":"Object.keys()"},{"location":"javascript/training/07-object/43-iterate/#objectvalues","text":"console . log ( Object . values ( obj )); // (3) ['one', 'two', 'three'] Output an array of object's values.","title":"Object.values()"},{"location":"javascript/training/07-object/43-iterate/#objectentries","text":"console . log ( Object . entries ( obj )); Output: (3) [Array(2), Array(2), Array(2)] 0: (2) ['a', 'one'] 1: (2) ['b', 'two'] 2: (2) ['c', 'three'] Output an array of arrays of object's keys, values.","title":"Object.entries()"},{"location":"javascript/training/07-object/44-json/","text":"44-json Introduction JavaScript Object Notation, just for data (no method): { \"firstname\" : \"John\" , \"lastname\" : \"Doe\" , \"age\" : 37 } JSON Native object to convert a JavaScript object to JSON and vice versa. stringify const obj = { firstname : \"John\" , lastname : \"Doe\" , age : 37 } console . log ( JSON . stringify ( obj )); // {\"firstname\":\"John\",\"lastname\":\"Doe\",\"age\":37} parse console . log ( JSON . parse ( '{\"firstname\": \"John\", \"lastname\": \"Doe\", \"age\": 37}' )); // {firstname: 'John', lastname: 'Doe', age: 37}","title":"44-json"},{"location":"javascript/training/07-object/44-json/#44-json","text":"","title":"44-json"},{"location":"javascript/training/07-object/44-json/#introduction","text":"JavaScript Object Notation, just for data (no method): { \"firstname\" : \"John\" , \"lastname\" : \"Doe\" , \"age\" : 37 }","title":"Introduction"},{"location":"javascript/training/07-object/44-json/#json","text":"Native object to convert a JavaScript object to JSON and vice versa.","title":"JSON"},{"location":"javascript/training/07-object/44-json/#stringify","text":"const obj = { firstname : \"John\" , lastname : \"Doe\" , age : 37 } console . log ( JSON . stringify ( obj )); // {\"firstname\":\"John\",\"lastname\":\"Doe\",\"age\":37}","title":"stringify"},{"location":"javascript/training/07-object/44-json/#parse","text":"console . log ( JSON . parse ( '{\"firstname\": \"John\", \"lastname\": \"Doe\", \"age\": 37}' )); // {firstname: 'John', lastname: 'Doe', age: 37}","title":"parse"},{"location":"javascript/training/07-object/45-copy/","text":"45-copy Introduction Copy object. Shallow copy The result of Object.assign() is a shallow copied object due to the fact that if the object contains a nested object, he still refer to the same original object after assignment. For a more modern approach use the spread operator: const a = { name : 'John' , foo : { bar : 'zoo' } } console . log ( a ); // {name: \"John\", foo: { bar: 123 }} // !! be aware, here we may expect to have \"zoo\" as bar's value (due to hoisting?) var b = Object . assign ({}, a ); b . name = 'Jane' ; b . foo . bar = 123 const c = { ... a } // spread operator console . log ( a ); // {name: \"John\", foo: { bar: 123 }} console . log ( b ); // {name: \"Jane\", foo: { bar: 123 }} console . log ( c ); // {name: \"John\", foo: { bar: 123 }} Deep copy With JSON native object: const a = { name : 'John' , foo : { bar : 'zoo' } } const b = JSON . parse ( JSON . stringify ( a )); b . name = 'Jane' ; b . foo . bar = 123 ; console . log ( a ); // {name: \"John\", foo: { bar: \"zoo\" }} console . log ( b ); // {name: \"Jane\", foo: { bar: 123 }}","title":"45-copy"},{"location":"javascript/training/07-object/45-copy/#45-copy","text":"","title":"45-copy"},{"location":"javascript/training/07-object/45-copy/#introduction","text":"Copy object.","title":"Introduction"},{"location":"javascript/training/07-object/45-copy/#shallow-copy","text":"The result of Object.assign() is a shallow copied object due to the fact that if the object contains a nested object, he still refer to the same original object after assignment. For a more modern approach use the spread operator: const a = { name : 'John' , foo : { bar : 'zoo' } } console . log ( a ); // {name: \"John\", foo: { bar: 123 }} // !! be aware, here we may expect to have \"zoo\" as bar's value (due to hoisting?) var b = Object . assign ({}, a ); b . name = 'Jane' ; b . foo . bar = 123 const c = { ... a } // spread operator console . log ( a ); // {name: \"John\", foo: { bar: 123 }} console . log ( b ); // {name: \"Jane\", foo: { bar: 123 }} console . log ( c ); // {name: \"John\", foo: { bar: 123 }}","title":"Shallow copy"},{"location":"javascript/training/07-object/45-copy/#deep-copy","text":"With JSON native object: const a = { name : 'John' , foo : { bar : 'zoo' } } const b = JSON . parse ( JSON . stringify ( a )); b . name = 'Jane' ; b . foo . bar = 123 ; console . log ( a ); // {name: \"John\", foo: { bar: \"zoo\" }} console . log ( b ); // {name: \"Jane\", foo: { bar: 123 }}","title":"Deep copy"},{"location":"javascript/training/08-function/46-expression/","text":"46-expression Introduction ...","title":"46-expression"},{"location":"javascript/training/08-function/46-expression/#46-expression","text":"","title":"46-expression"},{"location":"javascript/training/08-function/46-expression/#introduction","text":"...","title":"Introduction"},{"location":"javascript/training/11-dom/81-css-with-webpack/","text":"81-css-with-webpack How to css Create a style.css file in src folder. syle.css: body { height : 300 px ; background-color : blue ; border : 1 px solid grey ; } js In index.js file, import css file. index.js: import \"./style.css\" ; Now css file is a dependency managed by Webpack. Webpack setup For Webpack to process the CSS file correctly, add two new loaders and configuration to the webpack.config.js file. Install necessary loaders: npm install css-loader style-loader -D webpack.config.js: const path = require ( \"path\" ); const HtmlWebpackPlugin = require ( \"html-webpack-plugin\" ); module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ) }, output : { path : path . join ( __dirname , \"dist\" ), filename : \"[name].bundle.js\" }, module : { rules : [ { test : /\\.js/ , exclude : /(node_modules)/ , use : [ \"babel-loader\" ] }, { test : /\\.css$/ , use : [ \"style-loader\" , \"css-loader\" ] } ] }, plugins : [ new HtmlWebpackPlugin ({ template : path . join ( __dirname , \"./src/index.html\" ) }) ], stats : \"minimal\" , devtool : \"source-map\" , mode : \"development\" , devServer : { open : false , static : path . resolve ( __dirname , './dist' ), port : 4000 } }; For CSS files, we first apply the css-loader and then the style-loader. Loaders are applied from right to left of the array passed to the use property.","title":"81-css-with-webpack"},{"location":"javascript/training/11-dom/81-css-with-webpack/#81-css-with-webpack","text":"","title":"81-css-with-webpack"},{"location":"javascript/training/11-dom/81-css-with-webpack/#how-to","text":"","title":"How to"},{"location":"javascript/training/11-dom/81-css-with-webpack/#css","text":"Create a style.css file in src folder. syle.css: body { height : 300 px ; background-color : blue ; border : 1 px solid grey ; }","title":"css"},{"location":"javascript/training/11-dom/81-css-with-webpack/#js","text":"In index.js file, import css file. index.js: import \"./style.css\" ; Now css file is a dependency managed by Webpack.","title":"js"},{"location":"javascript/training/11-dom/81-css-with-webpack/#webpack-setup","text":"For Webpack to process the CSS file correctly, add two new loaders and configuration to the webpack.config.js file. Install necessary loaders: npm install css-loader style-loader -D webpack.config.js: const path = require ( \"path\" ); const HtmlWebpackPlugin = require ( \"html-webpack-plugin\" ); module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ) }, output : { path : path . join ( __dirname , \"dist\" ), filename : \"[name].bundle.js\" }, module : { rules : [ { test : /\\.js/ , exclude : /(node_modules)/ , use : [ \"babel-loader\" ] }, { test : /\\.css$/ , use : [ \"style-loader\" , \"css-loader\" ] } ] }, plugins : [ new HtmlWebpackPlugin ({ template : path . join ( __dirname , \"./src/index.html\" ) }) ], stats : \"minimal\" , devtool : \"source-map\" , mode : \"development\" , devServer : { open : false , static : path . resolve ( __dirname , './dist' ), port : 4000 } }; For CSS files, we first apply the css-loader and then the style-loader. Loaders are applied from right to left of the array passed to the use property.","title":"Webpack setup"},{"location":"javascript/training/13-project-todo/92-initialization/","text":"92-initialization Project presentation and setup. Source code on GitHub Goals Our goal is to create a to-do list. We want to be able to create, edit, validate and delete todos, entirely in JavaScript. To achieve this we gonna use Webpack and initialize project's root folder accordingly. Create project (Create a new project's folder) Refer to environment chapter to get more details Git Git setup Npm Initialize package.json file at the project's root folder level by typing: npm init Dependencies Install dependencies: npm i -D @babel/cli @babel/core @babel/preset-env babel-loader css-loader html-webpack-plugin style-loader webpack webpack-cli webpack-dev-server Webpack Create a Webpack configuration file, webpack.config.js: const path = require ( \"path\" ); const HtmlWebpackPlugin = require ( \"html-webpack-plugin\" ); module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ), }, output : { path : path . join ( __dirname , \"dist\" ), filename : \"[name].bundle.js\" , }, module : { rules : [ { test : /\\.js/ , exclude : /(node_modules)/ , use : [ \"babel-loader\" ], }, { test : /\\.css$/i , use : [ \"style-loader\" , \"css-loader\" ], }, ], }, plugins : [ new HtmlWebpackPlugin ({ template : path . join ( __dirname , \"./src/index.html\" ), }), ], stats : \"minimal\" , devtool : \"source-map\" , mode : \"development\" , devServer : { open : false , static : path . resolve ( __dirname , \"./dist\" ), port : 4000 , }, }; Babel Then for Babel, a configuration file as well, babel.config.js: module . exports = { presets : [[ \"@babel/preset-env\" ]], }; Gitignore Edit .gitignore file by adding following entries (.history folder is for VS Code): .history node_modules dist Source folder Create a source folder and add the minimum files needed for the application: mkdir src cd src touch index.html touch style.css touch index.js Git push initial setup git status git add . git commit -m \"initial setup\" git push VS Code Launch VS Code from terminal in project's root folder: code . Ready! Setup index.html Edit src/index.html with following content: <!DOCTYPE html> < html lang = \"fr\" > < head > < meta charset = \"UTF-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"ie=edge\" /> < title > To do </ title > </ head > < body > < div class = \"container\" > < form > < input type = \"text\" /> < button > Add </ button > </ form > < ul > < li > < span class = \"todo done\" ></ span > < p > text </ p > < button > Edit </ button > < button > Delete </ button > </ li > </ ul > </ div > </ body > </ html > index.js Import styles in index.js to be considered as a dependency by Webpack and then, bundled. src/index.js: import \"./style.css\" ; style.css Edit src/style.css with following content: * body { box-sizing : border-box ; } body { margin : 0 ; min-height : 100 vh ; display : flex ; align-items : center ; justify-content : center ; } p { margin : 0 ; } . container { width : 500 px ; border : 1 px solid #eee ; border-radius : 3 px ; padding : 20 px ; display : flex ; flex-direction : column ; } form { display : flex ; margin-bottom : 20 px ; } button { padding : 5 px 15 px ; border : 0 px ; border-radius : 5 px ; cursor : pointer ; margin : 0 3 px ; } input { padding : 8 px 15 px ; outline : 0 ; border : 1 px solid #ddd ; border-radius : 3 px ; } form input { flex : 1 ; margin-right : 15 px ; } ul { padding : 0 ; list-style : none ; } li { display : flex ; align-items : center ; } li p { flex : 1 ; } li . todo { flex : 0 0 20 px ; height : 20 px ; border-radius : 30 px ; margin-right : 15 px ; border : 2 px solid #333 ; } li . todo . done { background : #333 ; } package.json Edit script part of the package.json file for Webpack: package.json: .. \"scripts\" : { \"test\" : \"echo 'Error: no test specified' && exit 1\" , \"webpack\" : \"webpack\" , \"start\" : \"webpack serve\" }, .. Git push minimal files git status git add . git commit -m \"minimal files\" git push Start Start local Webpack development server: npm start Browse to http://localhost:4000/","title":"92-initialization"},{"location":"javascript/training/13-project-todo/92-initialization/#92-initialization","text":"Project presentation and setup. Source code on GitHub","title":"92-initialization"},{"location":"javascript/training/13-project-todo/92-initialization/#goals","text":"Our goal is to create a to-do list. We want to be able to create, edit, validate and delete todos, entirely in JavaScript. To achieve this we gonna use Webpack and initialize project's root folder accordingly.","title":"Goals"},{"location":"javascript/training/13-project-todo/92-initialization/#create-project","text":"(Create a new project's folder) Refer to environment chapter to get more details","title":"Create project"},{"location":"javascript/training/13-project-todo/92-initialization/#git","text":"Git setup","title":"Git"},{"location":"javascript/training/13-project-todo/92-initialization/#npm","text":"Initialize package.json file at the project's root folder level by typing: npm init","title":"Npm"},{"location":"javascript/training/13-project-todo/92-initialization/#dependencies","text":"Install dependencies: npm i -D @babel/cli @babel/core @babel/preset-env babel-loader css-loader html-webpack-plugin style-loader webpack webpack-cli webpack-dev-server","title":"Dependencies"},{"location":"javascript/training/13-project-todo/92-initialization/#webpack","text":"Create a Webpack configuration file, webpack.config.js: const path = require ( \"path\" ); const HtmlWebpackPlugin = require ( \"html-webpack-plugin\" ); module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ), }, output : { path : path . join ( __dirname , \"dist\" ), filename : \"[name].bundle.js\" , }, module : { rules : [ { test : /\\.js/ , exclude : /(node_modules)/ , use : [ \"babel-loader\" ], }, { test : /\\.css$/i , use : [ \"style-loader\" , \"css-loader\" ], }, ], }, plugins : [ new HtmlWebpackPlugin ({ template : path . join ( __dirname , \"./src/index.html\" ), }), ], stats : \"minimal\" , devtool : \"source-map\" , mode : \"development\" , devServer : { open : false , static : path . resolve ( __dirname , \"./dist\" ), port : 4000 , }, };","title":"Webpack"},{"location":"javascript/training/13-project-todo/92-initialization/#babel","text":"Then for Babel, a configuration file as well, babel.config.js: module . exports = { presets : [[ \"@babel/preset-env\" ]], };","title":"Babel"},{"location":"javascript/training/13-project-todo/92-initialization/#gitignore","text":"Edit .gitignore file by adding following entries (.history folder is for VS Code): .history node_modules dist","title":"Gitignore"},{"location":"javascript/training/13-project-todo/92-initialization/#source-folder","text":"Create a source folder and add the minimum files needed for the application: mkdir src cd src touch index.html touch style.css touch index.js","title":"Source folder"},{"location":"javascript/training/13-project-todo/92-initialization/#git-push-initial-setup","text":"git status git add . git commit -m \"initial setup\" git push","title":"Git push initial setup"},{"location":"javascript/training/13-project-todo/92-initialization/#vs-code","text":"Launch VS Code from terminal in project's root folder: code . Ready!","title":"VS Code"},{"location":"javascript/training/13-project-todo/92-initialization/#setup","text":"","title":"Setup"},{"location":"javascript/training/13-project-todo/92-initialization/#indexhtml","text":"Edit src/index.html with following content: <!DOCTYPE html> < html lang = \"fr\" > < head > < meta charset = \"UTF-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"ie=edge\" /> < title > To do </ title > </ head > < body > < div class = \"container\" > < form > < input type = \"text\" /> < button > Add </ button > </ form > < ul > < li > < span class = \"todo done\" ></ span > < p > text </ p > < button > Edit </ button > < button > Delete </ button > </ li > </ ul > </ div > </ body > </ html >","title":"index.html"},{"location":"javascript/training/13-project-todo/92-initialization/#indexjs","text":"Import styles in index.js to be considered as a dependency by Webpack and then, bundled. src/index.js: import \"./style.css\" ;","title":"index.js"},{"location":"javascript/training/13-project-todo/92-initialization/#stylecss","text":"Edit src/style.css with following content: * body { box-sizing : border-box ; } body { margin : 0 ; min-height : 100 vh ; display : flex ; align-items : center ; justify-content : center ; } p { margin : 0 ; } . container { width : 500 px ; border : 1 px solid #eee ; border-radius : 3 px ; padding : 20 px ; display : flex ; flex-direction : column ; } form { display : flex ; margin-bottom : 20 px ; } button { padding : 5 px 15 px ; border : 0 px ; border-radius : 5 px ; cursor : pointer ; margin : 0 3 px ; } input { padding : 8 px 15 px ; outline : 0 ; border : 1 px solid #ddd ; border-radius : 3 px ; } form input { flex : 1 ; margin-right : 15 px ; } ul { padding : 0 ; list-style : none ; } li { display : flex ; align-items : center ; } li p { flex : 1 ; } li . todo { flex : 0 0 20 px ; height : 20 px ; border-radius : 30 px ; margin-right : 15 px ; border : 2 px solid #333 ; } li . todo . done { background : #333 ; }","title":"style.css"},{"location":"javascript/training/13-project-todo/92-initialization/#packagejson","text":"Edit script part of the package.json file for Webpack: package.json: .. \"scripts\" : { \"test\" : \"echo 'Error: no test specified' && exit 1\" , \"webpack\" : \"webpack\" , \"start\" : \"webpack serve\" }, ..","title":"package.json"},{"location":"javascript/training/13-project-todo/92-initialization/#git-push-minimal-files","text":"git status git add . git commit -m \"minimal files\" git push","title":"Git push minimal files"},{"location":"javascript/training/13-project-todo/92-initialization/#start","text":"Start local Webpack development server: npm start Browse to http://localhost:4000/","title":"Start"},{"location":"javascript/training/14-asynchronous-browser/102-polyfill-webpack/","text":"102-polyfill-webpack A polyfill is a piece of code (usually JavaScript on the Web) used to provide modern functionality on older browsers that do not natively support it. Webpack setup Webpack alone does not take care of adding polyfills to your application, you must use Babel and Browserlist which is a dependency of Babel. Webpack setup to use polyfills, in a terminal, at the root folder level of the project, type in the command below: npm i -D core-js@3 regenerator-runtime Babel setup Edit file babel.config.js as follow: const presets = [ [ \"@babel/preset-env\" , { useBuiltIns : \"usage\" , debug : true , corejs : 3 , targets : \"> 0.25%, not dead\" } ] ]; module . exports = { presets }; useBuiltIns 'useBuiltIns' option, allow Babel's polyfills management. Default value is 'false' and Babel therefor will not add polyfills automatically. 'usage' value allow to import only needed polyfills for targeted browsers. 'entry' value will import all polyfills for targeted browsers (even those not used by your application). debug 'debug' option, will log browser release targets and all your code needs in term of polyfills. corejs 'corejs' option, specify release of core-js library to use. targets 'target' option, allow to specifiy targeted browser. This option is extremely powerful. Support the browsers used in France by at least 0.2% of people: \"> 0.2% in FR\" Support all versions of browsers representing at least 1% of users or which are not dead: \"> 1%, not dead\" Latest three versions of browsers: \"last 3 versions\" Recommended: \"defaults\" \"default\" is equivalent to \"> 0.5%, last 2 versions, Firefox ESR, not dead\" Conclusion The more polyfills there are, the heavier your application will be, but the more it will be supported by all browsers. There is no recommendation, it all depends on your target! If you don't have a particular target use 'defaults' which is the official Browserslist recommendation.","title":"102-polyfill-webpack"},{"location":"javascript/training/14-asynchronous-browser/102-polyfill-webpack/#102-polyfill-webpack","text":"A polyfill is a piece of code (usually JavaScript on the Web) used to provide modern functionality on older browsers that do not natively support it.","title":"102-polyfill-webpack"},{"location":"javascript/training/14-asynchronous-browser/102-polyfill-webpack/#webpack-setup","text":"Webpack alone does not take care of adding polyfills to your application, you must use Babel and Browserlist which is a dependency of Babel. Webpack setup to use polyfills, in a terminal, at the root folder level of the project, type in the command below: npm i -D core-js@3 regenerator-runtime","title":"Webpack setup"},{"location":"javascript/training/14-asynchronous-browser/102-polyfill-webpack/#babel-setup","text":"Edit file babel.config.js as follow: const presets = [ [ \"@babel/preset-env\" , { useBuiltIns : \"usage\" , debug : true , corejs : 3 , targets : \"> 0.25%, not dead\" } ] ]; module . exports = { presets };","title":"Babel setup"},{"location":"javascript/training/14-asynchronous-browser/102-polyfill-webpack/#usebuiltins","text":"'useBuiltIns' option, allow Babel's polyfills management. Default value is 'false' and Babel therefor will not add polyfills automatically. 'usage' value allow to import only needed polyfills for targeted browsers. 'entry' value will import all polyfills for targeted browsers (even those not used by your application).","title":"useBuiltIns"},{"location":"javascript/training/14-asynchronous-browser/102-polyfill-webpack/#debug","text":"'debug' option, will log browser release targets and all your code needs in term of polyfills.","title":"debug"},{"location":"javascript/training/14-asynchronous-browser/102-polyfill-webpack/#corejs","text":"'corejs' option, specify release of core-js library to use.","title":"corejs"},{"location":"javascript/training/14-asynchronous-browser/102-polyfill-webpack/#targets","text":"'target' option, allow to specifiy targeted browser. This option is extremely powerful. Support the browsers used in France by at least 0.2% of people: \"> 0.2% in FR\" Support all versions of browsers representing at least 1% of users or which are not dead: \"> 1%, not dead\" Latest three versions of browsers: \"last 3 versions\" Recommended: \"defaults\" \"default\" is equivalent to \"> 0.5%, last 2 versions, Firefox ESR, not dead\"","title":"targets"},{"location":"javascript/training/14-asynchronous-browser/102-polyfill-webpack/#conclusion","text":"The more polyfills there are, the heavier your application will be, but the more it will be supported by all browsers. There is no recommendation, it all depends on your target! If you don't have a particular target use 'defaults' which is the official Browserslist recommendation.","title":"Conclusion"},{"location":"javascript/training/16-project-blog/115-introduction/","text":"115-introduction Project presentation and setup. Dyma.fr Source code on GitHub OLDU Source code on GitHub Goals Our goal is to create a real app to blog. Project setup use Polyfill and Sass in addtion of what was used in previous project (todo). Goals: Create many pages (many Webpack bundles) Sass setup Use of images Creation of articles Deletion of articles Responsive Topbar with menu for mobile We gonna use Webpack and initialize project's root folder accordingly. Create project (Create a new project's folder) Refer to environment chapter to get more details Git Git setup Npm Initialize package.json file at the project's root folder level by typing: npm init Dependencies Install dependencies: npm i -D @babel/cli @babel/core @babel/preset-env babel-loader css-loader html-webpack-plugin style-loader webpack webpack-cli webpack-dev-server core-js@3 regenerator-runtime Webpack Create a Webpack configuration file, webpack.config.js: const path = require ( \"path\" ); const HtmlWebpackPlugin = require ( \"html-webpack-plugin\" ); module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ) }, output : { path : path . join ( __dirname , \"dist\" ), filename : \"[name].bundle.js\" }, module : { rules : [ { test : /\\.js/ , exclude : /(node_modules)/ , use : [ \"babel-loader\" ] }, { test : /\\.css$/i , use : [ \"style-loader\" , \"css-loader\" ] } ] }, plugins : [ new HtmlWebpackPlugin ({ template : path . join ( __dirname , \"./src/index.html\" ) }) ], stats : \"minimal\" , devtool : \"source-map\" , mode : \"development\" , devServer : { open : false , static : path . resolve ( __dirname , './dist' ), port : 4000 } }; Babel Then for Babel, a configuration file as well, babel.config.js: const presets = [ [ \"@babel/preset-env\" , { useBuiltIns : \"usage\" , debug : true , corejs : 3 , targets : \"> 0.25%, not dead\" } ] ]; module . exports = { presets }; Gitignore Edit .gitignore file by adding following entries (.history folder is for VS Code): .history node_modules dist Source folder Create a source folder and add the minimum files needed for the application: mkdir src cd src touch index.html touch style.css touch index.js Git push initial setup git status git add . git commit -m \"initial setup\" git push VS Code Launch VS Code from terminal in project's root folder: code . Ready! Sass with Webpack Edit webpack.config.js: ... module : { rules : [ { test : /\\.js/ , exclude : /(node_modules)/ , use : [ \"babel-loader\" ] }, { test : /\\.scss$/i , use : [ \"style-loader\" , \"css-loader\" , \"sass-loader\" ] } ] }, ... Then install loader and compiler that allow to transform Sass to CSS: npm install sass-loader sass --save-dev Setup index.html Edit src/index.html with following content: <!DOCTYPE html> < html lang = \"fr\" > < head > < meta charset = \"UTF-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"ie=edge\" /> < title > Blog </ title > </ head > < body > < div class = \"container\" > < h1 > Accueil </ h1 > </ div > </ body > </ html > index.js Import styles in index.js to be considered as a dependency by Webpack and then, bundled. src/index.js: import \"./style.scss\" ; style.scss Rename style.css file created above with Sass file extension: mv src/style.css src/style.scss Edit src/style.scss with following content: .container { h1 { color :red ; } } package.json Edit script part of the package.json file for Webpack: package.json: .. \"scripts\" : { \"test\" : \"echo 'Error: no test specified' && exit 1\" , \"webpack\" : \"webpack\" , \"start\" : \"webpack serve\" }, .. Git push minimal files git status git add . git commit -m \"minimal files\" git push Start Start local Webpack development server: npm start Browse to http://localhost:4000/","title":"115-introduction"},{"location":"javascript/training/16-project-blog/115-introduction/#115-introduction","text":"Project presentation and setup. Dyma.fr Source code on GitHub OLDU Source code on GitHub","title":"115-introduction"},{"location":"javascript/training/16-project-blog/115-introduction/#goals","text":"Our goal is to create a real app to blog. Project setup use Polyfill and Sass in addtion of what was used in previous project (todo). Goals: Create many pages (many Webpack bundles) Sass setup Use of images Creation of articles Deletion of articles Responsive Topbar with menu for mobile We gonna use Webpack and initialize project's root folder accordingly.","title":"Goals"},{"location":"javascript/training/16-project-blog/115-introduction/#create-project","text":"(Create a new project's folder) Refer to environment chapter to get more details","title":"Create project"},{"location":"javascript/training/16-project-blog/115-introduction/#git","text":"Git setup","title":"Git"},{"location":"javascript/training/16-project-blog/115-introduction/#npm","text":"Initialize package.json file at the project's root folder level by typing: npm init","title":"Npm"},{"location":"javascript/training/16-project-blog/115-introduction/#dependencies","text":"Install dependencies: npm i -D @babel/cli @babel/core @babel/preset-env babel-loader css-loader html-webpack-plugin style-loader webpack webpack-cli webpack-dev-server core-js@3 regenerator-runtime","title":"Dependencies"},{"location":"javascript/training/16-project-blog/115-introduction/#webpack","text":"Create a Webpack configuration file, webpack.config.js: const path = require ( \"path\" ); const HtmlWebpackPlugin = require ( \"html-webpack-plugin\" ); module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ) }, output : { path : path . join ( __dirname , \"dist\" ), filename : \"[name].bundle.js\" }, module : { rules : [ { test : /\\.js/ , exclude : /(node_modules)/ , use : [ \"babel-loader\" ] }, { test : /\\.css$/i , use : [ \"style-loader\" , \"css-loader\" ] } ] }, plugins : [ new HtmlWebpackPlugin ({ template : path . join ( __dirname , \"./src/index.html\" ) }) ], stats : \"minimal\" , devtool : \"source-map\" , mode : \"development\" , devServer : { open : false , static : path . resolve ( __dirname , './dist' ), port : 4000 } };","title":"Webpack"},{"location":"javascript/training/16-project-blog/115-introduction/#babel","text":"Then for Babel, a configuration file as well, babel.config.js: const presets = [ [ \"@babel/preset-env\" , { useBuiltIns : \"usage\" , debug : true , corejs : 3 , targets : \"> 0.25%, not dead\" } ] ]; module . exports = { presets };","title":"Babel"},{"location":"javascript/training/16-project-blog/115-introduction/#gitignore","text":"Edit .gitignore file by adding following entries (.history folder is for VS Code): .history node_modules dist","title":"Gitignore"},{"location":"javascript/training/16-project-blog/115-introduction/#source-folder","text":"Create a source folder and add the minimum files needed for the application: mkdir src cd src touch index.html touch style.css touch index.js","title":"Source folder"},{"location":"javascript/training/16-project-blog/115-introduction/#git-push-initial-setup","text":"git status git add . git commit -m \"initial setup\" git push","title":"Git push initial setup"},{"location":"javascript/training/16-project-blog/115-introduction/#vs-code","text":"Launch VS Code from terminal in project's root folder: code . Ready!","title":"VS Code"},{"location":"javascript/training/16-project-blog/115-introduction/#sass-with-webpack","text":"Edit webpack.config.js: ... module : { rules : [ { test : /\\.js/ , exclude : /(node_modules)/ , use : [ \"babel-loader\" ] }, { test : /\\.scss$/i , use : [ \"style-loader\" , \"css-loader\" , \"sass-loader\" ] } ] }, ... Then install loader and compiler that allow to transform Sass to CSS: npm install sass-loader sass --save-dev","title":"Sass with Webpack"},{"location":"javascript/training/16-project-blog/115-introduction/#setup","text":"","title":"Setup"},{"location":"javascript/training/16-project-blog/115-introduction/#indexhtml","text":"Edit src/index.html with following content: <!DOCTYPE html> < html lang = \"fr\" > < head > < meta charset = \"UTF-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"ie=edge\" /> < title > Blog </ title > </ head > < body > < div class = \"container\" > < h1 > Accueil </ h1 > </ div > </ body > </ html >","title":"index.html"},{"location":"javascript/training/16-project-blog/115-introduction/#indexjs","text":"Import styles in index.js to be considered as a dependency by Webpack and then, bundled. src/index.js: import \"./style.scss\" ;","title":"index.js"},{"location":"javascript/training/16-project-blog/115-introduction/#stylescss","text":"Rename style.css file created above with Sass file extension: mv src/style.css src/style.scss Edit src/style.scss with following content: .container { h1 { color :red ; } }","title":"style.scss"},{"location":"javascript/training/16-project-blog/115-introduction/#packagejson","text":"Edit script part of the package.json file for Webpack: package.json: .. \"scripts\" : { \"test\" : \"echo 'Error: no test specified' && exit 1\" , \"webpack\" : \"webpack\" , \"start\" : \"webpack serve\" }, ..","title":"package.json"},{"location":"javascript/training/16-project-blog/115-introduction/#git-push-minimal-files","text":"git status git add . git commit -m \"minimal files\" git push","title":"Git push minimal files"},{"location":"javascript/training/16-project-blog/115-introduction/#start","text":"Start local Webpack development server: npm start Browse to http://localhost:4000/","title":"Start"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/","text":"116-multipages-webpack Setup Webpack project for multi page application. Goals It's not a \"Single Page Application\" as we may do like with a framework (Angular, Vue.js or React). The objective is not to recode a framework but to master Webpack and a project in vanilla JavaScript (without framework). After initial setup from previous chapter we want to create a multi page application. To do this, you must configure Webpack to have exactly one bundle per page. Second page Create a second page. First create a new folder 'form' in 'src' folder: ../blog-001/src$ mkdir form In this 'form' folder create the three base needed files: ../blog-001/src$ touch form.html ../blog-001/src$ touch form.scss ../blog-001/src$ touch form.js Edit html form.html <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > < title > Form </ title > </ head > < body > < a href = \"./index.html\" > Accueil </ a > </ body > </ html > index.html <!DOCTYPE html> < html lang = \"fr\" > < head > < meta charset = \"UTF-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"ie=edge\" /> < title > Blog </ title > </ head > < body > < div class = \"container\" > < a href = \"./form.html\" > Form </ a > </ div > </ body > </ html > Edit scss form.scss a { color : red ; } index.scss Rename 'style.scss' to 'index.scss' mv style.scss index.scss index.scss: a { color : orange ; } Edit js form.js import \"./form.scss\" ; console . log ( \"form\" ); index.js import \"./index.scss\" ; console . log ( \"index\" ); Webpack Edit Webpack configuration file to add new entry point and then a new bundle, webpack.config.js: ... module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ), form : path . join ( __dirname , \"src/form/form.js\" ), }, ... plugins : [ new HtmlWebpackPlugin ({ filename : \"index.html\" , template : path . join ( __dirname , \"./src/index.html\" ), chunks : [ \"main\" ] }), new HtmlWebpackPlugin ({ filename : \"form.html\" , template : path . join ( __dirname , \"./src/form/form.html\" ), chunks : [ \"form\" ] }), ], ... Properties filename to specify html output template to specify html input chunks specify the bundle to insert Generate To observe generated structure by Webpack: npm run webpack Then browse the 'dist' folder. Test Test it: npm start Then browse to http://localhost:4000/","title":"116-multipages-webpack"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#116-multipages-webpack","text":"Setup Webpack project for multi page application.","title":"116-multipages-webpack"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#goals","text":"It's not a \"Single Page Application\" as we may do like with a framework (Angular, Vue.js or React). The objective is not to recode a framework but to master Webpack and a project in vanilla JavaScript (without framework). After initial setup from previous chapter we want to create a multi page application. To do this, you must configure Webpack to have exactly one bundle per page.","title":"Goals"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#second-page","text":"Create a second page. First create a new folder 'form' in 'src' folder: ../blog-001/src$ mkdir form In this 'form' folder create the three base needed files: ../blog-001/src$ touch form.html ../blog-001/src$ touch form.scss ../blog-001/src$ touch form.js","title":"Second page"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#edit-html","text":"","title":"Edit html"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#formhtml","text":"<!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < meta http-equiv = \"X-UA-Compatible\" content = \"IE=edge\" > < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" > < title > Form </ title > </ head > < body > < a href = \"./index.html\" > Accueil </ a > </ body > </ html >","title":"form.html"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#indexhtml","text":"<!DOCTYPE html> < html lang = \"fr\" > < head > < meta charset = \"UTF-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"ie=edge\" /> < title > Blog </ title > </ head > < body > < div class = \"container\" > < a href = \"./form.html\" > Form </ a > </ div > </ body > </ html >","title":"index.html"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#edit-scss","text":"","title":"Edit scss"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#formscss","text":"a { color : red ; }","title":"form.scss"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#indexscss","text":"Rename 'style.scss' to 'index.scss' mv style.scss index.scss index.scss: a { color : orange ; }","title":"index.scss"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#edit-js","text":"","title":"Edit js"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#formjs","text":"import \"./form.scss\" ; console . log ( \"form\" );","title":"form.js"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#indexjs","text":"import \"./index.scss\" ; console . log ( \"index\" );","title":"index.js"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#webpack","text":"Edit Webpack configuration file to add new entry point and then a new bundle, webpack.config.js: ... module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ), form : path . join ( __dirname , \"src/form/form.js\" ), }, ... plugins : [ new HtmlWebpackPlugin ({ filename : \"index.html\" , template : path . join ( __dirname , \"./src/index.html\" ), chunks : [ \"main\" ] }), new HtmlWebpackPlugin ({ filename : \"form.html\" , template : path . join ( __dirname , \"./src/form/form.html\" ), chunks : [ \"form\" ] }), ], ...","title":"Webpack"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#properties","text":"filename to specify html output template to specify html input chunks specify the bundle to insert","title":"Properties"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#generate","text":"To observe generated structure by Webpack: npm run webpack Then browse the 'dist' folder.","title":"Generate"},{"location":"javascript/training/16-project-blog/116-multipages-webpack/#test","text":"Test it: npm start Then browse to http://localhost:4000/","title":"Test"},{"location":"javascript/training/16-project-blog/117-assets-webpack/","text":"117-assets-webpack Assets are common files used throughout the application, such as images, javascripts, styles, etc.. Setup Webpack setup for assets. We will have to use two specific plugins to copy assets and clean 'dist' folder. copy-webpack-plugin This plugin will copy the contents of the 'assets' directory from 'src' to the 'dist' directory. clean-webpack-plugin This plugin will empty the 'dist' directory before each build (npm run webpack) Install: npm i -D copy-webpack-plugin clean-webpack-plugin webpack.config.js: ... const CopyWebpackPlugin = require ( \"copy-webpack-plugin\" ); const { CleanWebpackPlugin } = require ( \"clean-webpack-plugin\" ); ... ... plugins : [ new CleanWebpackPlugin (), new CopyWebpackPlugin ({ patterns : [ { from : \"./src/assets/images/*\" , to : path . resolve ( __dirname , 'dist' , 'assets/images' , '[name][ext]' ), }, ], }), ... For testing, add an image in 'src/assets/images' folder and edit html files accordingly. index.html: ... < body > < div class = \"container\" > < h1 > Blog </ h1 > < a href = \"./form.html\" > Form </ a >< br />< br /> < img src = \"./assets/images/proto.jpg\" alt = \"protocoles\" width = \"300\" /> </ div > </ body > ... form.html: ... < body > < h1 > Form </ h1 > < a href = \"./index.html\" > Accueil </ a >< br />< br /> < img src = \"./assets/images/proto.jpg\" alt = \"protocoles\" width = \"600\" /> </ body > ... Build application: npm run webpack Observe 'dist' folder that now contain an 'assets' folder with copied image. Code sharing Sharing javascript code among bundles . Create a 'javascripts' folder inside 'assets' folder with a 'topbar.js' in it. topbar.js: console . log ( \"topbar\" ); We want to share this code with the two pages, 'index.html' and 'form.html'. To do so, we should add a new entry point in the webpack configuration and share the 'chunk' among our two html files. webpack.config.js: ... module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ), form : path . join ( __dirname , \"src/form/form.js\" ), topbar : path . join ( __dirname , \"src/assets/javascripts/topbar.js\" ), }, ... new HtmlWebpackPlugin ({ filename : \"index.html\" , template : path . join ( __dirname , \"./src/index.html\" ), chunks : [ \"main\" , \"topbar\" ], }), new HtmlWebpackPlugin ({ filename : \"form.html\" , template : path . join ( __dirname , \"./src/form/form.html\" ), chunks : [ \"form\" , \"topbar\" ], }), ... Build application: npm run webpack Observe in 'dist' folder that now html files include 'topbar.js'.","title":"117-assets-webpack"},{"location":"javascript/training/16-project-blog/117-assets-webpack/#117-assets-webpack","text":"Assets are common files used throughout the application, such as images, javascripts, styles, etc..","title":"117-assets-webpack"},{"location":"javascript/training/16-project-blog/117-assets-webpack/#setup","text":"Webpack setup for assets. We will have to use two specific plugins to copy assets and clean 'dist' folder. copy-webpack-plugin This plugin will copy the contents of the 'assets' directory from 'src' to the 'dist' directory. clean-webpack-plugin This plugin will empty the 'dist' directory before each build (npm run webpack) Install: npm i -D copy-webpack-plugin clean-webpack-plugin webpack.config.js: ... const CopyWebpackPlugin = require ( \"copy-webpack-plugin\" ); const { CleanWebpackPlugin } = require ( \"clean-webpack-plugin\" ); ... ... plugins : [ new CleanWebpackPlugin (), new CopyWebpackPlugin ({ patterns : [ { from : \"./src/assets/images/*\" , to : path . resolve ( __dirname , 'dist' , 'assets/images' , '[name][ext]' ), }, ], }), ... For testing, add an image in 'src/assets/images' folder and edit html files accordingly. index.html: ... < body > < div class = \"container\" > < h1 > Blog </ h1 > < a href = \"./form.html\" > Form </ a >< br />< br /> < img src = \"./assets/images/proto.jpg\" alt = \"protocoles\" width = \"300\" /> </ div > </ body > ... form.html: ... < body > < h1 > Form </ h1 > < a href = \"./index.html\" > Accueil </ a >< br />< br /> < img src = \"./assets/images/proto.jpg\" alt = \"protocoles\" width = \"600\" /> </ body > ... Build application: npm run webpack Observe 'dist' folder that now contain an 'assets' folder with copied image.","title":"Setup"},{"location":"javascript/training/16-project-blog/117-assets-webpack/#code-sharing","text":"Sharing javascript code among bundles . Create a 'javascripts' folder inside 'assets' folder with a 'topbar.js' in it. topbar.js: console . log ( \"topbar\" ); We want to share this code with the two pages, 'index.html' and 'form.html'. To do so, we should add a new entry point in the webpack configuration and share the 'chunk' among our two html files. webpack.config.js: ... module . exports = { entry : { main : path . join ( __dirname , \"src/index.js\" ), form : path . join ( __dirname , \"src/form/form.js\" ), topbar : path . join ( __dirname , \"src/assets/javascripts/topbar.js\" ), }, ... new HtmlWebpackPlugin ({ filename : \"index.html\" , template : path . join ( __dirname , \"./src/index.html\" ), chunks : [ \"main\" , \"topbar\" ], }), new HtmlWebpackPlugin ({ filename : \"form.html\" , template : path . join ( __dirname , \"./src/form/form.html\" ), chunks : [ \"form\" , \"topbar\" ], }), ... Build application: npm run webpack Observe in 'dist' folder that now html files include 'topbar.js'.","title":"Code sharing"},{"location":"javascript/training/16-project-blog/118-styles-setup/","text":"118-styles-setup Setting up styles for our application. Setup Create a styles folder inside assets folder: ../blog-001$ mkdir src/assets/styles We start by creating the scss files we will need in folder src/assets/styles : Sass @import and Partials ../blog-001$ cd src/assets/styles touch styles.scss touch _reset.scss touch _media-queries.scss touch _classes.scss touch _base.scss touch _variables.scss touch _utils.scss Edit styles.scss to import partials, no need to put underscore as file name prefix due to the fact those are partials: @import \"base\" ; @import \"classes\" ; @import \"media-queries\" ; @import \"reset\" ; @import \"utils\" ; @import \"variables\" ; reset _reset.scss: * { margin : 0 ; padding : 0 ; box-sizing : border-box ; } variables To choose colors, FLAT UI COLOR For --font-family c.f. fonts section For --box-shadow it comes from inspected element, \"card\" class on firebase web site. _variables.scss: :root { --primary : #2ecc71 ; --dark : #27ae60 ; --accent : #2c3e50 ; --text : #333 ; --divider : #ecf0f1 ; --font-family : \"Muli\" , sans-serif ; --box-shadow : 0 1px 2px 0 rgba ( 60 , 64 , 67 , .3 ), 0 1px 3px 1px rgba ( 60 , 64 , 67 , .15 ) } fonts We use Muli from google fonts light 300, regular 400, bold 700 index.html: < head > ... < link href = \"https://fonts.googleapis.com/css?family=Muli:300,400,700&display=swap\" rel = \"stylesheet\" /> ... </ head > base _base.scss: :root { font-size : 62 .5 % ; } body { font-size : 1 .6rem ; color : var ( --text ); font-family : var ( --font-family ); } h1 , h2 , h3 , h4 { margin-bottom : 2rem ; } h1 { font-size : 3 .5rem ; } h2 { font-size : 3rem ; } h3 { font-size : 2 .5rem ; } h4 { font-size : 2rem ; } ul { list-style : none ; } img { max-width : 100 % ; } a { color : var ( --text ); text-decoration : none ; } media-queries From CV project of HTML & CSS course _media-queries.scss: /* Landscape phones and down */ @mixin xs { @media ( max-width : 480 px ) { @content ; } } /* Landscape phone to portrait tablet */ @mixin sm { @media ( max-width : 767 px ) { @content ; } } /* Portrait tablet to landscape and desktop */ @mixin md { @media ( min-width : 768 px ) and ( max-width : 979 px ) { @content ; } } /* Large desktop */ @mixin xl { @media ( min-width : 1200 px ) { @content ; } } index scss and js If index.scss file exist at root src folder from previous chapter, empty it otherwise create it. index.js: import \"./assets/styles/styles.scss\" ; import \"./index.scss\" ; console . log ( \"index\" ); Have a try npm start browse to local","title":"118-styles-setup"},{"location":"javascript/training/16-project-blog/118-styles-setup/#118-styles-setup","text":"Setting up styles for our application.","title":"118-styles-setup"},{"location":"javascript/training/16-project-blog/118-styles-setup/#setup","text":"Create a styles folder inside assets folder: ../blog-001$ mkdir src/assets/styles We start by creating the scss files we will need in folder src/assets/styles : Sass @import and Partials ../blog-001$ cd src/assets/styles touch styles.scss touch _reset.scss touch _media-queries.scss touch _classes.scss touch _base.scss touch _variables.scss touch _utils.scss Edit styles.scss to import partials, no need to put underscore as file name prefix due to the fact those are partials: @import \"base\" ; @import \"classes\" ; @import \"media-queries\" ; @import \"reset\" ; @import \"utils\" ; @import \"variables\" ;","title":"Setup"},{"location":"javascript/training/16-project-blog/118-styles-setup/#reset","text":"_reset.scss: * { margin : 0 ; padding : 0 ; box-sizing : border-box ; }","title":"reset"},{"location":"javascript/training/16-project-blog/118-styles-setup/#variables","text":"To choose colors, FLAT UI COLOR For --font-family c.f. fonts section For --box-shadow it comes from inspected element, \"card\" class on firebase web site. _variables.scss: :root { --primary : #2ecc71 ; --dark : #27ae60 ; --accent : #2c3e50 ; --text : #333 ; --divider : #ecf0f1 ; --font-family : \"Muli\" , sans-serif ; --box-shadow : 0 1px 2px 0 rgba ( 60 , 64 , 67 , .3 ), 0 1px 3px 1px rgba ( 60 , 64 , 67 , .15 ) }","title":"variables"},{"location":"javascript/training/16-project-blog/118-styles-setup/#fonts","text":"We use Muli from google fonts light 300, regular 400, bold 700 index.html: < head > ... < link href = \"https://fonts.googleapis.com/css?family=Muli:300,400,700&display=swap\" rel = \"stylesheet\" /> ... </ head >","title":"fonts"},{"location":"javascript/training/16-project-blog/118-styles-setup/#base","text":"_base.scss: :root { font-size : 62 .5 % ; } body { font-size : 1 .6rem ; color : var ( --text ); font-family : var ( --font-family ); } h1 , h2 , h3 , h4 { margin-bottom : 2rem ; } h1 { font-size : 3 .5rem ; } h2 { font-size : 3rem ; } h3 { font-size : 2 .5rem ; } h4 { font-size : 2rem ; } ul { list-style : none ; } img { max-width : 100 % ; } a { color : var ( --text ); text-decoration : none ; }","title":"base"},{"location":"javascript/training/16-project-blog/118-styles-setup/#media-queries","text":"From CV project of HTML & CSS course _media-queries.scss: /* Landscape phones and down */ @mixin xs { @media ( max-width : 480 px ) { @content ; } } /* Landscape phone to portrait tablet */ @mixin sm { @media ( max-width : 767 px ) { @content ; } } /* Portrait tablet to landscape and desktop */ @mixin md { @media ( min-width : 768 px ) and ( max-width : 979 px ) { @content ; } } /* Large desktop */ @mixin xl { @media ( min-width : 1200 px ) { @content ; } }","title":"media-queries"},{"location":"javascript/training/16-project-blog/118-styles-setup/#index-scss-and-js","text":"If index.scss file exist at root src folder from previous chapter, empty it otherwise create it. index.js: import \"./assets/styles/styles.scss\" ; import \"./index.scss\" ; console . log ( \"index\" );","title":"index scss and js"},{"location":"javascript/training/16-project-blog/118-styles-setup/#have-a-try","text":"npm start browse to local","title":"Have a try"},{"location":"javascript/training/16-project-blog/119-layout-setup/","text":"119-layout-setup Setting up layout for the main page, locations for the topbar, footer and main content of the page html index.html: ... < body > < div class = \"container\" > < header > header </ header > < div class = \"content\" > content </ div > < footer > footer </ footer > </ div > </ body > ... sass styles.scss: ... .container { min-height : 100vh ; display : grid ; grid : \"header\" auto \"content\" 1fr \"footer\" auto / auto ; } header { grid-area : header ; background : green ; padding : 20px ; } .content { grid-area : content ; background : red ; padding : 20px ; } footer { grid-area : footer ; background : #333 ; padding : 20px ; } ...","title":"119-layout-setup"},{"location":"javascript/training/16-project-blog/119-layout-setup/#119-layout-setup","text":"Setting up layout for the main page, locations for the topbar, footer and main content of the page","title":"119-layout-setup"},{"location":"javascript/training/16-project-blog/119-layout-setup/#html","text":"index.html: ... < body > < div class = \"container\" > < header > header </ header > < div class = \"content\" > content </ div > < footer > footer </ footer > </ div > </ body > ...","title":"html"},{"location":"javascript/training/16-project-blog/119-layout-setup/#sass","text":"styles.scss: ... .container { min-height : 100vh ; display : grid ; grid : \"header\" auto \"content\" 1fr \"footer\" auto / auto ; } header { grid-area : header ; background : green ; padding : 20px ; } .content { grid-area : content ; background : red ; padding : 20px ; } footer { grid-area : footer ; background : #333 ; padding : 20px ; } ...","title":"sass"},{"location":"javascript/training/16-project-blog/120-header-footer-setup/","text":"120-header-footer-setup Three elements, logo, link to welcome page and link to article creation page in header and only copyright mention in footer. html index.html: ... < body > < div class = \"container\" > < header > < a class = \"header-brand\" href = \"./index.html\" > Blog </ a > < ul > < li > < a href = \"./index.html\" class = \"header-nav active\" > Welcome </ a > </ li > < li > < a href = \"./form.html\" class = \"header-nav\" > Article creation </ a > </ li > </ ul > </ header > < div class = \"content\" > content </ div > < footer > \u00a9 blog 2022 </ footer > </ div > </ body > ... sass _variables.scss: :root { --primary : #2ecc71 ; --dark : #27ae60 ; --accent : #2c3e50 ; --text : #333 ; --divider : #ecf0f1 ; --dark-gey : #324355 ; --font-family : \"Muli\" , sans-serif ; --box-shadow : 0 1px 2px 0 rgba ( 60 , 64 , 67 , .3 ), 0 1px 3px 1px rgba ( 60 , 64 , 67 , .15 ) } styles.scss: ... header { grid-area : header ; background : var ( --dark ); padding : 20px ; display : flex ; flex-flow : row nowrap ; justify-content : space-between ; align-items : center ; a { color : white ; } .header-brand { font-size : 4rem ; font-weight : 700 ; } ul { display : flex ; li { .header-nav { font-size : 1 .8rem ; padding : 0 10px ; } .active { font-weight : 700 ; text-decoration : underline ; } } } } ... footer { grid-area : footer ; padding : 2rem ; background : var ( --dark-gey ); font-size : 1 .8rem ; text-align : center ; color : white ; } ...","title":"120-header-footer-setup"},{"location":"javascript/training/16-project-blog/120-header-footer-setup/#120-header-footer-setup","text":"Three elements, logo, link to welcome page and link to article creation page in header and only copyright mention in footer.","title":"120-header-footer-setup"},{"location":"javascript/training/16-project-blog/120-header-footer-setup/#html","text":"index.html: ... < body > < div class = \"container\" > < header > < a class = \"header-brand\" href = \"./index.html\" > Blog </ a > < ul > < li > < a href = \"./index.html\" class = \"header-nav active\" > Welcome </ a > </ li > < li > < a href = \"./form.html\" class = \"header-nav\" > Article creation </ a > </ li > </ ul > </ header > < div class = \"content\" > content </ div > < footer > \u00a9 blog 2022 </ footer > </ div > </ body > ...","title":"html"},{"location":"javascript/training/16-project-blog/120-header-footer-setup/#sass","text":"_variables.scss: :root { --primary : #2ecc71 ; --dark : #27ae60 ; --accent : #2c3e50 ; --text : #333 ; --divider : #ecf0f1 ; --dark-gey : #324355 ; --font-family : \"Muli\" , sans-serif ; --box-shadow : 0 1px 2px 0 rgba ( 60 , 64 , 67 , .3 ), 0 1px 3px 1px rgba ( 60 , 64 , 67 , .15 ) } styles.scss: ... header { grid-area : header ; background : var ( --dark ); padding : 20px ; display : flex ; flex-flow : row nowrap ; justify-content : space-between ; align-items : center ; a { color : white ; } .header-brand { font-size : 4rem ; font-weight : 700 ; } ul { display : flex ; li { .header-nav { font-size : 1 .8rem ; padding : 0 10px ; } .active { font-weight : 700 ; text-decoration : underline ; } } } } ... footer { grid-area : footer ; padding : 2rem ; background : var ( --dark-gey ); font-size : 1 .8rem ; text-align : center ; color : white ; } ...","title":"sass"},{"location":"javascript/training/16-project-blog/121-form-create-article/","text":"121-form-create-article Set up of form to create article. form.js Import global styles in form.js : import \"../assets/styles/styles.scss\" ; import \"./form.scss\" ; console . log ( \"form\" ); form.html Html setup for form page, form.html : <!DOCTYPE html> < html lang = \"fr\" > < head > < meta charset = \"UTF-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"ie=edge\" /> < link href = \"https://fonts.googleapis.com/css?family=Muli:300,400,700&display=swap\" rel = \"stylesheet\" /> < title > Form </ title > </ head > < body > < div class = \"container\" > < header > < a class = \"header-brand\" href = \"./index.html\" > Blog </ a > < ul > < li > < a href = \"./index.html\" class = \"header-nav\" > Welcome </ a > </ li > < li > < a href = \"./form.html\" class = \"header-nav active\" > Article creation </ a > </ li > </ ul > </ header > < div class = \"content\" > < form > < h2 class = \"title-underline\" > Add new article </ h2 > < div class = \"form-group\" > < label > Author </ label > < input type = \"text\" name = \"author\" /> </ div > < div class = \"form-group\" > < label > Category </ label > < input type = \"text\" name = \"category\" /> </ div > < div class = \"form-group\" > < label > Article </ label > < textarea name = \"content\" ></ textarea > </ div > < div class = \"form-btn-container\" > < button class = \"btn btn-secondary\" > Cancel </ button > < button class = \"btn btn-primary\" > Save </ button > </ div > </ form > </ div > < footer > \u00a9 blog 2022 </ footer > </ div > </ body > </ html > active class is set on Article creation link, this way we know where we are. form.scss Own style for the form defined in the file form.scss : .content { display : flex ; justify-content : center ; align-items : center ; form { width : 700px ; padding : 4rem ; box-shadow : var ( --box-shadow ); border-radius : 3px ; display : flex ; flex-direction : column ; .form-group { margin-bottom : 2rem ; display : flex ; flex-direction : column ; label { font-size : 1 .8rem ; color : var ( --accent ); font-weight : 700 ; margin-bottom : 1rem ; } input , textarea { border : 1px solid var ( --divider ); padding : 1rem ; outline : 0 ; border-radius : 3px ; & :focus { border-color : var ( --accent ); } } textarea { min-height : 20rem ; } } .form-btn-container { display : flex ; justify-content : flex-end ; align-content : center ; .btn { margin-left : 0 .5rem ; } } } } _classes.scss We add some generique classes commonly used in partial, file _classes.scss: .title { & -underline { padding-bottom : 2rem ; border-bottom : 1px solid var ( --divider ); } } .btn { border : 0 ; border-radius : 3px ; padding : 1rem 2rem ; font-weight : 700 ; cursor : pointer ; transition : box-shadow 0 .2s ; & :hover { box-shadow : var ( --box-shadow ); transition : box-shadow 0 .2s ; } & -primary { background : var ( --primary ); color : white ; } & -secondary { border : 1px solid var ( --primary ); background : white ; color : var ( --primary ); } }","title":"121-form-create-article"},{"location":"javascript/training/16-project-blog/121-form-create-article/#121-form-create-article","text":"Set up of form to create article.","title":"121-form-create-article"},{"location":"javascript/training/16-project-blog/121-form-create-article/#formjs","text":"Import global styles in form.js : import \"../assets/styles/styles.scss\" ; import \"./form.scss\" ; console . log ( \"form\" );","title":"form.js"},{"location":"javascript/training/16-project-blog/121-form-create-article/#formhtml","text":"Html setup for form page, form.html : <!DOCTYPE html> < html lang = \"fr\" > < head > < meta charset = \"UTF-8\" /> < meta name = \"viewport\" content = \"width=device-width, initial-scale=1.0\" /> < meta http-equiv = \"X-UA-Compatible\" content = \"ie=edge\" /> < link href = \"https://fonts.googleapis.com/css?family=Muli:300,400,700&display=swap\" rel = \"stylesheet\" /> < title > Form </ title > </ head > < body > < div class = \"container\" > < header > < a class = \"header-brand\" href = \"./index.html\" > Blog </ a > < ul > < li > < a href = \"./index.html\" class = \"header-nav\" > Welcome </ a > </ li > < li > < a href = \"./form.html\" class = \"header-nav active\" > Article creation </ a > </ li > </ ul > </ header > < div class = \"content\" > < form > < h2 class = \"title-underline\" > Add new article </ h2 > < div class = \"form-group\" > < label > Author </ label > < input type = \"text\" name = \"author\" /> </ div > < div class = \"form-group\" > < label > Category </ label > < input type = \"text\" name = \"category\" /> </ div > < div class = \"form-group\" > < label > Article </ label > < textarea name = \"content\" ></ textarea > </ div > < div class = \"form-btn-container\" > < button class = \"btn btn-secondary\" > Cancel </ button > < button class = \"btn btn-primary\" > Save </ button > </ div > </ form > </ div > < footer > \u00a9 blog 2022 </ footer > </ div > </ body > </ html > active class is set on Article creation link, this way we know where we are.","title":"form.html"},{"location":"javascript/training/16-project-blog/121-form-create-article/#formscss","text":"Own style for the form defined in the file form.scss : .content { display : flex ; justify-content : center ; align-items : center ; form { width : 700px ; padding : 4rem ; box-shadow : var ( --box-shadow ); border-radius : 3px ; display : flex ; flex-direction : column ; .form-group { margin-bottom : 2rem ; display : flex ; flex-direction : column ; label { font-size : 1 .8rem ; color : var ( --accent ); font-weight : 700 ; margin-bottom : 1rem ; } input , textarea { border : 1px solid var ( --divider ); padding : 1rem ; outline : 0 ; border-radius : 3px ; & :focus { border-color : var ( --accent ); } } textarea { min-height : 20rem ; } } .form-btn-container { display : flex ; justify-content : flex-end ; align-content : center ; .btn { margin-left : 0 .5rem ; } } } }","title":"form.scss"},{"location":"javascript/training/16-project-blog/121-form-create-article/#_classesscss","text":"We add some generique classes commonly used in partial, file _classes.scss: .title { & -underline { padding-bottom : 2rem ; border-bottom : 1px solid var ( --divider ); } } .btn { border : 0 ; border-radius : 3px ; padding : 1rem 2rem ; font-weight : 700 ; cursor : pointer ; transition : box-shadow 0 .2s ; & :hover { box-shadow : var ( --box-shadow ); transition : box-shadow 0 .2s ; } & -primary { background : var ( --primary ); color : white ; } & -secondary { border : 1px solid var ( --primary ); background : white ; color : var ( --primary ); } }","title":"_classes.scss"},{"location":"javascript/training/16-project-blog/122-form-javascript/","text":"122-form-javascript Form management by JavaScript. _classes.scss Text style for error message in file _classes.scss: ... .text { & -error { color : var ( --error ); } } ... _variables.scss Variable color for error message in file _variables.scss: :root { ... --error : #e74c3c ; ... } form.html Html to handle errors in file form.html: < div class = \"content\" > < form > ... < ul class = \"text-error\" id = \"errors\" ></ ul > ... </ form > </ div > form.js JavaScript to handle form in file form.js: import \"../assets/styles/styles.scss\" ; import \"./form.scss\" ; console . log ( \"form.js\" ); const form = document . querySelector ( \"form\" ); const errorElement = document . querySelector ( \"#errors\" ); let errors = []; form . addEventListener ( \"submit\" , ( event ) => { event . preventDefault (); const formData = new FormData ( form ); const article = Object . fromEntries ( formData . entries ()); if ( formIsValid ( article )) { const json = JSON . stringify ( article ); // fetch } }); const formIsValid = ( article ) => { errors = []; if ( ! article . author || ! article . category || ! article . content ) { errors . push ( \"All fields must be filled!\" ); } if ( article . content . length < 20 ) { errors . push ( \"Article too short!\" ); } if ( errors . length ) { let errorHtml = \"\" ; errors . forEach (( e ) => { errorHtml += `<li> ${ e } <\\li>` ; }); errorElement . innerHTML = errorHtml ; return false ; } else { errorElement . innerHTML = \"\" ; return true ; } };","title":"122-form-javascript"},{"location":"javascript/training/16-project-blog/122-form-javascript/#122-form-javascript","text":"Form management by JavaScript.","title":"122-form-javascript"},{"location":"javascript/training/16-project-blog/122-form-javascript/#_classesscss","text":"Text style for error message in file _classes.scss: ... .text { & -error { color : var ( --error ); } } ...","title":"_classes.scss"},{"location":"javascript/training/16-project-blog/122-form-javascript/#_variablesscss","text":"Variable color for error message in file _variables.scss: :root { ... --error : #e74c3c ; ... }","title":"_variables.scss"},{"location":"javascript/training/16-project-blog/122-form-javascript/#formhtml","text":"Html to handle errors in file form.html: < div class = \"content\" > < form > ... < ul class = \"text-error\" id = \"errors\" ></ ul > ... </ form > </ div >","title":"form.html"},{"location":"javascript/training/16-project-blog/122-form-javascript/#formjs","text":"JavaScript to handle form in file form.js: import \"../assets/styles/styles.scss\" ; import \"./form.scss\" ; console . log ( \"form.js\" ); const form = document . querySelector ( \"form\" ); const errorElement = document . querySelector ( \"#errors\" ); let errors = []; form . addEventListener ( \"submit\" , ( event ) => { event . preventDefault (); const formData = new FormData ( form ); const article = Object . fromEntries ( formData . entries ()); if ( formIsValid ( article )) { const json = JSON . stringify ( article ); // fetch } }); const formIsValid = ( article ) => { errors = []; if ( ! article . author || ! article . category || ! article . content ) { errors . push ( \"All fields must be filled!\" ); } if ( article . content . length < 20 ) { errors . push ( \"Article too short!\" ); } if ( errors . length ) { let errorHtml = \"\" ; errors . forEach (( e ) => { errorHtml += `<li> ${ e } <\\li>` ; }); errorElement . innerHTML = errorHtml ; return false ; } else { errorElement . innerHTML = \"\" ; return true ; } };","title":"form.js"},{"location":"javascript/training/16-project-blog/123-send-article/","text":"123-send-article As a backend for testing purpose we use api of restapi.fr form.js JavaScript to send form (article content) to server in file form.js: import \"../assets/styles/styles.scss\" ; import \"./form.scss\" ; console . log ( \"form.js\" ); const form = document . querySelector ( \"form\" ); const errorElement = document . querySelector ( \"#errors\" ); let errors = []; form . addEventListener ( \"submit\" , async ( event ) => { event . preventDefault (); const formData = new FormData ( form ); const article = Object . fromEntries ( formData . entries ()); try { if ( formIsValid ( article )) { const json = JSON . stringify ( article ); const response = await fetch ( \"https://restapi.fr/api/article\" , { method : \"POST\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, }); const body = await response . json (); console . log ( body ); } } catch ( e ) { console . error ( \"e: \" , e ); } }); const formIsValid = ( article ) => { errors = []; if ( ! article . author || ! article . category || ! article . content ) { errors . push ( \"All fields must be filled!\" ); } if ( article . content . length < 20 ) { errors . push ( \"Article too short!\" ); } if ( errors . length ) { let errorHtml = \"\" ; errors . forEach (( e ) => { errorHtml += `<li> ${ e } <\\li>` ; }); errorElement . innerHTML = errorHtml ; return false ; } else { errorElement . innerHTML = \"\" ; return true ; } };","title":"123-send-article"},{"location":"javascript/training/16-project-blog/123-send-article/#123-send-article","text":"As a backend for testing purpose we use api of restapi.fr","title":"123-send-article"},{"location":"javascript/training/16-project-blog/123-send-article/#formjs","text":"JavaScript to send form (article content) to server in file form.js: import \"../assets/styles/styles.scss\" ; import \"./form.scss\" ; console . log ( \"form.js\" ); const form = document . querySelector ( \"form\" ); const errorElement = document . querySelector ( \"#errors\" ); let errors = []; form . addEventListener ( \"submit\" , async ( event ) => { event . preventDefault (); const formData = new FormData ( form ); const article = Object . fromEntries ( formData . entries ()); try { if ( formIsValid ( article )) { const json = JSON . stringify ( article ); const response = await fetch ( \"https://restapi.fr/api/article\" , { method : \"POST\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, }); const body = await response . json (); console . log ( body ); } } catch ( e ) { console . error ( \"e: \" , e ); } }); const formIsValid = ( article ) => { errors = []; if ( ! article . author || ! article . category || ! article . content ) { errors . push ( \"All fields must be filled!\" ); } if ( article . content . length < 20 ) { errors . push ( \"Article too short!\" ); } if ( errors . length ) { let errorHtml = \"\" ; errors . forEach (( e ) => { errorHtml += `<li> ${ e } <\\li>` ; }); errorElement . innerHTML = errorHtml ; return false ; } else { errorElement . innerHTML = \"\" ; return true ; } };","title":"form.js"},{"location":"javascript/training/16-project-blog/124-articles-list/","text":"124-articles-list Design setup on how to render article list to home page. index.html ... < div class = \"content\" > < div class = \"articles-container\" > < div class = \"article\" > < img src = \"./assets/images/73.jpg\" alt = \"profile\" /> < h2 > Article title </ h2 > < p class = \"article-author\" > John Doe </ p > < p class = \"article-content\" > Lorem ipsum dolor sit amet consectetur adipisicing elit. Magni autem, porro dicta dolores beatae dolor officia accusantium, esse dolore pariatur ad tempore, aperiam odit? Placeat doloremque, voluptatum provident praesentium minus reiciendis fuga magni. Incidunt reiciendis corrupti consectetur nihil aperiam! Repudiandae qui sint rem inventore modi impedit, provident eaque consequatur deserunt? </ p > < div class = \"article-actions\" > < button class = \"btn btn-delete\" > Delete </ button > < button class = \"btn btn-primary\" > Edit </ button > </ div > </ div > </ div > </ div > ... index.scss .content { display : flex ; justify-content : center ; align-items : center ; .articles-container { max-width : 800px ; width : 100 % ; margin : 5rem 0 10rem 0 ; .article { background : white ; padding : 0 5rem ; border-radius : 0 .3rem ; display : flex ; margin-bottom : 5rem ; flex-direction : column ; align-items : center ; box-shadow : var ( --box-shadow ); img { height : 6rem ; width : 6rem ; border-radius : 50 % ; margin-top : -3rem ; } h2 { margin-top : 2rem ; margin-bottom : 0 ; } .article-author { color : var ( --hint ); margin-bottom : 3rem ; } .article-content { max-width : 550px ; } .article-actions { width : 100 % ; display : flex ; justify-content : flex-end ; padding : 2rem 0 ; margin-top : 2rem ; border-top : 1px solid var ( --divider ); .btn { margin-left : 1rem ; } } } } } _variables.scss ... --hint : #95a5a6 ; ... styles.scss ... .content { ... background : var ( --divider ); } ... form.scss .content { ... form { background : white ; ... } ... } ...","title":"124-articles-list"},{"location":"javascript/training/16-project-blog/124-articles-list/#124-articles-list","text":"Design setup on how to render article list to home page.","title":"124-articles-list"},{"location":"javascript/training/16-project-blog/124-articles-list/#indexhtml","text":"... < div class = \"content\" > < div class = \"articles-container\" > < div class = \"article\" > < img src = \"./assets/images/73.jpg\" alt = \"profile\" /> < h2 > Article title </ h2 > < p class = \"article-author\" > John Doe </ p > < p class = \"article-content\" > Lorem ipsum dolor sit amet consectetur adipisicing elit. Magni autem, porro dicta dolores beatae dolor officia accusantium, esse dolore pariatur ad tempore, aperiam odit? Placeat doloremque, voluptatum provident praesentium minus reiciendis fuga magni. Incidunt reiciendis corrupti consectetur nihil aperiam! Repudiandae qui sint rem inventore modi impedit, provident eaque consequatur deserunt? </ p > < div class = \"article-actions\" > < button class = \"btn btn-delete\" > Delete </ button > < button class = \"btn btn-primary\" > Edit </ button > </ div > </ div > </ div > </ div > ...","title":"index.html"},{"location":"javascript/training/16-project-blog/124-articles-list/#indexscss","text":".content { display : flex ; justify-content : center ; align-items : center ; .articles-container { max-width : 800px ; width : 100 % ; margin : 5rem 0 10rem 0 ; .article { background : white ; padding : 0 5rem ; border-radius : 0 .3rem ; display : flex ; margin-bottom : 5rem ; flex-direction : column ; align-items : center ; box-shadow : var ( --box-shadow ); img { height : 6rem ; width : 6rem ; border-radius : 50 % ; margin-top : -3rem ; } h2 { margin-top : 2rem ; margin-bottom : 0 ; } .article-author { color : var ( --hint ); margin-bottom : 3rem ; } .article-content { max-width : 550px ; } .article-actions { width : 100 % ; display : flex ; justify-content : flex-end ; padding : 2rem 0 ; margin-top : 2rem ; border-top : 1px solid var ( --divider ); .btn { margin-left : 1rem ; } } } } }","title":"index.scss"},{"location":"javascript/training/16-project-blog/124-articles-list/#_variablesscss","text":"... --hint : #95a5a6 ; ...","title":"_variables.scss"},{"location":"javascript/training/16-project-blog/124-articles-list/#stylesscss","text":"... .content { ... background : var ( --divider ); } ...","title":"styles.scss"},{"location":"javascript/training/16-project-blog/124-articles-list/#formscss","text":".content { ... form { background : white ; ... } ... } ...","title":"form.scss"},{"location":"javascript/training/16-project-blog/125-articles-list-display/","text":"125-articles-list-display form.html We add two new properties to article, title and img for author profil image. ... < div class = \"content\" > < form > ... < div class = \"form-group\" > < label > Profile Picture </ label > < input type = \"text\" name = \"img\" /> </ div > ... < div class = \"form-group\" > < label > Title </ label > < input type = \"text\" name = \"title\" /> </ div > ... </ div > </ form > ... form.js Validation of those two new fields. ... const formIsValid = ( article ) => { errors = []; if ( ! article . author || ! article . category || ! article . content || ! article . img || ! article . title ) ... }; index.html Remove hard coded article because the list will be then created from JavaScript. ... < body > < div class = \"container\" > ... < div class = \"content\" > < div class = \"articles-container\" ></ div > </ div > ... </ div > </ body > ... _classes.scss New class for delete button. ... .btn { ... & -delete { border : 1px solid var ( --error ); background : white ; color : var ( --error ); } } ... index.scss Add a CSS property to allow new lines to be correctly interpreted in textarea of article content. .content { ... .articles-container { ... .article { ... .article-content { ... white-space : pre-line ; } ... } } } index.js To handle display of articles list. import \"./assets/styles/styles.scss\" ; import \"./index.scss\" ; console . log ( \"index.js\" ); const articleContainerElement = document . querySelector ( \".articles-container\" ); const createArticles = ( articles ) => { const articlesDOM = articles . map (( article ) => { const articleDOM = document . createElement ( \"div\" ); articleDOM . classList . add ( \"article\" ); articleDOM . innerHTML = ` <img src=\" ${ article . img } \" alt=\"profile\" /> <h2> ${ article . title } </h2> <p class=\"article-author\"> ${ article . author } - ${ article . category } </p> <p class=\"article-content\"> ${ article . content } </p> <div class=\"article-actions\"> <button class=\"btn btn-delete\" data-id= ${ article . _id } >Delete</button> </div> ` ; return articleDOM ; }); articleContainerElement . innerHTML = \"\" ; articleContainerElement . append (... articlesDOM ); }; const fetchArticle = async () => { try { const response = await fetch ( \"https://restapi.fr/api/article\" ); let articles = await response . json (); console . log ( articles ); // Standard api behavior return not an array if only one element // so below code convert it (one element) into an array // otherwise it cause an error when 'map' method (to create articles) will be called on it. if ( ! Array . isArray ( articles )) { articles = [ articles ]; } createArticles ( articles ); } catch ( e ) { console . log ( \"e : \" , e ); } }; fetchArticle ();","title":"125-articles-list-display"},{"location":"javascript/training/16-project-blog/125-articles-list-display/#125-articles-list-display","text":"","title":"125-articles-list-display"},{"location":"javascript/training/16-project-blog/125-articles-list-display/#formhtml","text":"We add two new properties to article, title and img for author profil image. ... < div class = \"content\" > < form > ... < div class = \"form-group\" > < label > Profile Picture </ label > < input type = \"text\" name = \"img\" /> </ div > ... < div class = \"form-group\" > < label > Title </ label > < input type = \"text\" name = \"title\" /> </ div > ... </ div > </ form > ...","title":"form.html"},{"location":"javascript/training/16-project-blog/125-articles-list-display/#formjs","text":"Validation of those two new fields. ... const formIsValid = ( article ) => { errors = []; if ( ! article . author || ! article . category || ! article . content || ! article . img || ! article . title ) ... };","title":"form.js"},{"location":"javascript/training/16-project-blog/125-articles-list-display/#indexhtml","text":"Remove hard coded article because the list will be then created from JavaScript. ... < body > < div class = \"container\" > ... < div class = \"content\" > < div class = \"articles-container\" ></ div > </ div > ... </ div > </ body > ...","title":"index.html"},{"location":"javascript/training/16-project-blog/125-articles-list-display/#_classesscss","text":"New class for delete button. ... .btn { ... & -delete { border : 1px solid var ( --error ); background : white ; color : var ( --error ); } } ...","title":"_classes.scss"},{"location":"javascript/training/16-project-blog/125-articles-list-display/#indexscss","text":"Add a CSS property to allow new lines to be correctly interpreted in textarea of article content. .content { ... .articles-container { ... .article { ... .article-content { ... white-space : pre-line ; } ... } } }","title":"index.scss"},{"location":"javascript/training/16-project-blog/125-articles-list-display/#indexjs","text":"To handle display of articles list. import \"./assets/styles/styles.scss\" ; import \"./index.scss\" ; console . log ( \"index.js\" ); const articleContainerElement = document . querySelector ( \".articles-container\" ); const createArticles = ( articles ) => { const articlesDOM = articles . map (( article ) => { const articleDOM = document . createElement ( \"div\" ); articleDOM . classList . add ( \"article\" ); articleDOM . innerHTML = ` <img src=\" ${ article . img } \" alt=\"profile\" /> <h2> ${ article . title } </h2> <p class=\"article-author\"> ${ article . author } - ${ article . category } </p> <p class=\"article-content\"> ${ article . content } </p> <div class=\"article-actions\"> <button class=\"btn btn-delete\" data-id= ${ article . _id } >Delete</button> </div> ` ; return articleDOM ; }); articleContainerElement . innerHTML = \"\" ; articleContainerElement . append (... articlesDOM ); }; const fetchArticle = async () => { try { const response = await fetch ( \"https://restapi.fr/api/article\" ); let articles = await response . json (); console . log ( articles ); // Standard api behavior return not an array if only one element // so below code convert it (one element) into an array // otherwise it cause an error when 'map' method (to create articles) will be called on it. if ( ! Array . isArray ( articles )) { articles = [ articles ]; } createArticles ( articles ); } catch ( e ) { console . log ( \"e : \" , e ); } }; fetchArticle ();","title":"index.js"},{"location":"javascript/training/16-project-blog/126-delete-article/","text":"126-delete-article index.js We only have to take care of JavaScript for welcome page. On every article we have added data-id=${article._id} that allow to get access on DOM to id of each article set by database that is unique. Create a reference to all delete buttons with querySelectorAll . Go through every buttons to add a listener on click event. If a click occure target button will execute his event handler function and we know which article is concerned with event.target.dataset.id . All that remain to do is to send a DELETE request to the server to delete article. Once server answerd and everything OK we request updated articles list. import { async } from \"regenerator-runtime\" ; ... const createArticles = ( articles ) => { ... const deleteButtons = articleContainerElement . querySelectorAll ( \".btn-delete\" ); console . log ( deleteButtons ); deleteButtons . forEach (( button ) => { button . addEventListener ( \"click\" , async ( event ) => { try { const target = event . target ; const articleId = target . dataset . id ; console . log ( articleId ); const response = await fetch ( `https://restapi.fr/api/article/ ${ articleId } ` , { method : \"DELETE\" , } ); const body = await response . json (); console . log ( body ); fetchArticle (); } catch ( e ) { console . log ( \"e: \" , e ); } }); }); }; ...","title":"126-delete-article"},{"location":"javascript/training/16-project-blog/126-delete-article/#126-delete-article","text":"","title":"126-delete-article"},{"location":"javascript/training/16-project-blog/126-delete-article/#indexjs","text":"We only have to take care of JavaScript for welcome page. On every article we have added data-id=${article._id} that allow to get access on DOM to id of each article set by database that is unique. Create a reference to all delete buttons with querySelectorAll . Go through every buttons to add a listener on click event. If a click occure target button will execute his event handler function and we know which article is concerned with event.target.dataset.id . All that remain to do is to send a DELETE request to the server to delete article. Once server answerd and everything OK we request updated articles list. import { async } from \"regenerator-runtime\" ; ... const createArticles = ( articles ) => { ... const deleteButtons = articleContainerElement . querySelectorAll ( \".btn-delete\" ); console . log ( deleteButtons ); deleteButtons . forEach (( button ) => { button . addEventListener ( \"click\" , async ( event ) => { try { const target = event . target ; const articleId = target . dataset . id ; console . log ( articleId ); const response = await fetch ( `https://restapi.fr/api/article/ ${ articleId } ` , { method : \"DELETE\" , } ); const body = await response . json (); console . log ( body ); fetchArticle (); } catch ( e ) { console . log ( \"e: \" , e ); } }); }); }; ...","title":"index.js"},{"location":"javascript/training/16-project-blog/127-responsive-project/","text":"127-responsive-project Adapt the project for mobile, by creating, among other things, a mobile menu in JavaScript . index.scss We set a maximum width and a width of 100% so that the width of the content fits the screen. We use an @include xs media query to decrease padding on mobile. index.scss: @import \"./assets/styles/media-queries\" ; .content { ... .articles-container { max-width : 800px ; width : 100 % ; ... .article { ... padding : 0 5rem ; @ include xs { padding : 0 2rem ; } ... } form.scss For the width on form.scss we make the same modifications as for index.scss . form.scss: .content { ... form { ... max-width : 700px ; width : 100 % ; ... } index.html We add fonteawsome in index.html to use hamburger icon (mobile menu). We also add a container for the header menu and for the icon. index.html: ... < html > < head > ... < script src = \"https://kit.fontawesome.com/671ba50d24.js\" crossorigin = \"anonymous\" ></ script > ... </ head > < body > ... < header > ... < div class = \"header-menu\" > ... < div class = \"header-menu-icon\" > < i class = \"fas fa-bars\" ></ i > </ div > </ div > </ header > ... </ html > styles.scss We make the list of links disappear with a media query on mobile. The default header-menu-icon is not displayed and we only display it on mobile with a media query as well. For the mobile menu, we do not display it by default. Only when we add a specific class in JavaScript . styles.scss: ... header { ... .header-menu { position : relative ; ul { @include xs { display : none ; } ... .header-menu-icon { display : none ; font-size : 3rem ; color : white ; @ include xs { display : block ; } } .mobile-menu { display : none ; position : absolute ; box-shadow : var ( --box-shadow ); top : 9 .5rem ; right : 1rem ; padding : 3rem 1 .5rem ; width : 20rem ; background : white ; ul { display : block ; li { margin : 2rem 0 ; a { color : var ( --text ); } } } } .mobile-menu.open { display : block ; ... topbar.js Manage to display the top bar menun in JavaScript . topbar.js: console . log ( \"topbar.js\" ); const iconMobile = document . querySelector ( \".header-menu-icon\" ); const headerMenu = document . querySelector ( \".header-menu\" ); let isMenuOpen = false ; let mobileMenuDom ; const closeMenu = () => { mobileMenuDom . classList . remove ( \"open\" ); }; const createMobileMenu = () => { mobileMenuDom = document . createElement ( \"div\" ); mobileMenuDom . classList . add ( \"mobile-menu\" ); mobileMenuDom . addEventListener ( \"click\" , ( event ) => { event . stopPropagation (); }); mobileMenuDom . append ( headerMenu . querySelector ( \"ul\" ). cloneNode ( true )); headerMenu . append ( mobileMenuDom ); }; const openMenu = () => { if ( mobileMenuDom ) { } else { createMobileMenu (); } mobileMenuDom . classList . add ( \"open\" ); }; const toggleMobileMenu = ( event ) => { console . log ( event ); if ( isMenuOpen ) { closeMenu (); } else { openMenu (); } isMenuOpen = ! isMenuOpen ; }; iconMobile . addEventListener ( \"click\" , ( event ) => { event . stopPropagation (); toggleMobileMenu (); }); window . addEventListener ( \"click\" , () => { if ( isMenuOpen ) { toggleMobileMenu (); } }); window . addEventListener ( \"resize\" , ( event ) => { console . log ( window . innerWidth ); if ( window . innerWidth > 480 && isMenuOpen ) { toggleMobileMenu (); } }); form.html Edit form.html to follow menu modifications. form.html: < html > < head > ... < script src = \"https://kit.fontawesome.com/671ba50d24.js\" crossorigin = \"anonymous\" ></ script > ... </ head > < body > ... < header > ... < div class = \"header-menu\" > ... < div class = \"header-menu-icon\" > < i class = \"fas fa-bars\" ></ i > </ div > </ div > </ header > ... </ body > </ html >","title":"127-responsive-project"},{"location":"javascript/training/16-project-blog/127-responsive-project/#127-responsive-project","text":"Adapt the project for mobile, by creating, among other things, a mobile menu in JavaScript .","title":"127-responsive-project"},{"location":"javascript/training/16-project-blog/127-responsive-project/#indexscss","text":"We set a maximum width and a width of 100% so that the width of the content fits the screen. We use an @include xs media query to decrease padding on mobile. index.scss: @import \"./assets/styles/media-queries\" ; .content { ... .articles-container { max-width : 800px ; width : 100 % ; ... .article { ... padding : 0 5rem ; @ include xs { padding : 0 2rem ; } ... }","title":"index.scss"},{"location":"javascript/training/16-project-blog/127-responsive-project/#formscss","text":"For the width on form.scss we make the same modifications as for index.scss . form.scss: .content { ... form { ... max-width : 700px ; width : 100 % ; ... }","title":"form.scss"},{"location":"javascript/training/16-project-blog/127-responsive-project/#indexhtml","text":"We add fonteawsome in index.html to use hamburger icon (mobile menu). We also add a container for the header menu and for the icon. index.html: ... < html > < head > ... < script src = \"https://kit.fontawesome.com/671ba50d24.js\" crossorigin = \"anonymous\" ></ script > ... </ head > < body > ... < header > ... < div class = \"header-menu\" > ... < div class = \"header-menu-icon\" > < i class = \"fas fa-bars\" ></ i > </ div > </ div > </ header > ... </ html >","title":"index.html"},{"location":"javascript/training/16-project-blog/127-responsive-project/#stylesscss","text":"We make the list of links disappear with a media query on mobile. The default header-menu-icon is not displayed and we only display it on mobile with a media query as well. For the mobile menu, we do not display it by default. Only when we add a specific class in JavaScript . styles.scss: ... header { ... .header-menu { position : relative ; ul { @include xs { display : none ; } ... .header-menu-icon { display : none ; font-size : 3rem ; color : white ; @ include xs { display : block ; } } .mobile-menu { display : none ; position : absolute ; box-shadow : var ( --box-shadow ); top : 9 .5rem ; right : 1rem ; padding : 3rem 1 .5rem ; width : 20rem ; background : white ; ul { display : block ; li { margin : 2rem 0 ; a { color : var ( --text ); } } } } .mobile-menu.open { display : block ; ...","title":"styles.scss"},{"location":"javascript/training/16-project-blog/127-responsive-project/#topbarjs","text":"Manage to display the top bar menun in JavaScript . topbar.js: console . log ( \"topbar.js\" ); const iconMobile = document . querySelector ( \".header-menu-icon\" ); const headerMenu = document . querySelector ( \".header-menu\" ); let isMenuOpen = false ; let mobileMenuDom ; const closeMenu = () => { mobileMenuDom . classList . remove ( \"open\" ); }; const createMobileMenu = () => { mobileMenuDom = document . createElement ( \"div\" ); mobileMenuDom . classList . add ( \"mobile-menu\" ); mobileMenuDom . addEventListener ( \"click\" , ( event ) => { event . stopPropagation (); }); mobileMenuDom . append ( headerMenu . querySelector ( \"ul\" ). cloneNode ( true )); headerMenu . append ( mobileMenuDom ); }; const openMenu = () => { if ( mobileMenuDom ) { } else { createMobileMenu (); } mobileMenuDom . classList . add ( \"open\" ); }; const toggleMobileMenu = ( event ) => { console . log ( event ); if ( isMenuOpen ) { closeMenu (); } else { openMenu (); } isMenuOpen = ! isMenuOpen ; }; iconMobile . addEventListener ( \"click\" , ( event ) => { event . stopPropagation (); toggleMobileMenu (); }); window . addEventListener ( \"click\" , () => { if ( isMenuOpen ) { toggleMobileMenu (); } }); window . addEventListener ( \"resize\" , ( event ) => { console . log ( window . innerWidth ); if ( window . innerWidth > 480 && isMenuOpen ) { toggleMobileMenu (); } });","title":"topbar.js"},{"location":"javascript/training/16-project-blog/127-responsive-project/#formhtml","text":"Edit form.html to follow menu modifications. form.html: < html > < head > ... < script src = \"https://kit.fontawesome.com/671ba50d24.js\" crossorigin = \"anonymous\" ></ script > ... </ head > < body > ... < header > ... < div class = \"header-menu\" > ... < div class = \"header-menu-icon\" > < i class = \"fas fa-bars\" ></ i > </ div > </ div > </ header > ... </ body > </ html >","title":"form.html"},{"location":"javascript/training/16-project-blog/aws-01-setup/","text":"aws-01-setup How to host the project on AWS Cloud . The objective is to host the blog project on AWS Cloud with AWS Amplify and to create the API with Amazon API Gateway to interact with Amazon DynamoDB through AWS Lambda controlled by AWS Identity and Access Management . This documentation is based on Amazon tutorial for Creating a Simple Web Application . All the time, pay attention in which geographical region the different components of your application are located. AWS Amplify Create web application with AWS Amplify . Log on to AWS Management Console Browse to AWS Amplify console Click on New app - Host web app Select GitHub Select repository and branch Build and test settings are auto-detected, click Next then Save and deploy Protect your app with a password in App settings - Access control - Manage access , Access setting = Restricted - password required , then fill Username and Password Amazon DynamoDB Log on to Amazon DynamoDB console Click on Create table , provide a Table name Choose id as Partition key name of String type Let all the rest as default and click on Create table Be aware of Amazon Resource Name (ARN) of the table, we will need it later","title":"aws-01-setup"},{"location":"javascript/training/16-project-blog/aws-01-setup/#aws-01-setup","text":"How to host the project on AWS Cloud . The objective is to host the blog project on AWS Cloud with AWS Amplify and to create the API with Amazon API Gateway to interact with Amazon DynamoDB through AWS Lambda controlled by AWS Identity and Access Management . This documentation is based on Amazon tutorial for Creating a Simple Web Application . All the time, pay attention in which geographical region the different components of your application are located.","title":"aws-01-setup"},{"location":"javascript/training/16-project-blog/aws-01-setup/#aws-amplify","text":"Create web application with AWS Amplify . Log on to AWS Management Console Browse to AWS Amplify console Click on New app - Host web app Select GitHub Select repository and branch Build and test settings are auto-detected, click Next then Save and deploy Protect your app with a password in App settings - Access control - Manage access , Access setting = Restricted - password required , then fill Username and Password","title":"AWS Amplify"},{"location":"javascript/training/16-project-blog/aws-01-setup/#amazon-dynamodb","text":"Log on to Amazon DynamoDB console Click on Create table , provide a Table name Choose id as Partition key name of String type Let all the rest as default and click on Create table Be aware of Amazon Resource Name (ARN) of the table, we will need it later","title":"Amazon DynamoDB"},{"location":"javascript/training/16-project-blog/aws-02-create-item/","text":"aws-02-create-item AWS Lambda Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to create item: const AWS = require ( \"aws-sdk\" ); // Instantiate a DynamoDB document client with the SDK let dynamodb = new AWS . DynamoDB . DocumentClient (); // Define handler function, the entry point to our code for the Lambda service // We receive the object that triggers the function as a parameter exports . handler = async ( event , context ) => { // id from context as unique key to record in database let id = context . awsRequestId ; let createdAt = new Date (). toISOString (); // Extract values from event and format as strings let values = JSON . stringify ( `id: ${ id } , createdAt: ${ createdAt } , author: ${ event . author } , img: ${ event . img } , category: ${ event . category } , title: ${ event . title } , content: ${ event . content } ` ); // Create JSON object with parameters for DynamoDB and store in a variable let params = { TableName : \"YOUR-TABLE-NAME\" , Item : { id : id , createdAt : createdAt , author : event . author , img : event . img , category : event . category , title : event . title , content : event . content , }, }; // Using await, make sure object writes to DynamoDB table before continuing execution await dynamodb . put ( params ). promise (); // Create a JSON object with our response and store it in a constant const response = { statusCode : 200 , body : values , }; // Return the response constant return response ; }; Notice that the id is taken from the context ( let id = context.awsRequestId ) as the unique key to record in database. Then click on deploy. Add permission Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] } Test function Click on Test button to configure a test event, provide a name and paste below json: { \"author\" : \"Author 48\" , \"img\" : \"https://randomuser.me/api/portraits/men/48.jpg\" , \"category\" : \"Category 48\" , \"title\" : \"Title 48\" , \"content\" : \"Hello 48 Hello 48 Hello 48 Hello 48\" } To validate test is successful, browse to database to see above element has correctly been created. Amazon API Gateway Create new REST API Log on to Amazon API Gateway Create API - REST API - Build Choose REST protocol Create New API Settings, provide a name, choose Endpoint Type as Edge optimized it is the most suitable option for public services accessed from the Internet Create new resource and method In the left pane, click Resources under your API. Select the \u201c/\u201d resource, then click Create Method from the Action drop-down menu. Select POST from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created POST method, then Enable CORS mechanism from the Action drop-down menu. Leave the POST box unchecked and click the blue Enable CORS mechanism and replace existing CORS headers button. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API API deployment From the Actions drop-down list, select Deploy API. Select [New Stage] from the Deploy Stage drop-down list. Enter dev in the Stage Name field. Select Deploy. Copy the URL next to Invoke URL (you will need this later) and save. API validation On the left, click Resources. The methods available for the API are displayed on the right. Click POST . Click the blue lightning bolt icon. Paste a JSON that correspond to data format you want to store in your database, into the Request Body field. Click the blue Test button. On the right, a response with Code 200 should appear. Congratulations! You have created and tested an API for calling a Lambda function. Web app create item Browse to API Gateway click on API you choose to use then select Stages in left column, expand Stages and select POST method created previously above. Copy Invoke URL . Paste it in form.js file of the blog project, as follow: .. form . addEventListener ( \"submit\" , async ( event ) => { .. try { if ( formIsValid ( article )) { const json = JSON . stringify ( article ); const response = await fetch ( \"<Invoke URL>\" , { method : \"POST\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, } ); const body = await response . json (); console . log ( body ); } } catch ( e ) { console . error ( \"e: \" , e ); } }); .. Test it locally npm start Browse to localhost:4000 Create an article, save and observe in DynamoDB that it has been created. Test it remotely Push code and observe automatic deployment when you browse to AWS Amplify and click on your application. Browse to aws hosted application Create an article, save and observe in DynamoDB that it has been created.","title":"aws-02-create-item"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#aws-02-create-item","text":"","title":"aws-02-create-item"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#aws-lambda","text":"Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to create item: const AWS = require ( \"aws-sdk\" ); // Instantiate a DynamoDB document client with the SDK let dynamodb = new AWS . DynamoDB . DocumentClient (); // Define handler function, the entry point to our code for the Lambda service // We receive the object that triggers the function as a parameter exports . handler = async ( event , context ) => { // id from context as unique key to record in database let id = context . awsRequestId ; let createdAt = new Date (). toISOString (); // Extract values from event and format as strings let values = JSON . stringify ( `id: ${ id } , createdAt: ${ createdAt } , author: ${ event . author } , img: ${ event . img } , category: ${ event . category } , title: ${ event . title } , content: ${ event . content } ` ); // Create JSON object with parameters for DynamoDB and store in a variable let params = { TableName : \"YOUR-TABLE-NAME\" , Item : { id : id , createdAt : createdAt , author : event . author , img : event . img , category : event . category , title : event . title , content : event . content , }, }; // Using await, make sure object writes to DynamoDB table before continuing execution await dynamodb . put ( params ). promise (); // Create a JSON object with our response and store it in a constant const response = { statusCode : 200 , body : values , }; // Return the response constant return response ; }; Notice that the id is taken from the context ( let id = context.awsRequestId ) as the unique key to record in database. Then click on deploy.","title":"AWS Lambda"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#add-permission","text":"Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] }","title":"Add permission"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#test-function","text":"Click on Test button to configure a test event, provide a name and paste below json: { \"author\" : \"Author 48\" , \"img\" : \"https://randomuser.me/api/portraits/men/48.jpg\" , \"category\" : \"Category 48\" , \"title\" : \"Title 48\" , \"content\" : \"Hello 48 Hello 48 Hello 48 Hello 48\" } To validate test is successful, browse to database to see above element has correctly been created.","title":"Test function"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#amazon-api-gateway","text":"","title":"Amazon API Gateway"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#create-new-rest-api","text":"Log on to Amazon API Gateway Create API - REST API - Build Choose REST protocol Create New API Settings, provide a name, choose Endpoint Type as Edge optimized it is the most suitable option for public services accessed from the Internet","title":"Create new REST API"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#create-new-resource-and-method","text":"In the left pane, click Resources under your API. Select the \u201c/\u201d resource, then click Create Method from the Action drop-down menu. Select POST from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created POST method, then Enable CORS mechanism from the Action drop-down menu. Leave the POST box unchecked and click the blue Enable CORS mechanism and replace existing CORS headers button. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API","title":"Create new resource and method"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#api-deployment","text":"From the Actions drop-down list, select Deploy API. Select [New Stage] from the Deploy Stage drop-down list. Enter dev in the Stage Name field. Select Deploy. Copy the URL next to Invoke URL (you will need this later) and save.","title":"API deployment"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#api-validation","text":"On the left, click Resources. The methods available for the API are displayed on the right. Click POST . Click the blue lightning bolt icon. Paste a JSON that correspond to data format you want to store in your database, into the Request Body field. Click the blue Test button. On the right, a response with Code 200 should appear. Congratulations! You have created and tested an API for calling a Lambda function.","title":"API validation"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#web-app-create-item","text":"Browse to API Gateway click on API you choose to use then select Stages in left column, expand Stages and select POST method created previously above. Copy Invoke URL . Paste it in form.js file of the blog project, as follow: .. form . addEventListener ( \"submit\" , async ( event ) => { .. try { if ( formIsValid ( article )) { const json = JSON . stringify ( article ); const response = await fetch ( \"<Invoke URL>\" , { method : \"POST\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, } ); const body = await response . json (); console . log ( body ); } } catch ( e ) { console . error ( \"e: \" , e ); } }); ..","title":"Web app create item"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#test-it-locally","text":"npm start Browse to localhost:4000 Create an article, save and observe in DynamoDB that it has been created.","title":"Test it locally"},{"location":"javascript/training/16-project-blog/aws-02-create-item/#test-it-remotely","text":"Push code and observe automatic deployment when you browse to AWS Amplify and click on your application. Browse to aws hosted application Create an article, save and observe in DynamoDB that it has been created.","title":"Test it remotely"},{"location":"javascript/training/16-project-blog/aws-03-list-item/","text":"aws-03-list-item AWS Lambda Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to list item: const AWS = require ( \"aws-sdk\" ); const docClient = new AWS . DynamoDB . DocumentClient (); const params = { TableName : \"YOUR-TABLE-NAME\" , }; async function listItems () { try { const data = await docClient . scan ( params ). promise (); return data ; } catch ( err ) { return err ; } } exports . handler = async ( event , context ) => { try { const data = await listItems (); return { body : data }; // return { body: JSON.stringify(data) } } catch ( err ) { return { error : err }; } }; Then click on deploy. Add permission Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] } Test function Click on Test button to configure a test event, provide a name and paste below json: {} Click on Test orange button after having saved the test and as a result you should get database content. Amazon API Gateway Select REST API Log on to Amazon API Gateway Select API created in aws-02-create-item Create new resource and method In the left pane, click Resources under your API. Select the \u201c/\u201d resource, then click Create Method from the Action drop-down menu. Select GET from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created GET method, then Enable CORS mechanism from the Action drop-down menu. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API API deployment From the Actions drop-down list, select Deploy API. Select existing dev stage created in aws-02-create-item from the Deploy Stage drop-down list. Select Deploy. Click on blue Save button. Copy the URL next to Invoke URL (you will need this later) and save. API validation On the left, click Resources. The methods available for the API are displayed on the right. Click GET . Click the blue lightning bolt icon. Click the blue Test button. On the right, a response with Code 200 should appear and Response Body field with data content from db through lambda. Web app list item Browse to API Gateway click on API you choose to use then select Stages in left column, expand Stages and select GET method created previously above. Copy Invoke URL . Paste it in index.js file of the blog project, as follow: .. const fetchArticle = async () => { try { const response = await fetch ( `<Invoke URL>` , { method : \"GET\" , } ); let content = await response . json (); let articles = content . body . Items ; console . log ( articles ); // Standard api behavior return not an array if only one element // so below code convert it (one element) into an array // otherwise it cause an error when 'map' method (to create articles) will be called on it. if ( ! Array . isArray ( articles )) { articles = [ articles ]; } createArticles ( articles ); } catch ( e ) { console . log ( \"e : \" , e ); } }; .. Test it locally npm start Browse to localhost:4000 Create an article, save and observe in main page (index) newly created article and rest of the list as well. Test it remotely Push code and observe automatic deployment when you browse to AWS Amplify and click on your application. Browse to aws hosted application Create an article, save and observe in main page (index) newly created article and rest of the list as well.","title":"aws-03-list-item"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#aws-03-list-item","text":"","title":"aws-03-list-item"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#aws-lambda","text":"Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to list item: const AWS = require ( \"aws-sdk\" ); const docClient = new AWS . DynamoDB . DocumentClient (); const params = { TableName : \"YOUR-TABLE-NAME\" , }; async function listItems () { try { const data = await docClient . scan ( params ). promise (); return data ; } catch ( err ) { return err ; } } exports . handler = async ( event , context ) => { try { const data = await listItems (); return { body : data }; // return { body: JSON.stringify(data) } } catch ( err ) { return { error : err }; } }; Then click on deploy.","title":"AWS Lambda"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#add-permission","text":"Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] }","title":"Add permission"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#test-function","text":"Click on Test button to configure a test event, provide a name and paste below json: {} Click on Test orange button after having saved the test and as a result you should get database content.","title":"Test function"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#amazon-api-gateway","text":"","title":"Amazon API Gateway"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#select-rest-api","text":"Log on to Amazon API Gateway Select API created in aws-02-create-item","title":"Select REST API"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#create-new-resource-and-method","text":"In the left pane, click Resources under your API. Select the \u201c/\u201d resource, then click Create Method from the Action drop-down menu. Select GET from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created GET method, then Enable CORS mechanism from the Action drop-down menu. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API","title":"Create new resource and method"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#api-deployment","text":"From the Actions drop-down list, select Deploy API. Select existing dev stage created in aws-02-create-item from the Deploy Stage drop-down list. Select Deploy. Click on blue Save button. Copy the URL next to Invoke URL (you will need this later) and save.","title":"API deployment"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#api-validation","text":"On the left, click Resources. The methods available for the API are displayed on the right. Click GET . Click the blue lightning bolt icon. Click the blue Test button. On the right, a response with Code 200 should appear and Response Body field with data content from db through lambda.","title":"API validation"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#web-app-list-item","text":"Browse to API Gateway click on API you choose to use then select Stages in left column, expand Stages and select GET method created previously above. Copy Invoke URL . Paste it in index.js file of the blog project, as follow: .. const fetchArticle = async () => { try { const response = await fetch ( `<Invoke URL>` , { method : \"GET\" , } ); let content = await response . json (); let articles = content . body . Items ; console . log ( articles ); // Standard api behavior return not an array if only one element // so below code convert it (one element) into an array // otherwise it cause an error when 'map' method (to create articles) will be called on it. if ( ! Array . isArray ( articles )) { articles = [ articles ]; } createArticles ( articles ); } catch ( e ) { console . log ( \"e : \" , e ); } }; ..","title":"Web app list item"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#test-it-locally","text":"npm start Browse to localhost:4000 Create an article, save and observe in main page (index) newly created article and rest of the list as well.","title":"Test it locally"},{"location":"javascript/training/16-project-blog/aws-03-list-item/#test-it-remotely","text":"Push code and observe automatic deployment when you browse to AWS Amplify and click on your application. Browse to aws hosted application Create an article, save and observe in main page (index) newly created article and rest of the list as well.","title":"Test it remotely"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/","text":"aws-04-delete-item AWS Lambda Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to delete item: const AWS = require ( \"aws-sdk\" ); // Instantiate a DynamoDB document client with the SDK let dynamodb = new AWS . DynamoDB . DocumentClient (); // Define handler function, the entry point to our code for the Lambda service // We receive the object that triggers the function as a parameter exports . handler = async ( event ) => { // Extract values from event and format as strings let values = JSON . stringify ( `id: ${ event . id } ` ); // Create JSON object with parameters for DynamoDB and store in a variable let params = { TableName : \"YOUR-TABLE-NAME\" , Key : { id : event . id , }, }; // Using await, make sure object writes to DynamoDB table before continuing execution await dynamodb . delete ( params ). promise (); // Create a JSON object with our response and store it in a constant const response = { statusCode : 200 , body : values , }; // Return the response constant return response ; }; Then click on deploy. Add permission Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] } Test function DynamoDB, browse to your table and copy an item id to test delete. Lambda, click on Test button to configure a test event, provide a name and paste below json with above copied id in it: { \"id\" : \"<DynamoDB item id>\" } Click on Test orange button after having saved the test and as a result you may get a 200 result code. Observe in your DynamoDB table that item has correctly been deleted. Amazon API Gateway Select REST API Log on to Amazon API Gateway Select API created in aws-02-create-item Create new resource and method In the left pane, click Resources under your API. Select the \u201c/\u201d resource, then click Create Method from the Action drop-down menu. Select DELETE from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created DELETE method, then Enable CORS mechanism from the Action drop-down menu. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API API deployment From the Actions drop-down list, select Deploy API. Select existing dev stage created in aws-02-create-item from the Deploy Stage drop-down list. Select Deploy. Click on blue Save button. Copy the URL next to Invoke URL (you will need this later) and save. API validation On the left, click Resources. The methods available for the API are displayed on the right. Click DELETE . Click the blue lightning bolt icon. In Request Body field paste an id in a JSON the same way you did it above to test function . Click the blue Test button. On the right, a response with Code 200 should appear. Observe in your DynamoDB table that item has correctly been deleted. Web app delete item Browse to API Gateway click on API you choose to use then select Stages in left column, expand Stages and select DELETE method created previously above. Copy Invoke URL . Paste it in index.js file of the blog project, as follow: .. const createArticles = ( articles ) => { const articlesDOM = articles . map (( article ) => { const articleDOM = document . createElement ( \"div\" ); articleDOM . classList . add ( \"article\" ); articleDOM . innerHTML = ` <img src=\" ${ article . img } \" alt=\"profile\" /> <h2> ${ article . title } </h2> <p class=\"article-author\"> ${ article . author } - ${ article . category } </p> <p class=\"article-content\"> ${ article . content } </p> <div class=\"article-actions\"> <button class=\"btn btn-delete\" data-id= ${ article . id } >Delete</button> </div> ` ; return articleDOM ; }); articleContainerElement . innerHTML = \"\" ; articleContainerElement . append (... articlesDOM ); const deleteButtons = articleContainerElement . querySelectorAll ( \".btn-delete\" ); console . log ( deleteButtons ); deleteButtons . forEach (( button ) => { button . addEventListener ( \"click\" , async ( event ) => { try { const target = event . target ; const articleId = target . dataset . id ; const articleToDelete = new Object (); articleToDelete . id = articleId ; const json = JSON . stringify ( articleToDelete ); console . log ( json ); const response = await fetch ( `<Invoke URL>` , { method : \"DELETE\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, } ); const body = await response . json (); console . log ( body ); fetchArticle (); } catch ( e ) { console . log ( \"e: \" , e ); } }); }); }; .. Test it locally npm start Browse to localhost:4000 Delete an article and observe in main page (index) that item has correctly been deleted. Test it remotely Push code and observe automatic deployment when you browse to AWS Amplify and click on your application. Browse to aws hosted application Delete an article and observe in main page (index) that item has correctly been deleted.","title":"aws-04-delete-item"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#aws-04-delete-item","text":"","title":"aws-04-delete-item"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#aws-lambda","text":"Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to delete item: const AWS = require ( \"aws-sdk\" ); // Instantiate a DynamoDB document client with the SDK let dynamodb = new AWS . DynamoDB . DocumentClient (); // Define handler function, the entry point to our code for the Lambda service // We receive the object that triggers the function as a parameter exports . handler = async ( event ) => { // Extract values from event and format as strings let values = JSON . stringify ( `id: ${ event . id } ` ); // Create JSON object with parameters for DynamoDB and store in a variable let params = { TableName : \"YOUR-TABLE-NAME\" , Key : { id : event . id , }, }; // Using await, make sure object writes to DynamoDB table before continuing execution await dynamodb . delete ( params ). promise (); // Create a JSON object with our response and store it in a constant const response = { statusCode : 200 , body : values , }; // Return the response constant return response ; }; Then click on deploy.","title":"AWS Lambda"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#add-permission","text":"Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] }","title":"Add permission"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#test-function","text":"DynamoDB, browse to your table and copy an item id to test delete. Lambda, click on Test button to configure a test event, provide a name and paste below json with above copied id in it: { \"id\" : \"<DynamoDB item id>\" } Click on Test orange button after having saved the test and as a result you may get a 200 result code. Observe in your DynamoDB table that item has correctly been deleted.","title":"Test function"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#amazon-api-gateway","text":"","title":"Amazon API Gateway"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#select-rest-api","text":"Log on to Amazon API Gateway Select API created in aws-02-create-item","title":"Select REST API"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#create-new-resource-and-method","text":"In the left pane, click Resources under your API. Select the \u201c/\u201d resource, then click Create Method from the Action drop-down menu. Select DELETE from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created DELETE method, then Enable CORS mechanism from the Action drop-down menu. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API","title":"Create new resource and method"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#api-deployment","text":"From the Actions drop-down list, select Deploy API. Select existing dev stage created in aws-02-create-item from the Deploy Stage drop-down list. Select Deploy. Click on blue Save button. Copy the URL next to Invoke URL (you will need this later) and save.","title":"API deployment"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#api-validation","text":"On the left, click Resources. The methods available for the API are displayed on the right. Click DELETE . Click the blue lightning bolt icon. In Request Body field paste an id in a JSON the same way you did it above to test function . Click the blue Test button. On the right, a response with Code 200 should appear. Observe in your DynamoDB table that item has correctly been deleted.","title":"API validation"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#web-app-delete-item","text":"Browse to API Gateway click on API you choose to use then select Stages in left column, expand Stages and select DELETE method created previously above. Copy Invoke URL . Paste it in index.js file of the blog project, as follow: .. const createArticles = ( articles ) => { const articlesDOM = articles . map (( article ) => { const articleDOM = document . createElement ( \"div\" ); articleDOM . classList . add ( \"article\" ); articleDOM . innerHTML = ` <img src=\" ${ article . img } \" alt=\"profile\" /> <h2> ${ article . title } </h2> <p class=\"article-author\"> ${ article . author } - ${ article . category } </p> <p class=\"article-content\"> ${ article . content } </p> <div class=\"article-actions\"> <button class=\"btn btn-delete\" data-id= ${ article . id } >Delete</button> </div> ` ; return articleDOM ; }); articleContainerElement . innerHTML = \"\" ; articleContainerElement . append (... articlesDOM ); const deleteButtons = articleContainerElement . querySelectorAll ( \".btn-delete\" ); console . log ( deleteButtons ); deleteButtons . forEach (( button ) => { button . addEventListener ( \"click\" , async ( event ) => { try { const target = event . target ; const articleId = target . dataset . id ; const articleToDelete = new Object (); articleToDelete . id = articleId ; const json = JSON . stringify ( articleToDelete ); console . log ( json ); const response = await fetch ( `<Invoke URL>` , { method : \"DELETE\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, } ); const body = await response . json (); console . log ( body ); fetchArticle (); } catch ( e ) { console . log ( \"e: \" , e ); } }); }); }; ..","title":"Web app delete item"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#test-it-locally","text":"npm start Browse to localhost:4000 Delete an article and observe in main page (index) that item has correctly been deleted.","title":"Test it locally"},{"location":"javascript/training/16-project-blog/aws-04-delete-item/#test-it-remotely","text":"Push code and observe automatic deployment when you browse to AWS Amplify and click on your application. Browse to aws hosted application Delete an article and observe in main page (index) that item has correctly been deleted.","title":"Test it remotely"},{"location":"javascript/training/16-project-blog/debug/","text":"debug Article width Article content width is greater than display. To help finding issue, right click on page and inspect. In Styles window click on + then click on inspector-stylesheet to add style with: * { outline: 1px solid #f00 !important; } Source: - Find element that is causing the showing of horizontal scrollbar in Google Chrome Edit index.scss as follow: ... .content { ... width : 100vw ; .articles-container { ... .article { ... .article-content { max-width : 100 % ; white-space : pre-line ; overflow : auto ; } ... } } } Source: - How to limit max width and height to screen size in CSS","title":"debug"},{"location":"javascript/training/16-project-blog/debug/#debug","text":"","title":"debug"},{"location":"javascript/training/16-project-blog/debug/#article-width","text":"Article content width is greater than display. To help finding issue, right click on page and inspect. In Styles window click on + then click on inspector-stylesheet to add style with: * { outline: 1px solid #f00 !important; } Source: - Find element that is causing the showing of horizontal scrollbar in Google Chrome Edit index.scss as follow: ... .content { ... width : 100vw ; .articles-container { ... .article { ... .article-content { max-width : 100 % ; white-space : pre-line ; overflow : auto ; } ... } } } Source: - How to limit max width and height to screen size in CSS","title":"Article width"},{"location":"javascript/training/16-project-blog/git-branch/","text":"git-branch Description of branches used for git project on github Branches main = Reference branch for aws hosting working implmentation restapi = Reference branch for restapi.fr api (dyma) working implmentation","title":"git-branch"},{"location":"javascript/training/16-project-blog/git-branch/#git-branch","text":"Description of branches used for git project on github","title":"git-branch"},{"location":"javascript/training/16-project-blog/git-branch/#branches","text":"main = Reference branch for aws hosting working implmentation restapi = Reference branch for restapi.fr api (dyma) working implmentation","title":"Branches"},{"location":"javascript/training/19-project-blog-p2/137-introduction/","text":"137-introduction Project blog, part 2, presentation. Dyma.fr Source code on GitHub OLDU Source code on GitHub Goals Goals: Improve article creation (redirection, date, cancel) Article edit Creation of a category menu Filter by category Sort by date with a select list Popup for deletion Redirection In create article form, after click on save or cancel. After save If response code < 299 (means it's ok) redirect to index. form.js: ... form . addEventListener ( \"submit\" , async ( event ) => { ... try { if ( formIsValid ( article )) { ... const response = await fetch ( ... ); if ( response . status < 299 ) { location . assign ( \"/index.html\" ); } } } ... }); ... After cancel Add types to buttons in form.html: ... < form > ... < div class = \"form-btn-container\" > < button type = \"button\" class = \"btn btn-secondary\" > Cancel </ button > < button type = \"submit\" class = \"btn btn-primary\" > Save </ button > </ div > </ form > ... By default, button in a form has submit action. By defining types, only submit button will have submit action, on click. ... Date In article list, display date instead of category. Edit index.js as follow: const createArticles = ( articles ) => { const articlesDOM = articles . map (( article ) => { ... articleDOM . innerHTML = ` ... <p class=\"article-author\"> ${ article . author } - ${ ( new Date ( article . createdAt )). toLocaleDateString ( \"fr-CH\" , { weekday : 'long' , day : '2-digit' , month : 'long' , year : 'numeric' } )}</p> ... ` ; return articleDOM ; }); ... }; data attribute data-* are global attributes to exchange information between HTML and its DOM .","title":"137-introduction"},{"location":"javascript/training/19-project-blog-p2/137-introduction/#137-introduction","text":"Project blog, part 2, presentation. Dyma.fr Source code on GitHub OLDU Source code on GitHub","title":"137-introduction"},{"location":"javascript/training/19-project-blog-p2/137-introduction/#goals","text":"Goals: Improve article creation (redirection, date, cancel) Article edit Creation of a category menu Filter by category Sort by date with a select list Popup for deletion","title":"Goals"},{"location":"javascript/training/19-project-blog-p2/137-introduction/#redirection","text":"In create article form, after click on save or cancel.","title":"Redirection"},{"location":"javascript/training/19-project-blog-p2/137-introduction/#after-save","text":"If response code < 299 (means it's ok) redirect to index. form.js: ... form . addEventListener ( \"submit\" , async ( event ) => { ... try { if ( formIsValid ( article )) { ... const response = await fetch ( ... ); if ( response . status < 299 ) { location . assign ( \"/index.html\" ); } } } ... }); ...","title":"After save"},{"location":"javascript/training/19-project-blog-p2/137-introduction/#after-cancel","text":"Add types to buttons in form.html: ... < form > ... < div class = \"form-btn-container\" > < button type = \"button\" class = \"btn btn-secondary\" > Cancel </ button > < button type = \"submit\" class = \"btn btn-primary\" > Save </ button > </ div > </ form > ... By default, button in a form has submit action. By defining types, only submit button will have submit action, on click. ...","title":"After cancel"},{"location":"javascript/training/19-project-blog-p2/137-introduction/#date","text":"In article list, display date instead of category. Edit index.js as follow: const createArticles = ( articles ) => { const articlesDOM = articles . map (( article ) => { ... articleDOM . innerHTML = ` ... <p class=\"article-author\"> ${ article . author } - ${ ( new Date ( article . createdAt )). toLocaleDateString ( \"fr-CH\" , { weekday : 'long' , day : '2-digit' , month : 'long' , year : 'numeric' } )}</p> ... ` ; return articleDOM ; }); ... };","title":"Date"},{"location":"javascript/training/19-project-blog-p2/137-introduction/#data-attribute","text":"data-* are global attributes to exchange information between HTML and its DOM .","title":"data attribute"},{"location":"javascript/training/19-project-blog-p2/138-edit-article/","text":"138-edit-article The goal is to retrieve the id of the article from the list of articles by calling the form with the id of the article to be able to modify it. index.js Add Edit button with data-id attibute and add listener on each of it. index.js ... const createArticles = ( articles ) => { ... articleDOM . innerHTML = ` ... <div class=\"article-actions\"> <button class=\"btn btn-delete\" data-id= ${ article . id } >Delete</button> <button class=\"btn btn-primary\" data-id= ${ article . id } >Edit</button> </div> ` ; return articleDOM ; }); ... const editButtons = articleContainerElement . querySelectorAll ( \".btn-primary\" ); editButtons . forEach (( button ) => { button . addEventListener ( \"click\" , async ( event ) => { const target = event . target ; const articleId = target . dataset . id ; location . assign ( `/form.html?id= ${ articleId } ` ); }); }); ... }; ... form.js First API call to get an item from article ID passed through query string parameter to backend API end point. Then call PATCH backend API method for updating article in form.js: import { async } from \"regenerator-runtime\" ; import \"../assets/styles/styles.scss\" ; import \"./form.scss\" ; console . log ( \"form.js\" ); const form = document . querySelector ( \"form\" ); const errorElement = document . querySelector ( \"#errors\" ); const btnCancel = document . querySelector ( \".btn-secondary\" ); let articleId ; let errors = []; const fillForm = ( article ) => { const author = document . querySelector ( 'input[name=\"author\"]' ); const img = document . querySelector ( 'input[name=\"img\"]' ); const category = document . querySelector ( 'input[name=\"category\"]' ); const title = document . querySelector ( 'input[name=\"title\"]' ); const content = document . querySelector ( \"textarea\" ); author . value = article . Item . author || \"\" ; img . value = article . Item . img || \"\" ; category . value = article . Item . category || \"\" ; title . value = article . Item . title || \"\" ; content . value = article . Item . content || \"\" ; }; const initForm = async () => { const params = new URL ( location . href ); articleId = params . searchParams . get ( \"id\" ); console . log ( \"articleId: \" , articleId ); if ( articleId ) { const response = await fetch ( `https://<backend url>/dev/article?id= ${ articleId } ` ); if ( response . status < 300 ) { const article = await response . json (); console . log ( \"article\" , article ); fillForm ( article ); } } }; initForm (); btnCancel . addEventListener ( \"click\" , () => { location . assign ( \"/index.html\" ); }); form . addEventListener ( \"submit\" , async ( event ) => { event . preventDefault (); const formData = new FormData ( form ); const article = Object . fromEntries ( formData . entries ()); try { if ( formIsValid ( article )) { const json = JSON . stringify ( article ); let response ; if ( articleId ) { response = await fetch ( `https://<backend url>/dev/article?id= ${ articleId } ` , { method : \"PATCH\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, } ); } else { response = await fetch ( \"https://<backend url>/dev\" , { method : \"POST\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, } ); } if ( response . status < 299 ) { location . assign ( \"/index.html\" ); } } } catch ( e ) { console . error ( \"e: \" , e ); } }); const formIsValid = ( article ) => { errors = []; if ( ! article . author || ! article . category || ! article . content || ! article . img || ! article . title ) { errors . push ( \"All fields must be filled!\" ); } if ( article . content . length < 20 ) { errors . push ( \"Article too short!\" ); } if ( errors . length ) { let errorHtml = \"\" ; errors . forEach (( e ) => { errorHtml += `<li> ${ e } <\\li>` ; }); errorElement . innerHTML = errorHtml ; return false ; } else { errorElement . innerHTML = \"\" ; return true ; } };","title":"138-edit-article"},{"location":"javascript/training/19-project-blog-p2/138-edit-article/#138-edit-article","text":"The goal is to retrieve the id of the article from the list of articles by calling the form with the id of the article to be able to modify it.","title":"138-edit-article"},{"location":"javascript/training/19-project-blog-p2/138-edit-article/#indexjs","text":"Add Edit button with data-id attibute and add listener on each of it. index.js ... const createArticles = ( articles ) => { ... articleDOM . innerHTML = ` ... <div class=\"article-actions\"> <button class=\"btn btn-delete\" data-id= ${ article . id } >Delete</button> <button class=\"btn btn-primary\" data-id= ${ article . id } >Edit</button> </div> ` ; return articleDOM ; }); ... const editButtons = articleContainerElement . querySelectorAll ( \".btn-primary\" ); editButtons . forEach (( button ) => { button . addEventListener ( \"click\" , async ( event ) => { const target = event . target ; const articleId = target . dataset . id ; location . assign ( `/form.html?id= ${ articleId } ` ); }); }); ... }; ...","title":"index.js"},{"location":"javascript/training/19-project-blog-p2/138-edit-article/#formjs","text":"First API call to get an item from article ID passed through query string parameter to backend API end point. Then call PATCH backend API method for updating article in form.js: import { async } from \"regenerator-runtime\" ; import \"../assets/styles/styles.scss\" ; import \"./form.scss\" ; console . log ( \"form.js\" ); const form = document . querySelector ( \"form\" ); const errorElement = document . querySelector ( \"#errors\" ); const btnCancel = document . querySelector ( \".btn-secondary\" ); let articleId ; let errors = []; const fillForm = ( article ) => { const author = document . querySelector ( 'input[name=\"author\"]' ); const img = document . querySelector ( 'input[name=\"img\"]' ); const category = document . querySelector ( 'input[name=\"category\"]' ); const title = document . querySelector ( 'input[name=\"title\"]' ); const content = document . querySelector ( \"textarea\" ); author . value = article . Item . author || \"\" ; img . value = article . Item . img || \"\" ; category . value = article . Item . category || \"\" ; title . value = article . Item . title || \"\" ; content . value = article . Item . content || \"\" ; }; const initForm = async () => { const params = new URL ( location . href ); articleId = params . searchParams . get ( \"id\" ); console . log ( \"articleId: \" , articleId ); if ( articleId ) { const response = await fetch ( `https://<backend url>/dev/article?id= ${ articleId } ` ); if ( response . status < 300 ) { const article = await response . json (); console . log ( \"article\" , article ); fillForm ( article ); } } }; initForm (); btnCancel . addEventListener ( \"click\" , () => { location . assign ( \"/index.html\" ); }); form . addEventListener ( \"submit\" , async ( event ) => { event . preventDefault (); const formData = new FormData ( form ); const article = Object . fromEntries ( formData . entries ()); try { if ( formIsValid ( article )) { const json = JSON . stringify ( article ); let response ; if ( articleId ) { response = await fetch ( `https://<backend url>/dev/article?id= ${ articleId } ` , { method : \"PATCH\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, } ); } else { response = await fetch ( \"https://<backend url>/dev\" , { method : \"POST\" , body : json , headers : { \"Content-Type\" : \"application/json\" , }, } ); } if ( response . status < 299 ) { location . assign ( \"/index.html\" ); } } } catch ( e ) { console . error ( \"e: \" , e ); } }); const formIsValid = ( article ) => { errors = []; if ( ! article . author || ! article . category || ! article . content || ! article . img || ! article . title ) { errors . push ( \"All fields must be filled!\" ); } if ( article . content . length < 20 ) { errors . push ( \"Article too short!\" ); } if ( errors . length ) { let errorHtml = \"\" ; errors . forEach (( e ) => { errorHtml += `<li> ${ e } <\\li>` ; }); errorElement . innerHTML = errorHtml ; return false ; } else { errorElement . innerHTML = \"\" ; return true ; } };","title":"form.js"},{"location":"javascript/training/19-project-blog-p2/139-menu-categories-setup/","text":"139-menu-categories-setup Setup of sidebar menu for articles categories. index.html We start with html structure in file index.html: ... < html lang = \"fr\" > ... < div class = \"content\" > < div class = \"sidebar\" > < ul class = \"categories\" > < li > History ( < strong > 3 </ strong > ) </ li > < li > Space ( < strong > 4 </ strong > ) </ li > < li > Tech ( < strong > 2 </ strong > ) </ li > </ ul > </ div > ... </ div > ... </ html > We add a side bar with which contain a none ordered list. For now it's hard coded to work on CSS but then it will be dynamically displayed by JavaScript . index.scss Adapt style to handle categories in index.scss: ... .content { display : flex ; justify-content : center ; align-items : flex-start ; padding-top : 5rem ; width : 100vw ; .sidebar { flex : 0 0 25rem ; padding : 0 2rem 2rem 2rem ; .categories { padding : 2rem ; background : white ; border-radius : 0 .3rem ; box-shadow : var ( --box-shadow ); li { margin : 1rem ; transition : color 0 .2s ; cursor : pointer ; & :hover { color : var ( --primary ); strong { color : var ( --primary ); } } } } } ... Alignement on cross vertical axis for class content is modified by using align-items: flex-start . Define then sidebar class that to prevent grow or shrink with flex: 0 0 25rem . Finally property flex of articles-container is set to allow grow and shrink depending on the space available. _base.scss Modifying the partial SCSS for color of strong element in file _base.scss: ... strong { color : var ( --accent ); }","title":"139-menu-categories-setup"},{"location":"javascript/training/19-project-blog-p2/139-menu-categories-setup/#139-menu-categories-setup","text":"Setup of sidebar menu for articles categories.","title":"139-menu-categories-setup"},{"location":"javascript/training/19-project-blog-p2/139-menu-categories-setup/#indexhtml","text":"We start with html structure in file index.html: ... < html lang = \"fr\" > ... < div class = \"content\" > < div class = \"sidebar\" > < ul class = \"categories\" > < li > History ( < strong > 3 </ strong > ) </ li > < li > Space ( < strong > 4 </ strong > ) </ li > < li > Tech ( < strong > 2 </ strong > ) </ li > </ ul > </ div > ... </ div > ... </ html > We add a side bar with which contain a none ordered list. For now it's hard coded to work on CSS but then it will be dynamically displayed by JavaScript .","title":"index.html"},{"location":"javascript/training/19-project-blog-p2/139-menu-categories-setup/#indexscss","text":"Adapt style to handle categories in index.scss: ... .content { display : flex ; justify-content : center ; align-items : flex-start ; padding-top : 5rem ; width : 100vw ; .sidebar { flex : 0 0 25rem ; padding : 0 2rem 2rem 2rem ; .categories { padding : 2rem ; background : white ; border-radius : 0 .3rem ; box-shadow : var ( --box-shadow ); li { margin : 1rem ; transition : color 0 .2s ; cursor : pointer ; & :hover { color : var ( --primary ); strong { color : var ( --primary ); } } } } } ... Alignement on cross vertical axis for class content is modified by using align-items: flex-start . Define then sidebar class that to prevent grow or shrink with flex: 0 0 25rem . Finally property flex of articles-container is set to allow grow and shrink depending on the space available.","title":"index.scss"},{"location":"javascript/training/19-project-blog-p2/139-menu-categories-setup/#_basescss","text":"Modifying the partial SCSS for color of strong element in file _base.scss: ... strong { color : var ( --accent ); }","title":"_base.scss"},{"location":"javascript/training/19-project-blog-p2/140-menu-categories-dynamic/","text":"140-menu-categories-dynamic Dynamic categories creation for articles categories sidebar menu. index.html We start by removing html elements that will be dynamically created in file index.html: ... < html lang = \"fr\" > ... < div class = \"content\" > < div class = \"sidebar\" > < ul class = \"categories\" ></ ul > </ div > ... </ div > ... </ html > index.js After fetching articles from server, generate dynamically categories menu content in file index.js: ... const categoriesContainerElement = document . querySelector ( \".categories\" ); ... const displayMenuCategories = ( categoriesArr ) => { const liElements = categoriesArr . map (( categoryElem ) => { const li = document . createElement ( \"li\" ); li . innerHTML = `<li> ${ categoryElem [ 0 ] } ( <strong> ${ categoryElem [ 1 ] } </strong> )</li>` ; return li ; }); categoriesContainerElement . innerHTML = \"\" ; categoriesContainerElement . append (... liElements ); }; const createMenuCategories = ( articles ) => { const categories = articles . reduce (( acc , article ) => { if ( acc [ article . category ]) { acc [ article . category ] ++ ; } else { acc [ article . category ] = 1 ; } return acc ; }, {}); const categoriesArr = Object . keys ( categories ). map (( category ) => { return [ category , categories [ category ]]; }); displayMenuCategories ( categoriesArr ); }; const fetchArticle = async () => { try { ... createMenuCategories ( articles ); } catch ( e ) { ... } }; ... Edit fetchArticle() function to call createMenuCategories() function. createMenuCategories() function get fetched articles and start by transforming them in object with reduce() method. This object keys are categories and values are number of articles by category. Then this object is transformed as an array with with first index, the name of the category and second, the number of articles by category. This array is passed to displayMenuCategories() function which will create a <li> html element for each categories contained in the array. Each <li> html element contain category name and articles number that belong to this category. Finally we append() elements on container with categories class for which we have the reference const categoriesContainerElement .","title":"140-menu-categories-dynamic"},{"location":"javascript/training/19-project-blog-p2/140-menu-categories-dynamic/#140-menu-categories-dynamic","text":"Dynamic categories creation for articles categories sidebar menu.","title":"140-menu-categories-dynamic"},{"location":"javascript/training/19-project-blog-p2/140-menu-categories-dynamic/#indexhtml","text":"We start by removing html elements that will be dynamically created in file index.html: ... < html lang = \"fr\" > ... < div class = \"content\" > < div class = \"sidebar\" > < ul class = \"categories\" ></ ul > </ div > ... </ div > ... </ html >","title":"index.html"},{"location":"javascript/training/19-project-blog-p2/140-menu-categories-dynamic/#indexjs","text":"After fetching articles from server, generate dynamically categories menu content in file index.js: ... const categoriesContainerElement = document . querySelector ( \".categories\" ); ... const displayMenuCategories = ( categoriesArr ) => { const liElements = categoriesArr . map (( categoryElem ) => { const li = document . createElement ( \"li\" ); li . innerHTML = `<li> ${ categoryElem [ 0 ] } ( <strong> ${ categoryElem [ 1 ] } </strong> )</li>` ; return li ; }); categoriesContainerElement . innerHTML = \"\" ; categoriesContainerElement . append (... liElements ); }; const createMenuCategories = ( articles ) => { const categories = articles . reduce (( acc , article ) => { if ( acc [ article . category ]) { acc [ article . category ] ++ ; } else { acc [ article . category ] = 1 ; } return acc ; }, {}); const categoriesArr = Object . keys ( categories ). map (( category ) => { return [ category , categories [ category ]]; }); displayMenuCategories ( categoriesArr ); }; const fetchArticle = async () => { try { ... createMenuCategories ( articles ); } catch ( e ) { ... } }; ... Edit fetchArticle() function to call createMenuCategories() function. createMenuCategories() function get fetched articles and start by transforming them in object with reduce() method. This object keys are categories and values are number of articles by category. Then this object is transformed as an array with with first index, the name of the category and second, the number of articles by category. This array is passed to displayMenuCategories() function which will create a <li> html element for each categories contained in the array. Each <li> html element contain category name and articles number that belong to this category. Finally we append() elements on container with categories class for which we have the reference const categoriesContainerElement .","title":"index.js"},{"location":"javascript/training/19-project-blog-p2/141-category-filter/","text":"141-category-filter Several changes to make the category filter work when clicking on a category. index.js Create an articles global (accessible from everywhere in index.js) variable to store all articles and to not http request each time a filter is selected: ... let articles ; ... Then articles argument when calling createArticles() and createMenuCategories() functions isn't needed any more: ... const createArticles = () => { } ... const createMenuCategories = () => { } ... For the filter, create a filter variable that we simply gonna use with filter() method on the articles array at articles elements creation stage on the DOM: ... let filter ; ... const createArticles = () => { const articlesDOM = articles . filter (( article ) => { if ( filter ) { return article . category === filter ; } else { return true ; } }) ... }; ... If filter is set we return the filtered array by selected category, otherwise we return the entire array with true for each elements. Then we add a listener on each li list element of the categories menu: ... const displayMenuCategories = ( categoriesArr ) => { const liElements = categoriesArr . map (( categoryElem ) => { const li = document . createElement ( \"li\" ); li . innerHTML = `<li> ${ categoryElem [ 0 ] } ( <strong> ${ categoryElem [ 1 ] } </strong> )</li>` ; li . addEventListener ( \"click\" , () => { if ( filter === categoryElem [ 0 ]) { filter = null ; li . classList . remove ( \"active\" ); } else { filter = categoryElem [ 0 ]; liElements . forEach (( li ) => { li . classList . remove ( \"active\" ); }); li . classList . add ( \"active\" ); } createArticles (); }); return li ; }); ... }; ... If a category is selected we check that there's no filter. If this is the case, we check that the filter is the same as the one the user clicked on, and in this case we remove the filter and active class. Else, filter si defined as the clicked category and we remove active class to all categories, and afterwards we apply active class to selected category. Finally, we sort categories alphabetically when creating the menu: ... const createMenuCategories = () => { ... const categoriesArr = Object . keys ( categories ) . map (( category ) => { return [ category , categories [ category ]]; }) . sort (); ... }; ... index.scss We simply add active class for selected filter: ... .content { ... .sidebar { ... .categories { ... li { ... } .active { color : var ( --primary ); font-weight : 700 ; strong { color : var ( --primary ); } } } } ... } ...","title":"141-category-filter"},{"location":"javascript/training/19-project-blog-p2/141-category-filter/#141-category-filter","text":"Several changes to make the category filter work when clicking on a category.","title":"141-category-filter"},{"location":"javascript/training/19-project-blog-p2/141-category-filter/#indexjs","text":"Create an articles global (accessible from everywhere in index.js) variable to store all articles and to not http request each time a filter is selected: ... let articles ; ... Then articles argument when calling createArticles() and createMenuCategories() functions isn't needed any more: ... const createArticles = () => { } ... const createMenuCategories = () => { } ... For the filter, create a filter variable that we simply gonna use with filter() method on the articles array at articles elements creation stage on the DOM: ... let filter ; ... const createArticles = () => { const articlesDOM = articles . filter (( article ) => { if ( filter ) { return article . category === filter ; } else { return true ; } }) ... }; ... If filter is set we return the filtered array by selected category, otherwise we return the entire array with true for each elements. Then we add a listener on each li list element of the categories menu: ... const displayMenuCategories = ( categoriesArr ) => { const liElements = categoriesArr . map (( categoryElem ) => { const li = document . createElement ( \"li\" ); li . innerHTML = `<li> ${ categoryElem [ 0 ] } ( <strong> ${ categoryElem [ 1 ] } </strong> )</li>` ; li . addEventListener ( \"click\" , () => { if ( filter === categoryElem [ 0 ]) { filter = null ; li . classList . remove ( \"active\" ); } else { filter = categoryElem [ 0 ]; liElements . forEach (( li ) => { li . classList . remove ( \"active\" ); }); li . classList . add ( \"active\" ); } createArticles (); }); return li ; }); ... }; ... If a category is selected we check that there's no filter. If this is the case, we check that the filter is the same as the one the user clicked on, and in this case we remove the filter and active class. Else, filter si defined as the clicked category and we remove active class to all categories, and afterwards we apply active class to selected category. Finally, we sort categories alphabetically when creating the menu: ... const createMenuCategories = () => { ... const categoriesArr = Object . keys ( categories ) . map (( category ) => { return [ category , categories [ category ]]; }) . sort (); ... }; ...","title":"index.js"},{"location":"javascript/training/19-project-blog-p2/141-category-filter/#indexscss","text":"We simply add active class for selected filter: ... .content { ... .sidebar { ... .categories { ... li { ... } .active { color : var ( --primary ); font-weight : 700 ; strong { color : var ( --primary ); } } } } ... } ...","title":"index.scss"},{"location":"javascript/training/19-project-blog-p2/aws-05-get-item/","text":"aws-05-get-item AWS Lambda Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to get item: const AWS = require ( \"aws-sdk\" ); // Instantiate a DynamoDB document client with the SDK let dynamodb = new AWS . DynamoDB . DocumentClient (); // Define handler function, the entry point to our code for the Lambda service // We receive the object that triggers the function as a parameter exports . handler = async ( event ) => { // Create JSON object with parameters for DynamoDB and store in a variable let params = { TableName : \"YOUR-TABLE-NAME\" , Key : { id : event . queryStringParameters . id , }, }; // Using await, make sure object reads from DynamoDB table before continuing execution const data = await dynamodb . get ( params ). promise (); // Create a JSON object with our response and store it in a constant const response = { statusCode : 200 , headers : { \"Access-Control-Allow-Headers\" : \"Content-Type\" , \"Access-Control-Allow-Origin\" : \"https://<amplify app url> or <localhost:4000>\" , \"Access-Control-Allow-Methods\" : \"OPTIONS,POST,GET\" , }, body : JSON . stringify ( data ), }; // Return the response constant return response ; }; Then click on deploy. Add permission Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] } Amazon API Gateway Select REST API Log on to Amazon API Gateway Select API created in aws-02-create-item Create new resource and method In the left pane, click Resources under your API. Select the \u201c/\u201d resource, then click Create Resource from the Action drop-down menu, to create a new resource called article . Select the \u201c/article\u201d resource, then click Create Method from the Action drop-down menu. Select GET from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Check Use Lambda Proxy integration to allow query string parameter from header in lambda (article id). Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created GET method, then Enable CORS mechanism from the Action drop-down menu. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API Edit Method Request to add id in URL Query String Parameters section as Required , you may also check Request Validator in header section of this form. API deployment From the Actions drop-down list, select Deploy API. Select existing dev stage created in aws-02-create-item from the Deploy Stage drop-down list. Select Deploy. Click on blue Save button. Copy the URL next to Invoke URL (you will need this later) and save. Web app edit item This part (get item) is the first of two parts (get + update) in the edit process. Application to edit, described in 138-edit-article is aws ready.","title":"aws-05-get-item"},{"location":"javascript/training/19-project-blog-p2/aws-05-get-item/#aws-05-get-item","text":"","title":"aws-05-get-item"},{"location":"javascript/training/19-project-blog-p2/aws-05-get-item/#aws-lambda","text":"Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to get item: const AWS = require ( \"aws-sdk\" ); // Instantiate a DynamoDB document client with the SDK let dynamodb = new AWS . DynamoDB . DocumentClient (); // Define handler function, the entry point to our code for the Lambda service // We receive the object that triggers the function as a parameter exports . handler = async ( event ) => { // Create JSON object with parameters for DynamoDB and store in a variable let params = { TableName : \"YOUR-TABLE-NAME\" , Key : { id : event . queryStringParameters . id , }, }; // Using await, make sure object reads from DynamoDB table before continuing execution const data = await dynamodb . get ( params ). promise (); // Create a JSON object with our response and store it in a constant const response = { statusCode : 200 , headers : { \"Access-Control-Allow-Headers\" : \"Content-Type\" , \"Access-Control-Allow-Origin\" : \"https://<amplify app url> or <localhost:4000>\" , \"Access-Control-Allow-Methods\" : \"OPTIONS,POST,GET\" , }, body : JSON . stringify ( data ), }; // Return the response constant return response ; }; Then click on deploy.","title":"AWS Lambda"},{"location":"javascript/training/19-project-blog-p2/aws-05-get-item/#add-permission","text":"Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] }","title":"Add permission"},{"location":"javascript/training/19-project-blog-p2/aws-05-get-item/#amazon-api-gateway","text":"","title":"Amazon API Gateway"},{"location":"javascript/training/19-project-blog-p2/aws-05-get-item/#select-rest-api","text":"Log on to Amazon API Gateway Select API created in aws-02-create-item","title":"Select REST API"},{"location":"javascript/training/19-project-blog-p2/aws-05-get-item/#create-new-resource-and-method","text":"In the left pane, click Resources under your API. Select the \u201c/\u201d resource, then click Create Resource from the Action drop-down menu, to create a new resource called article . Select the \u201c/article\u201d resource, then click Create Method from the Action drop-down menu. Select GET from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Check Use Lambda Proxy integration to allow query string parameter from header in lambda (article id). Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created GET method, then Enable CORS mechanism from the Action drop-down menu. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API Edit Method Request to add id in URL Query String Parameters section as Required , you may also check Request Validator in header section of this form.","title":"Create new resource and method"},{"location":"javascript/training/19-project-blog-p2/aws-05-get-item/#api-deployment","text":"From the Actions drop-down list, select Deploy API. Select existing dev stage created in aws-02-create-item from the Deploy Stage drop-down list. Select Deploy. Click on blue Save button. Copy the URL next to Invoke URL (you will need this later) and save.","title":"API deployment"},{"location":"javascript/training/19-project-blog-p2/aws-05-get-item/#web-app-edit-item","text":"This part (get item) is the first of two parts (get + update) in the edit process. Application to edit, described in 138-edit-article is aws ready.","title":"Web app edit item"},{"location":"javascript/training/19-project-blog-p2/aws-06-update-item/","text":"aws-06-update-item AWS Lambda Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to get item: const AWS = require ( \"aws-sdk\" ); let dynamodb = new AWS . DynamoDB . DocumentClient (); exports . handler = async ( event ) => { let item = JSON . parse ( event . body ); let paramsAuthor = { TableName : \"YOUR-TABLE-NAME\" , Key : { id : event . queryStringParameters . id , }, UpdateExpression : `set author = :author` , ExpressionAttributeValues : { \":author\" : item . author , }, }; let paramsContent = { TableName : \"YOUR-TABLE-NAME\" , Key : { id : event . queryStringParameters . id , }, UpdateExpression : `set content = :content` , ExpressionAttributeValues : { \":content\" : item . content , }, }; // Using await, make sure object updates to DynamoDB table before continuing execution await dynamodb . update ( paramsAuthor ). promise (); await dynamodb . update ( paramsContent ). promise (); const response = { statusCode : 200 , headers : { \"Access-Control-Allow-Headers\" : \"Content-Type\" , \"Access-Control-Allow-Origin\" : \"https://<amplify app url> or <localhost:4000>\" , \"Access-Control-Allow-Methods\" : \"OPTIONS,POST,GET,PATCH\" , }, body : JSON . stringify ( event . body ), isBase64Encoded : false , }; return response ; }; Then click on deploy. Add permission Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] } Amazon API Gateway Select REST API Log on to Amazon API Gateway Select API created in aws-02-create-item Create new resource and method In the left pane, click Resources under your API. Select the \u201c/article\u201d resource, then click Create Method from the Action drop-down menu. Select PATCH from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Check Use Lambda Proxy integration to allow query string parameter from header in lambda (article id). Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created PATCH method, then Enable CORS mechanism from the Action drop-down menu. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API Edit Method Request to add id in URL Query String Parameters section as Required , you may also check Request Validator in header section of this form. API deployment From the Actions drop-down list, select Deploy API. Select existing dev stage created in aws-02-create-item from the Deploy Stage drop-down list. Select Deploy. Click on blue Save button. Copy the URL next to Invoke URL (you will need this later) and save. Web app update item This part (update item) is the second of two parts (get + update) in the edit process. Application to edit, described in 138-edit-article is aws ready.","title":"aws-06-update-item"},{"location":"javascript/training/19-project-blog-p2/aws-06-update-item/#aws-06-update-item","text":"","title":"aws-06-update-item"},{"location":"javascript/training/19-project-blog-p2/aws-06-update-item/#aws-lambda","text":"Log on to AWS Lambda console Create function - Author from scratch provide a name, choose runtime Node.js 16.x , architecture x86_64 then click on Create function JS code (paste in index.js) of the function to get item: const AWS = require ( \"aws-sdk\" ); let dynamodb = new AWS . DynamoDB . DocumentClient (); exports . handler = async ( event ) => { let item = JSON . parse ( event . body ); let paramsAuthor = { TableName : \"YOUR-TABLE-NAME\" , Key : { id : event . queryStringParameters . id , }, UpdateExpression : `set author = :author` , ExpressionAttributeValues : { \":author\" : item . author , }, }; let paramsContent = { TableName : \"YOUR-TABLE-NAME\" , Key : { id : event . queryStringParameters . id , }, UpdateExpression : `set content = :content` , ExpressionAttributeValues : { \":content\" : item . content , }, }; // Using await, make sure object updates to DynamoDB table before continuing execution await dynamodb . update ( paramsAuthor ). promise (); await dynamodb . update ( paramsContent ). promise (); const response = { statusCode : 200 , headers : { \"Access-Control-Allow-Headers\" : \"Content-Type\" , \"Access-Control-Allow-Origin\" : \"https://<amplify app url> or <localhost:4000>\" , \"Access-Control-Allow-Methods\" : \"OPTIONS,POST,GET,PATCH\" , }, body : JSON . stringify ( event . body ), isBase64Encoded : false , }; return response ; }; Then click on deploy.","title":"AWS Lambda"},{"location":"javascript/training/19-project-blog-p2/aws-06-update-item/#add-permission","text":"Using AWS Identity and Access Management (IAM), add permissions to your function so that it can use the DynamoDB service. Copy DynamoDB table ARN . Steps: Click the Configuration - Permissions tab. In the Execution Role field, click the appropriate role. A new tab opens in your browser. Click Add inline policy to the right of Authorization policies and select the JSON tab. Copy below json into the text box, being careful to replace your table's ARN in the Resource field, line 15: This permission allows your Lambda function to read, modify, or delete items, but only in the table you created. Click the Review Policy blue button. Next to Name, type policy name. Click the Create Policy blue button. You can now close this tab and return to the one dedicated to your Lambda function. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"dynamodb:PutItem\" , \"dynamodb:DeleteItem\" , \"dynamodb:GetItem\" , \"dynamodb:Scan\" , \"dynamodb:Query\" , \"dynamodb:UpdateItem\" ], \"Resource\" : \"YOUR-TABLE-ARN\" } ] }","title":"Add permission"},{"location":"javascript/training/19-project-blog-p2/aws-06-update-item/#amazon-api-gateway","text":"","title":"Amazon API Gateway"},{"location":"javascript/training/19-project-blog-p2/aws-06-update-item/#select-rest-api","text":"Log on to Amazon API Gateway Select API created in aws-02-create-item","title":"Select REST API"},{"location":"javascript/training/19-project-blog-p2/aws-06-update-item/#create-new-resource-and-method","text":"In the left pane, click Resources under your API. Select the \u201c/article\u201d resource, then click Create Method from the Action drop-down menu. Select PATCH from the drop-down menu that appears, then click the check mark. Select Lambda Function as the integration type. Check Use Lambda Proxy integration to allow query string parameter from header in lambda (article id). Enter Function Name in the Function field. Click the blue Save button. You should see a message telling you that you allow the API being created to call your Lambda function. Click the OK button. Select the newly created PATCH method, then Enable CORS mechanism from the Action drop-down menu. A message asking you to confirm the changes made to the method should appear. Click the blue Yes, Replace Existing Values button. To allow only specific IP addresses to access your API Gateway REST API Edit Method Request to add id in URL Query String Parameters section as Required , you may also check Request Validator in header section of this form.","title":"Create new resource and method"},{"location":"javascript/training/19-project-blog-p2/aws-06-update-item/#api-deployment","text":"From the Actions drop-down list, select Deploy API. Select existing dev stage created in aws-02-create-item from the Deploy Stage drop-down list. Select Deploy. Click on blue Save button. Copy the URL next to Invoke URL (you will need this later) and save.","title":"API deployment"},{"location":"javascript/training/19-project-blog-p2/aws-06-update-item/#web-app-update-item","text":"This part (update item) is the second of two parts (get + update) in the edit process. Application to edit, described in 138-edit-article is aws ready.","title":"Web app update item"},{"location":"jmeter/1-jmeter-misc/","text":"JMeter - 01 - Misc Constant number of requests by second SRC When launching a test, in up right corner, check for number of active units. Limit at 100 messages / second for a total of 900k messages set the \"Ramp Up\" - time taken to bring up all the threads (Dur\u00e9e de mont\u00e9e en charge (s)\" in french) to 90000.","title":"JMeter - 01 - Misc"},{"location":"jmeter/1-jmeter-misc/#jmeter-01-misc","text":"","title":"JMeter - 01 - Misc"},{"location":"jmeter/1-jmeter-misc/#constant-number-of-requests-by-second","text":"SRC When launching a test, in up right corner, check for number of active units. Limit at 100 messages / second for a total of 900k messages set the \"Ramp Up\" - time taken to bring up all the threads (Dur\u00e9e de mont\u00e9e en charge (s)\" in french) to 90000.","title":"Constant number of requests by second"},{"location":"kafka/1-kafka-misc/","text":"Kafka - 01 - Misc Listing Kafka Topics Source In a Docker Kafka Container CLI. Listing Topics kafka-topics.sh --list --zookeeper localhost:2181 kafka-topics.sh --list --zookeeper <zookeeper_container_name_in_docker_stack_network>:2181 kafka-topics.sh --list --zookeeper zookeeper-3.5.7:2181 Topic Details kafka-topics.sh --bootstrap-server=localhost:9092 --describe --topic <topic_name> View message How to view kafka message kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <topic name> (--from-beginning)","title":"Kafka - 01 - Misc"},{"location":"kafka/1-kafka-misc/#kafka-01-misc","text":"","title":"Kafka - 01 - Misc"},{"location":"kafka/1-kafka-misc/#listing-kafka-topics","text":"Source In a Docker Kafka Container CLI.","title":"Listing Kafka Topics"},{"location":"kafka/1-kafka-misc/#listing-topics","text":"kafka-topics.sh --list --zookeeper localhost:2181 kafka-topics.sh --list --zookeeper <zookeeper_container_name_in_docker_stack_network>:2181 kafka-topics.sh --list --zookeeper zookeeper-3.5.7:2181","title":"Listing Topics"},{"location":"kafka/1-kafka-misc/#topic-details","text":"kafka-topics.sh --bootstrap-server=localhost:9092 --describe --topic <topic_name>","title":"Topic Details"},{"location":"kafka/1-kafka-misc/#view-message","text":"How to view kafka message kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <topic name> (--from-beginning)","title":"View message"},{"location":"linux/0-linux-debug/","text":"Linux - 00 - Debug Toolbox 20 one-line Linux commands to add to your toolbox df // report file system disk space usage free // Display amount of free and used memory in the system top // display Linux processes netstat // Print network connections (sudo) netstat -tunlp // opened port cat /var/log/run.log systemctl status metricbeat // Control the systemd system and service manager journalctl -f -u metricbeat // Query the systemd journal Quick live log monitor Monitor E.g. for \"success\" pattern presence in log file. 1st terminal watch (120s) grep 'success' in 'file.log' and output in 'logsuccess' file: watch -t -n 120 '({ date; echo \"-\"; cat file.log | grep success | wc -l; } | tr \"\\n\" \" \" ; printf \"\\n\") | tee -a logsuccess' When done, remove 'logsuccess' file: rm logsuccess 2nd terminal tailed watch (120s) of 'logsuccess' file: watch -n 120 -d 'cat logsuccess | tail -n $(($LINES - 2))' Monitor of monitor Prerequisite, PID of current bash session: echo $$ Monitor of \"success\" monitor 3rd terminal live top PID: top -p <pid1>,<pid2> Monitor of Monitor of \"success\" monitor 4th terminal log top PID: top | grep -E '<pid1>|<pid2>' Monitor of Monitor of Monitor of \"success\" monitor 5th terminal live top processes resource consumption: top | grep top Log repartition To see repartition in log file of a matching pattern less + /<searched pattern> Terminal ring For long command, terminal ring when finished: <long command>; echo -e \"\\a\" Grep binary file Grep binary file, or supposed to be (some text files are considered as): grep -a <pattern> file Add -a argument to 'grep' command for processing a binary file as if it were text. Log between time range For logs like in file.log: 185.98.28.113: - - [16/Jun/2022:07:30:00 +0000] \"GET /... Logs from 07:30 to 07:40: grep -E '(16/Jun/2022:07:3)' file.log URL by time Statisitic of requested URL by time (round to minutes), for logs like in file.log: 82.146.192.163:- - - [03/Nov/2021:06:25:21 +0000] \"GET /base/services/report/type_2/data... Command, for timestamp catch pattern from first '[' until last ':' (trunc seconds), then catch type of report in URL, from 'report/' until none of [a-zA-Z0-9_.], hoping it is a '/': cat file.log | sed -n 's/^.*\\[\\(.*\\):[^:].*\\/base\\/services\\/report\\/\\([a-zA-Z0-9_.]*\\)\\(.*\\)/\\1 -> \\2/p' | sort | uniq -c | sort -n -r | head -20 Output: 5 31/May/2022:07:36 -> type_1 5 31/May/2022:07:24 -> type_2 5 31/May/2022:07:18 -> type_2 5 31/May/2022:06:20 -> type_2 5 31/May/2022:05:21 -> type_2 5 31/May/2022:04:55 -> type_2 4 31/May/2022:20:20 -> type_2 4 31/May/2022:18:41 -> type_2 4 31/May/2022:12:45 -> type_3 4 31/May/2022:12:42 -> type_2 4 31/May/2022:12:35 -> type_3 4 31/May/2022:11:57 -> type_2 List of requested base URL file.log: 82.146.192.163:- - - [03/Nov/2021:06:25:21 +0000] \"GET /base/services/report/type_2/data... Command: grep -a -o -P '(?<=\\/base\\/services\\/report\\/).*' file.log | cut -d/ -f1 | sort | uniq Output: type_1 type_2 type_3 Count by minute of base URL file.log: 82.146.192.163:- - - [03/Nov/2021:06:25:21 +0000] \"GET /base/services/report/type_2/data... Command: grep -a /base/services/report/ file.log | grep -oP '\\[.*\\]' | cut -c2- | rev | cut -c11- | rev | sort | uniq -c | sort -n -r | head Output: 11 31/May/2022:12:48 07 31/May/2022:12:45 06 31/May/2022:12:44 05 31/May/2022:12:42 05 31/May/2022:12:35 05 31/May/2022:11:57 05 31/May/2022:07:36 05 31/May/2022:07:24 05 31/May/2022:07:18 05 31/May/2022:06:20 Monitor live file activity Monitor live file activity (sorted by size) in a folder: watch -d 'ls -laS' Filter java stack trace in log Quick Remove \"at\" lines: grep -v '^.*at' debug.log | less Long Remove other line that contain (recurring) pattern: cat debug.log | grep -v -E '^.*at|pattern1|pattern2|pattern3' | less tail in WSL (2) By default tail in WSL on a Windows handled file does not follow. To make it works: tail -f ---disable-inotify file","title":"Linux - 00 - Debug"},{"location":"linux/0-linux-debug/#linux-00-debug","text":"","title":"Linux - 00 - Debug"},{"location":"linux/0-linux-debug/#toolbox","text":"20 one-line Linux commands to add to your toolbox df // report file system disk space usage free // Display amount of free and used memory in the system top // display Linux processes netstat // Print network connections (sudo) netstat -tunlp // opened port cat /var/log/run.log systemctl status metricbeat // Control the systemd system and service manager journalctl -f -u metricbeat // Query the systemd journal","title":"Toolbox"},{"location":"linux/0-linux-debug/#quick-live-log-monitor","text":"","title":"Quick live log monitor"},{"location":"linux/0-linux-debug/#monitor","text":"E.g. for \"success\" pattern presence in log file. 1st terminal watch (120s) grep 'success' in 'file.log' and output in 'logsuccess' file: watch -t -n 120 '({ date; echo \"-\"; cat file.log | grep success | wc -l; } | tr \"\\n\" \" \" ; printf \"\\n\") | tee -a logsuccess' When done, remove 'logsuccess' file: rm logsuccess 2nd terminal tailed watch (120s) of 'logsuccess' file: watch -n 120 -d 'cat logsuccess | tail -n $(($LINES - 2))'","title":"Monitor"},{"location":"linux/0-linux-debug/#monitor-of-monitor","text":"Prerequisite, PID of current bash session: echo $$","title":"Monitor of monitor"},{"location":"linux/0-linux-debug/#monitor-of-success-monitor","text":"3rd terminal live top PID: top -p <pid1>,<pid2>","title":"Monitor of \"success\" monitor"},{"location":"linux/0-linux-debug/#monitor-of-monitor-of-success-monitor","text":"4th terminal log top PID: top | grep -E '<pid1>|<pid2>'","title":"Monitor of Monitor of \"success\" monitor"},{"location":"linux/0-linux-debug/#monitor-of-monitor-of-monitor-of-success-monitor","text":"5th terminal live top processes resource consumption: top | grep top","title":"Monitor of Monitor of Monitor of \"success\" monitor"},{"location":"linux/0-linux-debug/#log-repartition","text":"To see repartition in log file of a matching pattern less + /<searched pattern>","title":"Log repartition"},{"location":"linux/0-linux-debug/#terminal-ring","text":"For long command, terminal ring when finished: <long command>; echo -e \"\\a\"","title":"Terminal ring"},{"location":"linux/0-linux-debug/#grep-binary-file","text":"Grep binary file, or supposed to be (some text files are considered as): grep -a <pattern> file Add -a argument to 'grep' command for processing a binary file as if it were text.","title":"Grep binary file"},{"location":"linux/0-linux-debug/#log-between-time-range","text":"For logs like in file.log: 185.98.28.113: - - [16/Jun/2022:07:30:00 +0000] \"GET /... Logs from 07:30 to 07:40: grep -E '(16/Jun/2022:07:3)' file.log","title":"Log between time range"},{"location":"linux/0-linux-debug/#url-by-time","text":"Statisitic of requested URL by time (round to minutes), for logs like in file.log: 82.146.192.163:- - - [03/Nov/2021:06:25:21 +0000] \"GET /base/services/report/type_2/data... Command, for timestamp catch pattern from first '[' until last ':' (trunc seconds), then catch type of report in URL, from 'report/' until none of [a-zA-Z0-9_.], hoping it is a '/': cat file.log | sed -n 's/^.*\\[\\(.*\\):[^:].*\\/base\\/services\\/report\\/\\([a-zA-Z0-9_.]*\\)\\(.*\\)/\\1 -> \\2/p' | sort | uniq -c | sort -n -r | head -20 Output: 5 31/May/2022:07:36 -> type_1 5 31/May/2022:07:24 -> type_2 5 31/May/2022:07:18 -> type_2 5 31/May/2022:06:20 -> type_2 5 31/May/2022:05:21 -> type_2 5 31/May/2022:04:55 -> type_2 4 31/May/2022:20:20 -> type_2 4 31/May/2022:18:41 -> type_2 4 31/May/2022:12:45 -> type_3 4 31/May/2022:12:42 -> type_2 4 31/May/2022:12:35 -> type_3 4 31/May/2022:11:57 -> type_2","title":"URL by time"},{"location":"linux/0-linux-debug/#list-of-requested-base-url","text":"file.log: 82.146.192.163:- - - [03/Nov/2021:06:25:21 +0000] \"GET /base/services/report/type_2/data... Command: grep -a -o -P '(?<=\\/base\\/services\\/report\\/).*' file.log | cut -d/ -f1 | sort | uniq Output: type_1 type_2 type_3","title":"List of requested base URL"},{"location":"linux/0-linux-debug/#count-by-minute-of-base-url","text":"file.log: 82.146.192.163:- - - [03/Nov/2021:06:25:21 +0000] \"GET /base/services/report/type_2/data... Command: grep -a /base/services/report/ file.log | grep -oP '\\[.*\\]' | cut -c2- | rev | cut -c11- | rev | sort | uniq -c | sort -n -r | head Output: 11 31/May/2022:12:48 07 31/May/2022:12:45 06 31/May/2022:12:44 05 31/May/2022:12:42 05 31/May/2022:12:35 05 31/May/2022:11:57 05 31/May/2022:07:36 05 31/May/2022:07:24 05 31/May/2022:07:18 05 31/May/2022:06:20","title":"Count by minute of base URL"},{"location":"linux/0-linux-debug/#monitor-live-file-activity","text":"Monitor live file activity (sorted by size) in a folder: watch -d 'ls -laS'","title":"Monitor live file activity"},{"location":"linux/0-linux-debug/#filter-java-stack-trace-in-log","text":"","title":"Filter java stack trace in log"},{"location":"linux/0-linux-debug/#quick","text":"Remove \"at\" lines: grep -v '^.*at' debug.log | less","title":"Quick"},{"location":"linux/0-linux-debug/#long","text":"Remove other line that contain (recurring) pattern: cat debug.log | grep -v -E '^.*at|pattern1|pattern2|pattern3' | less","title":"Long"},{"location":"linux/0-linux-debug/#tail-in-wsl-2","text":"By default tail in WSL on a Windows handled file does not follow. To make it works: tail -f ---disable-inotify file","title":"tail in WSL (2)"},{"location":"linux/1-linux-misc/","text":"Linux - 01 - Misc Curl curl -i localhost curl -sb -H \"Accept: application/json\" \"http://localhost\" | json_pp Compress Decompress Compress (czf) - c compress, -z zip, -f file tar -czf /targetfolder/targetfile.tar.gz /sourcefolder Decompress (xzf) - x extract, -z zip, -f file tar -xzf targetfile.tar.gz or gzip, -k to keep original, -d to decompress: gzip -kd file.gz Package List installed apt list --installed Info apt-cache show packagename Remove sudo apt-get --purge autoremove packagename Switch user to root Switch current user to root sudo su - Grep lines before after match -B before -A after Stick ne lines just after option grep -B2 -A3 pattern infile.txt Copy files from list Copy specific files from a text list of files rsync -a sourcefolder --files-from=list.txt destinationfolder Get data between two patterns In error.log . . 05:59:30.024 [nioEventLoopGroup-3-5] ERROR c.l.d.c.ConnectorServerHandlerTCP.parseAndSendMessageTeltonika(177) - class java.util.concurrent.ExecutionException FOR RAW DATA : 0000017d55747fb00003f5380c1be045ce00000000000000000804ef005000c8024503034230fc430f8d440000011007e0fca600 WITH STACKTRACE : {} . . Get data between \"DATA : \" and \" WITH\" (hex raw data) cat error.log | sed -nr 's/.*DATA : (.*) WITH.*/\\1/p' or cat file | grep -o -P '(?<=left).*(?=right)' Grep patterns from a file -f option, maybe -oF options also grep -f patterns_file *.log or grep -oFf patterns.txt *.log If result's count's not OK, check by not found pattern (-h option to hide filename in output) grep -hoFf patterns.txt *.log | grep -vFf - patterns.txt For list of file For list of file in current folder, do operation. Here we want to have file name, cat content and separate result with a new line ll *.txt 1.txt 2.txt 3.txt 4.txt 5.txt for f in {2..4}.txt; do echo \"$f\"; cat \"$f\"; printf \"\\n\"; done 2.txt jkl mno pqr 3.txt stu vwx yza 4.txt bcd efg hij Search between timestamp sed -rne '/10:50/,/11:05/ p' file Put existing time range in file (10:50 - 11:05). Highlight search result grep --color=always -z pattern file | less -R always to transmit color through pipe -z to show everything, not only the matching pattern -R to avoid showing esc char instead of color Delete history history 1003 25-04-2016 17:54:52 echo \"Command 1\" 1004 25-04-2016 17:54:54 echo \"Command 2\" 1005 25-04-2016 17:54:57 echo \"Command 3\" 1006 25-04-2016 17:54:59 echo \"Command 4\" 1007 25-04-2016 17:55:01 echo \"Command 5\" 1008 25-04-2016 17:55:03 echo \"Command 6\" 1009 25-04-2016 17:55:07 echo \"Command 7\" 1010 25-04-2016 17:55:09 echo \"Command 8\" 1011 25-04-2016 17:55:11 echo \"Command 9\" 1012 25-04-2016 17:55:14 echo \"Command 10\" for h in $(seq 1006 1008); do history -d 1006; done Where is a program How to know where reside a program, e.g. ls? which ls /usr/bin/ls env env | grep PATH Difference of cmd output diff <(ls test1) <(ls test2) Difference of sorted lists sort ok.txt > okSorted.txt sort all.txt > allSorted.txt diff --new-line-format=\"\" --unchanged-line-format=\"\" allSorted.txt okSorted.txt Column Min cat ... | grep ... | sort -n -r | tail -n10 Max cat ... | grep ... | sort -n -r | head -n10 Average test.txt (warning on empty lines (maybe at the end)) 1 3 7 cat test.txt | awk '{ total += $1 } END { print total/NR }' 3.66667 Median test.txt (warning on empty lines (maybe at the end)) 1 3 7 11 22 45 71 median.awk #/usr/bin/env awk { count[NR] = $1; } END { if (NR % 2) { print count[(NR + 1) / 2]; } else { print (count[(NR / 2)] + count[(NR / 2) + 1]) / 2.0; } } cat test.txt | awk -f median.awk 11 Check equal number of values below and above median cat test.txt | awk '{if($1 < 11) print $1}' | wc -l 3 cat test.txt | awk '{if($1 > 11) print $1}' | wc -l 3 Get nth column from file Get nth column from file with field separated values test.txt column 1 row 1;column 2 row 1;column 3 row 1 column 1 row 2;column 2 row 2;column 3 row 2 column 1 row 3;column 2 row 3;column 3 row 3 cat test.txt | awk -F ';' '{print $2}' column 2 row 1 column 2 row 2 column 2 row 3 Conditional cat test.txt | awk -F ';' '{if($1 == \"column 1 row 2\") print $2}' column 2 row 2 List files Sorted by sizes and human readable ll -S -h Uniq values in a file Uniq values in a file, sorted and counted (first sort is mandatory) test.txt 1 71 3 7 22 1 11 22 45 71 22 cat test.txt | sort | uniq -c 2 1 1 11 3 22 1 3 1 45 1 7 2 71 sorted output cat test.txt | sort | uniq -c | sort 1 11 1 3 1 45 1 7 2 1 2 71 3 22 revert sorted output, numeric value (-n) otherwise it's alphabetically (1.. 10.. 100.. then 2.. 20.., etc.) cat test.txt | sort | uniq -c | sort -n -r 3 22 2 71 2 1 1 7 1 45 1 3 1 11 All file containing a pattern List all file that contain a pattern in current folder: Be aware to filtered out subfolder by precising some file name pattern (e.g. '*.log', not only '*') grep searchedString *.log Output only file name that contain pattern with option -l (that is a lowercase L): grep -l searchedString *.log Grep end of line after match SRC cat error.log | grep -A 1 -B 1 --group-separator==============\\\\r\\\\n \"not valid\" Empty file Empty File Content by Redirecting to Null: > access.log Disk usage df -h Huge file: sudo du -xh / | grep -P \"G\\t\" Last modification of a file date -r fileName Creation date of a file Below process to find creation date of a test file In a tmp directory create a test file Find inode of the file Find partition on which current folder belongs to Use file system debugger to find creation date mkdir tmp cd tmp touch test ls -i 201769 test df . Filesystem 1K-blocks Used Available Use% Mounted on /dev/sdd 263174212 10339304 239396752 5% / sudo debugfs -R 'stat <201769>' /dev/sdd . . debugfs 1.45.5 (07-Jan-2020) . . Filesystem 1K-blocks Used Available Use% Mounted on /dev/sdd 263174212 10339304 239396752 5% / 16:15:36 \u2714 oldu:(main)~/git/doc/tmp$ sudo debugfs -R 'stat <201769>' /dev/sdd Inode: 201769 Type: regular Mode: 0644 Flags: 0x80000 Generation: 333662723 Version: 0x00000000:00000001 User: 1000 Group: 1000 Project: 0 Size: 0 File ACL: 0 Links: 1 Blockcount: 0 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x624da077:74d33a00 -- Wed Apr 6 16:15:19 2022 atime: 0x624da077:74d33a00 -- Wed Apr 6 16:15:19 2022 mtime: 0x624da077:74d33a00 -- Wed Apr 6 16:15:19 2022 crtime: 0x624da077:74d33a00 -- Wed Apr 6 16:15:19 2022 Size of extra inode fields: 32 Inode checksum: 0x38e4efd5 Since when a process run ps -p pid -o etime Then use this online tool to calculate time between above result and now. How many file descriptors opened by a process disk usage find processName's process id (e.g. 28043) number of fd opened for pid max limit of opened fd for pid df -H ps ax | grep processName sudo ls /proc/28043/fd | wc -l sudo grep \"Max open files\" /proc/28043/limits | awk '{ print $4; }' Copy filtered file list Copy (for move replace below cp by mv) grep\"ed\" filtered files list to another folder. ls | grep pattern1 | grep pattern2 | ... | xargs cp -t /destinationFolder Add prefix to file names list for f in * ; do mv -- \"$f\" \"PRE_$f\" ; done Curly braces Using braces to build a sequence. All about {Curly Braces} in Bash echo {0..10} 0 1 2 3 4 5 6 7 8 9 10 Search replace Search (s) and replace in place (-i) all occurrences (g) of each lines in a file: sed -i 's/SEARCH/REPLACE/g' filename Add new line after delimiter cat someFile | tr ',' '\\n' > someOtherFile Find file Find file everywhere find / -type f -iname \"foo*txt\" Occurrence between positions file.log 8308353738102486352f010201027fcb627c31e5627c31e51b1fed6e057142e2000050430000006f01200800000affbf2f0a19006a12060000957ff100005896000000000000000000000001007e2e6c 8308352739097829334f0102010252e2627c31e5627c31e51c06e102042bcf150000cd1c0000000200e309000001ffb54f0919006a120600001bd13600006861000000000000000000000003006ed60b 8308353738102489273f01020102a34f627c31e6627c31e61c9f9ea6ff6a31fa00003f220000001000c60800000affc96f0a19006a12060000cc05c700005ea5000000000000000000000003005ecd9f 8308352739097829607f010201025a0d627c31e6627c31e61c071bfd042c35d60000c71f00000002004409000001ffb14f0a19006a12060000259d1c0000689e000000000000000000000003007a3a7b 8308352739097880667f0102010259e1627c31e6627c31e61c072119042c417a0000c87f0000000000320a000001ffb74f0919006a12060000268bd600006991000000000000000000000003007a6424 8308359739077046597f01020102dc55627c31e6627c31e61c0c20cc043558d20000c0df0000000b00fc07000001ffaf4f0b01005012050002498f8300002f24000000000000000000000001 8308352739097869488f0102010295a6627c31e7627c31e71c22e13f048031af0000bd94000000f700f40b000001ffcb4f0719006a1206000131f5b200005c2600000000000000000000000100784265 8308359739074198193f010201092d26627c31ea627c31ea1c403622047e2e0e0000cdf800000005000607000001ffc34f0c010000004700000000000000001f0306a54300005d75000000000000000000000005001400293030313030323033323030393030323033333232303137393030303030303030303033323030320d0a 8308359739074921768f010201028cd9627c31ea627c31ea1c0276a2044346fc0000c0bc00000160008b0a000001ffb34f080100501205002539df8700006d9b000000000000000000000001 8308359739074921768f010201028cda627c31ec627c31ec1c02747a044347270000c0de0000012300b40a000001ffb34f082100501205002539df8e00006d21000000000000000000000001 8308353738102196969f010201097329627c31cb627c31a01bd886d5042236490001564100000008000007200001ffaf0010000800004700000000000000001f00f4fec700002e13000000000000000000000000001400293030313030323033323030393030383030313030333036313030303030303030303036343037350d0a 8308353738102196969f01020109732a627c31cb627c31a01bd886d5042236490001564100000008000007240000ff8f0010000800004700000000000000001f00f4fec700002e13000000000000000000000000001400293030313030323033323030393030383030313030343232363030303030303030303036343136350d0a 8308353738102196969f01020109732b627c31cb627c31a01bd886d5042236490001564100000008000007240000ff8f0010000800004700000000000000001f00f4fec700002dd6000000000000000000000000001400293030313030323033323030393030383030313030343034363030303030303030303033323132310d0a 8308359739074921768f010201028cdb627c31ee627c31ee1c02739c0443456a0000c0840000011800e009000001ffb34f090100501205002539df9400006d03000000000000000000000001 8308353738102499926f010201029e30627c31ee627c31ee1bf9c2b40478e1070000f1ef00000042013c08000001ffad4f0c19006a1206000052d4cf000066b8000000000000000000000003007efec1 8308353738102524517f010201024489627c31ef627c31ef1bcf8c4803e28a9f0000d3f200000000003409000001ffbb4f0919006a120600001a58fa00003756000000000000000000000003002ab371 8308353738102312889f010201024c09627c31ef627c31ef1bf9b47a0478fad60000f27e00000042013807000001ffc94f0d19006a120600004dadf600006955000000000000000000000003009e80a3 8308359739077086684f010201090f9d627c31ce627c314b1b8cfbad04643f4c0000cd6900000008016206240001ffc3000e000800004700000000000000001f02d207da00003166000000000000000000000000001400293030313030323033323030393030383031323038333039353030303030303030303033323233380d0a 8308353738102523980f010201020f2f627c31f1627c31f11b8d9d7304674fb10000d76900000000000108000001ffb94f0a59006a1206000004b7de0000001e000000000000000000000001000c201b 8308353738103013429f010201029ffb627c31f2627c316d1ba209ca049fedab0001e82000000000015504040001ffb34f0e01086a120600000851b200006b59000000000000000000000003001d909e ... cut -c5-19 file.log | sort | uniq -c | sort -r | head -20 7301 352739090974673 2615 352431063062294 2593 359739074198193 2086 359739074201385 1701 359739074263179 1636 359739074206988 1600 359739074197658 1451 353738102180005 1358 359739074259185 1342 352739097909052 1306 352739090926269 1294 359739074263237 1292 353738102197058 1268 359739074261710 1247 359739072525975 1228 352739098097725 1222 353738102229216 1154 359739074198177 1146 359739074261678 1143 352739097814815 Occurrence of column value file.log 864394040507255;2022-05-12T07:35:05.027Z;000000000000004c080100000180b7314c5800047ba3c41b96f137021600ba0d001d00110aef01f00150011505c8004501ed02715dfb00fc0006b50007b60003426f1118001b430fd4440000011003a27f140001000010dd 864394040405104;2022-05-12T07:35:05.029Z;000000000000004c080100000180b73144880003f617d01bc600df02cf00be0e0000ef110aef00f00150011505c8004501ed02715ffb00fc0006b50004b6000242377d180000430fe2440000011001b0a87a00010000765d 864394040167563;2022-05-12T07:35:05.029Z;000000000000004c080100000180b7314c580003a87f4b1b866a18018501140c000e00110aef01f00150011505c8004501ed027164fb00fc0006b50004b600034234cb18000e43101b4400000110014d5e330001000091b9 864394040096895;2022-05-12T07:35:05.033Z;000000000000005d080100000180b7314c5800040b57011c0faa1903f100240d002b00160eef01f00150011505c8004501b300b400ed02715dfa01fb00fc00f80106b50004b6000242385718002b430fd0440000011001033b1a014e012451dd310000e60100009472 864394041295967;2022-05-12T07:35:05.080Z;000000000000004c080100000180b7314c580004159ddd1bb0dd9b016f009610000c00110aef01f00150011505c8004501ed02715efb00fc0006b5000ab600064231ef18000c430fd94400000110003be7ae0001000061cd 864394040166508;2022-05-12T07:35:05.118Z;0000000000000062080100000180b7314c580003eb71a41bbd7af801ab00700e002519180fef01f00150011505c8004501b300b4001d00ed02715bfa01fb00fc00f80107b50005b600034235f6180025430fb84400001907210110081c85f9014e0000000000000000010000911e 864394040318653;2022-05-12T07:35:05.118Z;0000000000000062080100000180b7314c58000498609a1c266cdb019f01480e003500180fef01f00150011505c8004501b300b4001d00ed027158fa01fb00fc00f80107b50008b60004423186180035430f9744000019073a011003820948014e0104696aba5f4d8a0100003eb9 864394040172985;2022-05-12T07:35:05.133Z;000000000000004c080100000180b73111c0000433a4951bc916950314014b110000ef110aef01f00050001505c8004501ed027151fb00fc0006b50004b60002422c85180000430f47440000011002551d6a000100009245 864394040892798;2022-05-12T07:35:05.149Z;00000000000000de8e0100000180b731504000042266531bae2fde02ea00da11001c0000001b000f00ef0100f00100500100150300c80000450100b30000b40000ed0200715201070100fa0100fb0000fc0000f801000800b5000b00b60007004238b70018001c00430f54004400000019022c001af8df000100100278bdf70001004e0104262baaf4721b0002014b002802010605166e2a2c020e0950205420454e2038303446333200000000000000000000000000000000014c002802010605166e2adff80e0950205420454e2038303446333300000000000000000000000000000000010000e60b 864394040894265;2022-05-12T07:35:05.198Z;000000000000004c080100000180b7314c580003f5e7d41be1a06c01b300920e000c00110aef01f00150011505c8004501ed02715cfb00fc0006b5000eb6000742386a18000c430fca4400000110011faa0300010000dcd0 864394041317902;2022-05-12T07:35:05.295Z;000000000000004c080100000180b7314c58000515aa7f1c3e230a0194015c0f000700110aef01f00150011504c8004501ed02715efb00fc0006b50004b600024236d4180007430fde4400000110005b17300001000082c3 864394041300338;2022-05-12T07:35:05.317Z;0000000000000095080200000180b730719800048b2dc31c0cd0bc021700bf0c0000fb110aef01f00050001505c8004501ed02715ffb00fc0006b50008b60004423009180000430fe744000001100065b5eb0000000180b730796800048b2dc31c0cd0bc021700bf0c0000ef110aef00f00050001505c8004501ed02715ffb00fc0006b50008b60004423045180000430fe744000001100065b5eb000200007de2 864394040179410;2022-05-12T07:35:05.320Z;000000000000005f080100000180b731487000044cc01f1b75b47d03e701050e000a00170fef01f00150011505c8004501b3000200b400ed027159fa01fb00fc00f80106b50006b6000342311618000e430f9d440000011001638427014e018478c68a0000600100007506 864394041319494;2022-05-12T07:35:05.335Z;00000000000000808e0100000180b730813800059eab7a1c517f9d000000000000000019000d000700ef0000f00000500000c80200450300715401070100040042310800430f680044000000190aa8000100100091f8a200000001014b002802010605166e2aa80a100950205450524f4245203030303432420000000000000000000000000000010000d0cb 864394040317283;2022-05-12T07:35:05.389Z;0000000000000062080100000180b731487000053e79c31c34d2aa020d01460d000d00180fef01f00150011505c8004501b300b4001d00ed027155fa01fb00fc00f80107b50007b6000342377418000d430f76440000190778011003e0419b014e01042d60ba5f4d670100007b72 864394040978779;2022-05-12T07:35:05.396Z;000000000000005d080100000180b73150400003f81ea01bb8e77c01af00690f001000160eef01f00150011505c8004501b300b400ed027160fa01fb00fc00f80106b50005b6000242345c180010430fec440000011000d6972b014e011464dd3100005d0100001c81 864394040892228;2022-05-12T07:35:05.414Z;00000000000000de8e0100000180b73150400003e42b561bb93f2e018200731000000019001b000f00ef0000f00000500000150500c80000450100b30000b40000ed0200715701070100fa0100fb0000fc0000f801000800b5000a00b60006004232120018000000430f8900440000001903e8001a03520001001001b159d90001004e01044d3ebaf472890002014b002802010605166e2ae8030e0950205420454e2038303446303600000000000000000000000000000000014c002802010605166e2a52030e0950205420454e20383034463037000000000000000000000000000000000100009fed 864394040912182;2022-05-12T07:35:05.480Z;00000000000000c0080300000180b7311d7800051878db1c3ae6d900000000000000010b07ef00f0005000c802450301017157034230fd430f8944000001100005168a0000000180b731216000051878db1c3ae6d900000000000000ef0b07ef01f0005000c802450301017157034230e4430f8944000001100005168a0000000180b731504000051878db1c3ae6d900000000000000f0110aef01f00150011500c80045020101ed027157fc0006b50000b6000042348c180000430f8944000001100005168a000300008465 866907053384763;2022-05-12T07:35:05.507Z;000000000000004c080100000180b73150400005a65ddb1c23799501bb015414001100110aef01f00150011504c8004501ed027100fb00fc0006b50008b600054239f018001143000044000001100010f980000100003a2c 864394040893135;2022-05-12T07:35:05.622Z;00000000000000de8e0100000180b7314c580003f225c91bbe004e026900f91000080000001b000f00ef0100f00100500100150500c80000450100b30000b40000ed0200715a01070100fa0100fb0000fc0000f801000800b5000a00b600060042383e0018000800430fb30044000000190539001a045e0001001001a997860001004e01041e6f12f572120002014b002802010605166e2a39050e0950205420454e2038303446303500000000000000000000000000000000014c002802010605166e2a5e040e0950205420454e203830344630410000000000000000000000000000000001000000cd ... cat file.log | awk -F ';' '{print $1}' | sort | uniq -c | sort -r | head -20 434 864394041317027 424 864394040899470 423 864394040909931 419 864394040912117 396 864394040896856 384 864394040913370 376 864394040891147 363 864394040989388 363 864394040892012 362 864394040292239 355 864394040996292 355 864394040907570 353 864394040915094 348 864394040917314 346 864394040924849 343 864394040894307 336 864394040892798 331 864394041302482 328 864394040907448 328 864394040892780 Occurrence live watch - execute a program periodically, showing output full screen Live occurrences change of column (first in example below '$1') value in file.log with ';' separated column values: watch -d \"awk -F ';' '{print \\$1}' file.log | sort | uniq -c | sort -n -r | head -20\" Linux Watch Command jq Command-line JSON processor Select a field that contain true and output another field (+ count + sort + head result): jq 'select(.fieldThatMayContainTrue|startswith(\"true\"))|.outputField' file.ndjson | sort | uniq -c | sort -n -r | head -50 Find patterns across multiple lines Find in file between \"abc\" AND \"efg\", in that order. test.txt: blah blah.. blah blah.. blah abc blah blah blah.. blah blah.. blah blah.. blah efg blah blah blah blah.. blah blah.. sed -n '/abc/,/efg/p' test.txt output: blah abc blah blah blah.. blah blah.. blah blah.. blah efg blah blah Number of pattern occurrence through files grep -c pattern files* Bash option select Script template launched with option(s). optionselect script, chmod +x then ./optionselect to execute: #!/bin/bash trap 'exit 130' INT optionwitharg = \"option with arg = -y arg\" optionwithoutarg = \"option without arg = -n\" if [ $# -eq 0 ] ; then printf '\\n' echo \"usage: \" $optionwitharg \", \" $optionwithoutarg printf '\\n' echo \"e.g.: $ ./optionselect -n -y hello\" printf '\\n' exit 1 fi while getopts \"h?y:?n\" opt ; do case \" $opt \" in h | \\? ) echo \"help!.. not implemented yet, sorry :(\" exit 0 ;; y ) filter = $OPTARG echo \" $OPTARG \" ;; n ) echo \"no arg\" ;; esac done shift $(( OPTIND-1 )) dos2unix Utility to reformat text files generated under Windows for use under Linux: dos2unix file","title":"Linux - 01 - Misc"},{"location":"linux/1-linux-misc/#linux-01-misc","text":"","title":"Linux - 01 - Misc"},{"location":"linux/1-linux-misc/#curl","text":"curl -i localhost curl -sb -H \"Accept: application/json\" \"http://localhost\" | json_pp","title":"Curl"},{"location":"linux/1-linux-misc/#compress-decompress","text":"","title":"Compress Decompress"},{"location":"linux/1-linux-misc/#compress-czf","text":"- c compress, -z zip, -f file tar -czf /targetfolder/targetfile.tar.gz /sourcefolder","title":"Compress (czf)"},{"location":"linux/1-linux-misc/#decompress-xzf","text":"- x extract, -z zip, -f file tar -xzf targetfile.tar.gz or gzip, -k to keep original, -d to decompress: gzip -kd file.gz","title":"Decompress (xzf)"},{"location":"linux/1-linux-misc/#package","text":"","title":"Package"},{"location":"linux/1-linux-misc/#list-installed","text":"apt list --installed","title":"List installed"},{"location":"linux/1-linux-misc/#info","text":"apt-cache show packagename","title":"Info"},{"location":"linux/1-linux-misc/#remove","text":"sudo apt-get --purge autoremove packagename","title":"Remove"},{"location":"linux/1-linux-misc/#switch-user-to-root","text":"Switch current user to root sudo su -","title":"Switch user to root"},{"location":"linux/1-linux-misc/#grep-lines-before-after-match","text":"-B before -A after Stick ne lines just after option grep -B2 -A3 pattern infile.txt","title":"Grep lines before after match"},{"location":"linux/1-linux-misc/#copy-files-from-list","text":"Copy specific files from a text list of files rsync -a sourcefolder --files-from=list.txt destinationfolder","title":"Copy files from list"},{"location":"linux/1-linux-misc/#get-data-between-two-patterns","text":"In error.log . . 05:59:30.024 [nioEventLoopGroup-3-5] ERROR c.l.d.c.ConnectorServerHandlerTCP.parseAndSendMessageTeltonika(177) - class java.util.concurrent.ExecutionException FOR RAW DATA : 0000017d55747fb00003f5380c1be045ce00000000000000000804ef005000c8024503034230fc430f8d440000011007e0fca600 WITH STACKTRACE : {} . . Get data between \"DATA : \" and \" WITH\" (hex raw data) cat error.log | sed -nr 's/.*DATA : (.*) WITH.*/\\1/p' or cat file | grep -o -P '(?<=left).*(?=right)'","title":"Get data between two patterns"},{"location":"linux/1-linux-misc/#grep-patterns-from-a-file","text":"-f option, maybe -oF options also grep -f patterns_file *.log or grep -oFf patterns.txt *.log If result's count's not OK, check by not found pattern (-h option to hide filename in output) grep -hoFf patterns.txt *.log | grep -vFf - patterns.txt","title":"Grep patterns from a file"},{"location":"linux/1-linux-misc/#for-list-of-file","text":"For list of file in current folder, do operation. Here we want to have file name, cat content and separate result with a new line ll *.txt 1.txt 2.txt 3.txt 4.txt 5.txt for f in {2..4}.txt; do echo \"$f\"; cat \"$f\"; printf \"\\n\"; done 2.txt jkl mno pqr 3.txt stu vwx yza 4.txt bcd efg hij","title":"For list of file"},{"location":"linux/1-linux-misc/#search-between-timestamp","text":"sed -rne '/10:50/,/11:05/ p' file Put existing time range in file (10:50 - 11:05).","title":"Search between timestamp"},{"location":"linux/1-linux-misc/#highlight-search-result","text":"grep --color=always -z pattern file | less -R always to transmit color through pipe -z to show everything, not only the matching pattern -R to avoid showing esc char instead of color","title":"Highlight search result"},{"location":"linux/1-linux-misc/#delete-history","text":"history 1003 25-04-2016 17:54:52 echo \"Command 1\" 1004 25-04-2016 17:54:54 echo \"Command 2\" 1005 25-04-2016 17:54:57 echo \"Command 3\" 1006 25-04-2016 17:54:59 echo \"Command 4\" 1007 25-04-2016 17:55:01 echo \"Command 5\" 1008 25-04-2016 17:55:03 echo \"Command 6\" 1009 25-04-2016 17:55:07 echo \"Command 7\" 1010 25-04-2016 17:55:09 echo \"Command 8\" 1011 25-04-2016 17:55:11 echo \"Command 9\" 1012 25-04-2016 17:55:14 echo \"Command 10\" for h in $(seq 1006 1008); do history -d 1006; done","title":"Delete history"},{"location":"linux/1-linux-misc/#where-is-a-program","text":"How to know where reside a program, e.g. ls? which ls /usr/bin/ls env env | grep PATH","title":"Where is a program"},{"location":"linux/1-linux-misc/#difference-of-cmd-output","text":"diff <(ls test1) <(ls test2) Difference of sorted lists sort ok.txt > okSorted.txt sort all.txt > allSorted.txt diff --new-line-format=\"\" --unchanged-line-format=\"\" allSorted.txt okSorted.txt","title":"Difference of cmd output"},{"location":"linux/1-linux-misc/#column","text":"","title":"Column"},{"location":"linux/1-linux-misc/#min","text":"cat ... | grep ... | sort -n -r | tail -n10","title":"Min"},{"location":"linux/1-linux-misc/#max","text":"cat ... | grep ... | sort -n -r | head -n10","title":"Max"},{"location":"linux/1-linux-misc/#average","text":"test.txt (warning on empty lines (maybe at the end)) 1 3 7 cat test.txt | awk '{ total += $1 } END { print total/NR }' 3.66667","title":"Average"},{"location":"linux/1-linux-misc/#median","text":"test.txt (warning on empty lines (maybe at the end)) 1 3 7 11 22 45 71 median.awk #/usr/bin/env awk { count[NR] = $1; } END { if (NR % 2) { print count[(NR + 1) / 2]; } else { print (count[(NR / 2)] + count[(NR / 2) + 1]) / 2.0; } } cat test.txt | awk -f median.awk 11 Check equal number of values below and above median cat test.txt | awk '{if($1 < 11) print $1}' | wc -l 3 cat test.txt | awk '{if($1 > 11) print $1}' | wc -l 3","title":"Median"},{"location":"linux/1-linux-misc/#get-nth-column-from-file","text":"Get nth column from file with field separated values test.txt column 1 row 1;column 2 row 1;column 3 row 1 column 1 row 2;column 2 row 2;column 3 row 2 column 1 row 3;column 2 row 3;column 3 row 3 cat test.txt | awk -F ';' '{print $2}' column 2 row 1 column 2 row 2 column 2 row 3","title":"Get nth column from file"},{"location":"linux/1-linux-misc/#conditional","text":"cat test.txt | awk -F ';' '{if($1 == \"column 1 row 2\") print $2}' column 2 row 2","title":"Conditional"},{"location":"linux/1-linux-misc/#list-files","text":"Sorted by sizes and human readable ll -S -h","title":"List files"},{"location":"linux/1-linux-misc/#uniq-values-in-a-file","text":"Uniq values in a file, sorted and counted (first sort is mandatory) test.txt 1 71 3 7 22 1 11 22 45 71 22 cat test.txt | sort | uniq -c 2 1 1 11 3 22 1 3 1 45 1 7 2 71 sorted output cat test.txt | sort | uniq -c | sort 1 11 1 3 1 45 1 7 2 1 2 71 3 22 revert sorted output, numeric value (-n) otherwise it's alphabetically (1.. 10.. 100.. then 2.. 20.., etc.) cat test.txt | sort | uniq -c | sort -n -r 3 22 2 71 2 1 1 7 1 45 1 3 1 11","title":"Uniq values in a file"},{"location":"linux/1-linux-misc/#all-file-containing-a-pattern","text":"List all file that contain a pattern in current folder: Be aware to filtered out subfolder by precising some file name pattern (e.g. '*.log', not only '*') grep searchedString *.log Output only file name that contain pattern with option -l (that is a lowercase L): grep -l searchedString *.log","title":"All file containing a pattern"},{"location":"linux/1-linux-misc/#grep-end-of-line-after-match","text":"SRC cat error.log | grep -A 1 -B 1 --group-separator==============\\\\r\\\\n \"not valid\"","title":"Grep end of line after match"},{"location":"linux/1-linux-misc/#empty-file","text":"Empty File Content by Redirecting to Null: > access.log","title":"Empty file"},{"location":"linux/1-linux-misc/#disk-usage","text":"df -h Huge file: sudo du -xh / | grep -P \"G\\t\"","title":"Disk usage"},{"location":"linux/1-linux-misc/#last-modification-of-a-file","text":"date -r fileName","title":"Last modification of a file"},{"location":"linux/1-linux-misc/#creation-date-of-a-file","text":"Below process to find creation date of a test file In a tmp directory create a test file Find inode of the file Find partition on which current folder belongs to Use file system debugger to find creation date mkdir tmp cd tmp touch test ls -i 201769 test df . Filesystem 1K-blocks Used Available Use% Mounted on /dev/sdd 263174212 10339304 239396752 5% / sudo debugfs -R 'stat <201769>' /dev/sdd . . debugfs 1.45.5 (07-Jan-2020) . . Filesystem 1K-blocks Used Available Use% Mounted on /dev/sdd 263174212 10339304 239396752 5% / 16:15:36 \u2714 oldu:(main)~/git/doc/tmp$ sudo debugfs -R 'stat <201769>' /dev/sdd Inode: 201769 Type: regular Mode: 0644 Flags: 0x80000 Generation: 333662723 Version: 0x00000000:00000001 User: 1000 Group: 1000 Project: 0 Size: 0 File ACL: 0 Links: 1 Blockcount: 0 Fragment: Address: 0 Number: 0 Size: 0 ctime: 0x624da077:74d33a00 -- Wed Apr 6 16:15:19 2022 atime: 0x624da077:74d33a00 -- Wed Apr 6 16:15:19 2022 mtime: 0x624da077:74d33a00 -- Wed Apr 6 16:15:19 2022 crtime: 0x624da077:74d33a00 -- Wed Apr 6 16:15:19 2022 Size of extra inode fields: 32 Inode checksum: 0x38e4efd5","title":"Creation date of a file"},{"location":"linux/1-linux-misc/#since-when-a-process-run","text":"ps -p pid -o etime Then use this online tool to calculate time between above result and now.","title":"Since when a process run"},{"location":"linux/1-linux-misc/#how-many-file-descriptors-opened-by-a-process","text":"disk usage find processName's process id (e.g. 28043) number of fd opened for pid max limit of opened fd for pid df -H ps ax | grep processName sudo ls /proc/28043/fd | wc -l sudo grep \"Max open files\" /proc/28043/limits | awk '{ print $4; }'","title":"How many file descriptors opened by a process"},{"location":"linux/1-linux-misc/#copy-filtered-file-list","text":"Copy (for move replace below cp by mv) grep\"ed\" filtered files list to another folder. ls | grep pattern1 | grep pattern2 | ... | xargs cp -t /destinationFolder","title":"Copy filtered file list"},{"location":"linux/1-linux-misc/#add-prefix-to-file-names-list","text":"for f in * ; do mv -- \"$f\" \"PRE_$f\" ; done","title":"Add prefix to file names list"},{"location":"linux/1-linux-misc/#curly-braces","text":"Using braces to build a sequence. All about {Curly Braces} in Bash echo {0..10} 0 1 2 3 4 5 6 7 8 9 10","title":"Curly braces"},{"location":"linux/1-linux-misc/#search-replace","text":"Search (s) and replace in place (-i) all occurrences (g) of each lines in a file: sed -i 's/SEARCH/REPLACE/g' filename","title":"Search replace"},{"location":"linux/1-linux-misc/#add-new-line-after-delimiter","text":"cat someFile | tr ',' '\\n' > someOtherFile","title":"Add new line after delimiter"},{"location":"linux/1-linux-misc/#find-file","text":"Find file everywhere find / -type f -iname \"foo*txt\"","title":"Find file"},{"location":"linux/1-linux-misc/#occurrence-between-positions","text":"file.log 8308353738102486352f010201027fcb627c31e5627c31e51b1fed6e057142e2000050430000006f01200800000affbf2f0a19006a12060000957ff100005896000000000000000000000001007e2e6c 8308352739097829334f0102010252e2627c31e5627c31e51c06e102042bcf150000cd1c0000000200e309000001ffb54f0919006a120600001bd13600006861000000000000000000000003006ed60b 8308353738102489273f01020102a34f627c31e6627c31e61c9f9ea6ff6a31fa00003f220000001000c60800000affc96f0a19006a12060000cc05c700005ea5000000000000000000000003005ecd9f 8308352739097829607f010201025a0d627c31e6627c31e61c071bfd042c35d60000c71f00000002004409000001ffb14f0a19006a12060000259d1c0000689e000000000000000000000003007a3a7b 8308352739097880667f0102010259e1627c31e6627c31e61c072119042c417a0000c87f0000000000320a000001ffb74f0919006a12060000268bd600006991000000000000000000000003007a6424 8308359739077046597f01020102dc55627c31e6627c31e61c0c20cc043558d20000c0df0000000b00fc07000001ffaf4f0b01005012050002498f8300002f24000000000000000000000001 8308352739097869488f0102010295a6627c31e7627c31e71c22e13f048031af0000bd94000000f700f40b000001ffcb4f0719006a1206000131f5b200005c2600000000000000000000000100784265 8308359739074198193f010201092d26627c31ea627c31ea1c403622047e2e0e0000cdf800000005000607000001ffc34f0c010000004700000000000000001f0306a54300005d75000000000000000000000005001400293030313030323033323030393030323033333232303137393030303030303030303033323030320d0a 8308359739074921768f010201028cd9627c31ea627c31ea1c0276a2044346fc0000c0bc00000160008b0a000001ffb34f080100501205002539df8700006d9b000000000000000000000001 8308359739074921768f010201028cda627c31ec627c31ec1c02747a044347270000c0de0000012300b40a000001ffb34f082100501205002539df8e00006d21000000000000000000000001 8308353738102196969f010201097329627c31cb627c31a01bd886d5042236490001564100000008000007200001ffaf0010000800004700000000000000001f00f4fec700002e13000000000000000000000000001400293030313030323033323030393030383030313030333036313030303030303030303036343037350d0a 8308353738102196969f01020109732a627c31cb627c31a01bd886d5042236490001564100000008000007240000ff8f0010000800004700000000000000001f00f4fec700002e13000000000000000000000000001400293030313030323033323030393030383030313030343232363030303030303030303036343136350d0a 8308353738102196969f01020109732b627c31cb627c31a01bd886d5042236490001564100000008000007240000ff8f0010000800004700000000000000001f00f4fec700002dd6000000000000000000000000001400293030313030323033323030393030383030313030343034363030303030303030303033323132310d0a 8308359739074921768f010201028cdb627c31ee627c31ee1c02739c0443456a0000c0840000011800e009000001ffb34f090100501205002539df9400006d03000000000000000000000001 8308353738102499926f010201029e30627c31ee627c31ee1bf9c2b40478e1070000f1ef00000042013c08000001ffad4f0c19006a1206000052d4cf000066b8000000000000000000000003007efec1 8308353738102524517f010201024489627c31ef627c31ef1bcf8c4803e28a9f0000d3f200000000003409000001ffbb4f0919006a120600001a58fa00003756000000000000000000000003002ab371 8308353738102312889f010201024c09627c31ef627c31ef1bf9b47a0478fad60000f27e00000042013807000001ffc94f0d19006a120600004dadf600006955000000000000000000000003009e80a3 8308359739077086684f010201090f9d627c31ce627c314b1b8cfbad04643f4c0000cd6900000008016206240001ffc3000e000800004700000000000000001f02d207da00003166000000000000000000000000001400293030313030323033323030393030383031323038333039353030303030303030303033323233380d0a 8308353738102523980f010201020f2f627c31f1627c31f11b8d9d7304674fb10000d76900000000000108000001ffb94f0a59006a1206000004b7de0000001e000000000000000000000001000c201b 8308353738103013429f010201029ffb627c31f2627c316d1ba209ca049fedab0001e82000000000015504040001ffb34f0e01086a120600000851b200006b59000000000000000000000003001d909e ... cut -c5-19 file.log | sort | uniq -c | sort -r | head -20 7301 352739090974673 2615 352431063062294 2593 359739074198193 2086 359739074201385 1701 359739074263179 1636 359739074206988 1600 359739074197658 1451 353738102180005 1358 359739074259185 1342 352739097909052 1306 352739090926269 1294 359739074263237 1292 353738102197058 1268 359739074261710 1247 359739072525975 1228 352739098097725 1222 353738102229216 1154 359739074198177 1146 359739074261678 1143 352739097814815","title":"Occurrence between positions"},{"location":"linux/1-linux-misc/#occurrence-of-column-value","text":"file.log 864394040507255;2022-05-12T07:35:05.027Z;000000000000004c080100000180b7314c5800047ba3c41b96f137021600ba0d001d00110aef01f00150011505c8004501ed02715dfb00fc0006b50007b60003426f1118001b430fd4440000011003a27f140001000010dd 864394040405104;2022-05-12T07:35:05.029Z;000000000000004c080100000180b73144880003f617d01bc600df02cf00be0e0000ef110aef00f00150011505c8004501ed02715ffb00fc0006b50004b6000242377d180000430fe2440000011001b0a87a00010000765d 864394040167563;2022-05-12T07:35:05.029Z;000000000000004c080100000180b7314c580003a87f4b1b866a18018501140c000e00110aef01f00150011505c8004501ed027164fb00fc0006b50004b600034234cb18000e43101b4400000110014d5e330001000091b9 864394040096895;2022-05-12T07:35:05.033Z;000000000000005d080100000180b7314c5800040b57011c0faa1903f100240d002b00160eef01f00150011505c8004501b300b400ed02715dfa01fb00fc00f80106b50004b6000242385718002b430fd0440000011001033b1a014e012451dd310000e60100009472 864394041295967;2022-05-12T07:35:05.080Z;000000000000004c080100000180b7314c580004159ddd1bb0dd9b016f009610000c00110aef01f00150011505c8004501ed02715efb00fc0006b5000ab600064231ef18000c430fd94400000110003be7ae0001000061cd 864394040166508;2022-05-12T07:35:05.118Z;0000000000000062080100000180b7314c580003eb71a41bbd7af801ab00700e002519180fef01f00150011505c8004501b300b4001d00ed02715bfa01fb00fc00f80107b50005b600034235f6180025430fb84400001907210110081c85f9014e0000000000000000010000911e 864394040318653;2022-05-12T07:35:05.118Z;0000000000000062080100000180b7314c58000498609a1c266cdb019f01480e003500180fef01f00150011505c8004501b300b4001d00ed027158fa01fb00fc00f80107b50008b60004423186180035430f9744000019073a011003820948014e0104696aba5f4d8a0100003eb9 864394040172985;2022-05-12T07:35:05.133Z;000000000000004c080100000180b73111c0000433a4951bc916950314014b110000ef110aef01f00050001505c8004501ed027151fb00fc0006b50004b60002422c85180000430f47440000011002551d6a000100009245 864394040892798;2022-05-12T07:35:05.149Z;00000000000000de8e0100000180b731504000042266531bae2fde02ea00da11001c0000001b000f00ef0100f00100500100150300c80000450100b30000b40000ed0200715201070100fa0100fb0000fc0000f801000800b5000b00b60007004238b70018001c00430f54004400000019022c001af8df000100100278bdf70001004e0104262baaf4721b0002014b002802010605166e2a2c020e0950205420454e2038303446333200000000000000000000000000000000014c002802010605166e2adff80e0950205420454e2038303446333300000000000000000000000000000000010000e60b 864394040894265;2022-05-12T07:35:05.198Z;000000000000004c080100000180b7314c580003f5e7d41be1a06c01b300920e000c00110aef01f00150011505c8004501ed02715cfb00fc0006b5000eb6000742386a18000c430fca4400000110011faa0300010000dcd0 864394041317902;2022-05-12T07:35:05.295Z;000000000000004c080100000180b7314c58000515aa7f1c3e230a0194015c0f000700110aef01f00150011504c8004501ed02715efb00fc0006b50004b600024236d4180007430fde4400000110005b17300001000082c3 864394041300338;2022-05-12T07:35:05.317Z;0000000000000095080200000180b730719800048b2dc31c0cd0bc021700bf0c0000fb110aef01f00050001505c8004501ed02715ffb00fc0006b50008b60004423009180000430fe744000001100065b5eb0000000180b730796800048b2dc31c0cd0bc021700bf0c0000ef110aef00f00050001505c8004501ed02715ffb00fc0006b50008b60004423045180000430fe744000001100065b5eb000200007de2 864394040179410;2022-05-12T07:35:05.320Z;000000000000005f080100000180b731487000044cc01f1b75b47d03e701050e000a00170fef01f00150011505c8004501b3000200b400ed027159fa01fb00fc00f80106b50006b6000342311618000e430f9d440000011001638427014e018478c68a0000600100007506 864394041319494;2022-05-12T07:35:05.335Z;00000000000000808e0100000180b730813800059eab7a1c517f9d000000000000000019000d000700ef0000f00000500000c80200450300715401070100040042310800430f680044000000190aa8000100100091f8a200000001014b002802010605166e2aa80a100950205450524f4245203030303432420000000000000000000000000000010000d0cb 864394040317283;2022-05-12T07:35:05.389Z;0000000000000062080100000180b731487000053e79c31c34d2aa020d01460d000d00180fef01f00150011505c8004501b300b4001d00ed027155fa01fb00fc00f80107b50007b6000342377418000d430f76440000190778011003e0419b014e01042d60ba5f4d670100007b72 864394040978779;2022-05-12T07:35:05.396Z;000000000000005d080100000180b73150400003f81ea01bb8e77c01af00690f001000160eef01f00150011505c8004501b300b400ed027160fa01fb00fc00f80106b50005b6000242345c180010430fec440000011000d6972b014e011464dd3100005d0100001c81 864394040892228;2022-05-12T07:35:05.414Z;00000000000000de8e0100000180b73150400003e42b561bb93f2e018200731000000019001b000f00ef0000f00000500000150500c80000450100b30000b40000ed0200715701070100fa0100fb0000fc0000f801000800b5000a00b60006004232120018000000430f8900440000001903e8001a03520001001001b159d90001004e01044d3ebaf472890002014b002802010605166e2ae8030e0950205420454e2038303446303600000000000000000000000000000000014c002802010605166e2a52030e0950205420454e20383034463037000000000000000000000000000000000100009fed 864394040912182;2022-05-12T07:35:05.480Z;00000000000000c0080300000180b7311d7800051878db1c3ae6d900000000000000010b07ef00f0005000c802450301017157034230fd430f8944000001100005168a0000000180b731216000051878db1c3ae6d900000000000000ef0b07ef01f0005000c802450301017157034230e4430f8944000001100005168a0000000180b731504000051878db1c3ae6d900000000000000f0110aef01f00150011500c80045020101ed027157fc0006b50000b6000042348c180000430f8944000001100005168a000300008465 866907053384763;2022-05-12T07:35:05.507Z;000000000000004c080100000180b73150400005a65ddb1c23799501bb015414001100110aef01f00150011504c8004501ed027100fb00fc0006b50008b600054239f018001143000044000001100010f980000100003a2c 864394040893135;2022-05-12T07:35:05.622Z;00000000000000de8e0100000180b7314c580003f225c91bbe004e026900f91000080000001b000f00ef0100f00100500100150500c80000450100b30000b40000ed0200715a01070100fa0100fb0000fc0000f801000800b5000a00b600060042383e0018000800430fb30044000000190539001a045e0001001001a997860001004e01041e6f12f572120002014b002802010605166e2a39050e0950205420454e2038303446303500000000000000000000000000000000014c002802010605166e2a5e040e0950205420454e203830344630410000000000000000000000000000000001000000cd ... cat file.log | awk -F ';' '{print $1}' | sort | uniq -c | sort -r | head -20 434 864394041317027 424 864394040899470 423 864394040909931 419 864394040912117 396 864394040896856 384 864394040913370 376 864394040891147 363 864394040989388 363 864394040892012 362 864394040292239 355 864394040996292 355 864394040907570 353 864394040915094 348 864394040917314 346 864394040924849 343 864394040894307 336 864394040892798 331 864394041302482 328 864394040907448 328 864394040892780","title":"Occurrence of column value"},{"location":"linux/1-linux-misc/#occurrence-live","text":"watch - execute a program periodically, showing output full screen Live occurrences change of column (first in example below '$1') value in file.log with ';' separated column values: watch -d \"awk -F ';' '{print \\$1}' file.log | sort | uniq -c | sort -n -r | head -20\" Linux Watch Command","title":"Occurrence live"},{"location":"linux/1-linux-misc/#jq","text":"Command-line JSON processor Select a field that contain true and output another field (+ count + sort + head result): jq 'select(.fieldThatMayContainTrue|startswith(\"true\"))|.outputField' file.ndjson | sort | uniq -c | sort -n -r | head -50","title":"jq"},{"location":"linux/1-linux-misc/#find-patterns-across-multiple-lines","text":"Find in file between \"abc\" AND \"efg\", in that order. test.txt: blah blah.. blah blah.. blah abc blah blah blah.. blah blah.. blah blah.. blah efg blah blah blah blah.. blah blah.. sed -n '/abc/,/efg/p' test.txt output: blah abc blah blah blah.. blah blah.. blah blah.. blah efg blah blah","title":"Find patterns across multiple lines"},{"location":"linux/1-linux-misc/#number-of-pattern-occurrence-through-files","text":"grep -c pattern files*","title":"Number of pattern occurrence through files"},{"location":"linux/1-linux-misc/#bash-option-select","text":"Script template launched with option(s). optionselect script, chmod +x then ./optionselect to execute: #!/bin/bash trap 'exit 130' INT optionwitharg = \"option with arg = -y arg\" optionwithoutarg = \"option without arg = -n\" if [ $# -eq 0 ] ; then printf '\\n' echo \"usage: \" $optionwitharg \", \" $optionwithoutarg printf '\\n' echo \"e.g.: $ ./optionselect -n -y hello\" printf '\\n' exit 1 fi while getopts \"h?y:?n\" opt ; do case \" $opt \" in h | \\? ) echo \"help!.. not implemented yet, sorry :(\" exit 0 ;; y ) filter = $OPTARG echo \" $OPTARG \" ;; n ) echo \"no arg\" ;; esac done shift $(( OPTIND-1 ))","title":"Bash option select"},{"location":"linux/1-linux-misc/#dos2unix","text":"Utility to reformat text files generated under Windows for use under Linux: dos2unix file","title":"dos2unix"},{"location":"linux/2-linux-network/","text":"Linux - 02 - Network Network connection (sudo) netstat -tunlp Live network connection Live log established network connections on a dedidcated port number of a load balancer server and forwarded IP of communication server filtered out: watch -d 'netstat -an | grep :<port> | grep ESTABLISHED | grep -v <ip>' Send raw data hex UDP Sending hex raw data through UDP. References Linux nc command help and examples How To Use Netcat to Establish and Test TCP and UDP Connections | DigitalOcean linux - convert a hex string to binary and send with netcat - Unix & Linux Stack Exchange Linux/UNIX: Bash Read a File Line By Line - nixCraft shell script - Wait for keyboard input inside a while-read loop - Unix & Linux Stack Exchange linux - How to automatically close netcat connection after data is sent? - Server Fault Echo newline in Bash prints literal \\n - Stack Overflow perl - Shell magic wanted: format output of hexdump in a pipe - Stack Overflow One shot Command line, manual sending, one raw data frame (Like Network Stuff): echo -n \"830f9d10362f4bc8a1c..\" | perl -e 'print pack \"H*\", <STDIN>' | timeout 0.2 nc -w 1 -u <ip> <port> | xxd -p Script straight or line by line Read file straight forward or line by line. sample (file): 8308352739096415820f01020102128362d80bb162d80bb11c1e6.. 8308359739077103745f01020102b2df62d80bb262d80bb11bd9d.. .. readsample script, chmod +x then ./readsample to execute: #!/bin/bash trap 'exit 130' INT modearg = \"mode arg = -s for straight or -l for line by line\" filearg = \"file arg = input file name\" iparg = \"ip arg = server IP address\" portarg = \"port arg = server port number\" if [ $# -eq 0 ] ; then printf '\\n' echo \"usage: \" $modearg \", \" $filearg \", \" $iparg \", \" $portarg printf '\\n' echo \"e.g.: $ ./readsample -s|-l sample 192.168.1.141 50300\" printf '\\n' exit 1 fi input = $2 ip = $3 port = $4 printf '\\n' while getopts \"h?s?l\" opt ; do case \" $opt \" in h | \\? ) echo \"help!.. not implemented yet, sorry :(\" printf '\\n' exit 0 ;; s ) echo \"straight mode..\" printf '\\n' printf '\\n' while IFS = read -r line do echo \" $line \" printf '\\n' echo -n \" $line \" | perl -e 'print pack \"H*\", <STDIN>' | timeout 0 .2 nc -w 1 -u \" $ip \" \" $port \" | xxd -p printf '\\n' done < \" $input \" printf '\\n' echo 'Done.' printf '\\n' exit 0 ;; l ) echo \"line by line mode..\" printf '\\n' printf '\\n' exec 3 < & 0 printf '\\n' while IFS = read -r line do echo \" $line \" printf '\\n' echo -n \" $line \" | perl -e 'print pack \"H*\", <STDIN>' | timeout 0 .2 nc -w 1 -u \" $ip \" \" $port \" | xxd -p printf '\\n\\nPress <enter> to continue: ' > & 2 read keypress < & 3 printf '\\n' done < \" $input \" exec 3 < & - printf '\\n' echo 'Done.' printf '\\n' exit 0 ;; esac done","title":"Linux - 02 - Network"},{"location":"linux/2-linux-network/#linux-02-network","text":"","title":"Linux - 02 - Network"},{"location":"linux/2-linux-network/#network-connection","text":"(sudo) netstat -tunlp","title":"Network connection"},{"location":"linux/2-linux-network/#live-network-connection","text":"Live log established network connections on a dedidcated port number of a load balancer server and forwarded IP of communication server filtered out: watch -d 'netstat -an | grep :<port> | grep ESTABLISHED | grep -v <ip>'","title":"Live network connection"},{"location":"linux/2-linux-network/#send-raw-data-hex-udp","text":"Sending hex raw data through UDP.","title":"Send raw data hex UDP"},{"location":"linux/2-linux-network/#references","text":"Linux nc command help and examples How To Use Netcat to Establish and Test TCP and UDP Connections | DigitalOcean linux - convert a hex string to binary and send with netcat - Unix & Linux Stack Exchange Linux/UNIX: Bash Read a File Line By Line - nixCraft shell script - Wait for keyboard input inside a while-read loop - Unix & Linux Stack Exchange linux - How to automatically close netcat connection after data is sent? - Server Fault Echo newline in Bash prints literal \\n - Stack Overflow perl - Shell magic wanted: format output of hexdump in a pipe - Stack Overflow","title":"References"},{"location":"linux/2-linux-network/#one-shot","text":"Command line, manual sending, one raw data frame (Like Network Stuff): echo -n \"830f9d10362f4bc8a1c..\" | perl -e 'print pack \"H*\", <STDIN>' | timeout 0.2 nc -w 1 -u <ip> <port> | xxd -p","title":"One shot"},{"location":"linux/2-linux-network/#script-straight-or-line-by-line","text":"Read file straight forward or line by line. sample (file): 8308352739096415820f01020102128362d80bb162d80bb11c1e6.. 8308359739077103745f01020102b2df62d80bb262d80bb11bd9d.. .. readsample script, chmod +x then ./readsample to execute: #!/bin/bash trap 'exit 130' INT modearg = \"mode arg = -s for straight or -l for line by line\" filearg = \"file arg = input file name\" iparg = \"ip arg = server IP address\" portarg = \"port arg = server port number\" if [ $# -eq 0 ] ; then printf '\\n' echo \"usage: \" $modearg \", \" $filearg \", \" $iparg \", \" $portarg printf '\\n' echo \"e.g.: $ ./readsample -s|-l sample 192.168.1.141 50300\" printf '\\n' exit 1 fi input = $2 ip = $3 port = $4 printf '\\n' while getopts \"h?s?l\" opt ; do case \" $opt \" in h | \\? ) echo \"help!.. not implemented yet, sorry :(\" printf '\\n' exit 0 ;; s ) echo \"straight mode..\" printf '\\n' printf '\\n' while IFS = read -r line do echo \" $line \" printf '\\n' echo -n \" $line \" | perl -e 'print pack \"H*\", <STDIN>' | timeout 0 .2 nc -w 1 -u \" $ip \" \" $port \" | xxd -p printf '\\n' done < \" $input \" printf '\\n' echo 'Done.' printf '\\n' exit 0 ;; l ) echo \"line by line mode..\" printf '\\n' printf '\\n' exec 3 < & 0 printf '\\n' while IFS = read -r line do echo \" $line \" printf '\\n' echo -n \" $line \" | perl -e 'print pack \"H*\", <STDIN>' | timeout 0 .2 nc -w 1 -u \" $ip \" \" $port \" | xxd -p printf '\\n\\nPress <enter> to continue: ' > & 2 read keypress < & 3 printf '\\n' done < \" $input \" exec 3 < & - printf '\\n' echo 'Done.' printf '\\n' exit 0 ;; esac done","title":"Script straight or line by line"},{"location":"microsoft/windows/1-windows-misc/","text":"Windows - 01 - Misc find process that uses a port Open CMD prompt as admin C:\\> netstat -ano -p tcp | find \"50307\" Last column shows the PID that uses the port C:\\> netstat --help -a Displays all connections and listening ports. -n Displays addresses and port numbers in numerical form. -o Displays the owning process ID associated with each connection. In PowerShell C:\\> Get-Process -Id (Get-NetTCPConnection -LocalPort 50307).OwningProcess grep equivalent find \"50307\" or findstr 50307 C:\\> netstat -ano -p tcp | findstr 50307 You can find the application based on the PID on the Processes tab in Windows Task Manager. netstat kill process C:\\> taskkill /F /PID pid_number unable to bind port SRC Windows Power Shell in admin mode: Restart-Service hns","title":"Windows - 01 - Misc"},{"location":"microsoft/windows/1-windows-misc/#windows-01-misc","text":"","title":"Windows - 01 - Misc"},{"location":"microsoft/windows/1-windows-misc/#find-process-that-uses-a-port","text":"Open CMD prompt as admin C:\\> netstat -ano -p tcp | find \"50307\" Last column shows the PID that uses the port C:\\> netstat --help -a Displays all connections and listening ports. -n Displays addresses and port numbers in numerical form. -o Displays the owning process ID associated with each connection. In PowerShell C:\\> Get-Process -Id (Get-NetTCPConnection -LocalPort 50307).OwningProcess","title":"find process that uses a port"},{"location":"microsoft/windows/1-windows-misc/#grep-equivalent","text":"find \"50307\" or findstr 50307 C:\\> netstat -ano -p tcp | findstr 50307 You can find the application based on the PID on the Processes tab in Windows Task Manager. netstat","title":"grep equivalent"},{"location":"microsoft/windows/1-windows-misc/#kill-process","text":"C:\\> taskkill /F /PID pid_number","title":"kill process"},{"location":"microsoft/windows/1-windows-misc/#unable-to-bind-port","text":"SRC Windows Power Shell in admin mode: Restart-Service hns","title":"unable to bind port"},{"location":"microsoft/wsl/1-wsl-misc/","text":"WSL - 01 - Misc Windows Subsystem for Linux Explorer SRC Windows explorer to retrieve files from WSL (terminal), type in: explorer.exe . From Windows explorer address bar (path), type in: \\\\wsl$","title":"WSL - 01 - Misc"},{"location":"microsoft/wsl/1-wsl-misc/#wsl-01-misc","text":"Windows Subsystem for Linux","title":"WSL - 01 - Misc"},{"location":"microsoft/wsl/1-wsl-misc/#explorer","text":"SRC Windows explorer to retrieve files from WSL (terminal), type in: explorer.exe . From Windows explorer address bar (path), type in: \\\\wsl$","title":"Explorer"},{"location":"mkdocs/misc/","text":"misc Installation pip install mkdocs check mkdocs --version Initialize current folder mkdocs new . Build documentation mkdocs build Deploy to github First synchronize current folder with corresponding github repository. mkdocs gh-deploy Material for MkDocs (theme) Set up pip install mkdocs-material Configuration Simply add the following lines to mkdocs.yml to enable the theme. Also feature for code block annotation and extension setup. theme : name : material features : - content.code.annotate markdown_extensions : - pymdownx.highlight : anchor_linenums : true - pymdownx.inlinehilite - pymdownx.snippets - pymdownx.superfences Link A double dash section title like below ## Node server project Should be referenced like below to be used in a link: #node-server-project To link this section from another markdown file: [Node server project](otherFile.md#node-server-project) VS Code Extension Usefull markdown editing VS Code extensions: Markdown All in One markdownlint Markdown All in One allow code autocompletion for inserting code block by hitting 'Ctrl+space' shortcut keys. Fix warning When published on github, fix warning issue","title":"misc"},{"location":"mkdocs/misc/#misc","text":"","title":"misc"},{"location":"mkdocs/misc/#installation","text":"pip install mkdocs check mkdocs --version","title":"Installation"},{"location":"mkdocs/misc/#initialize-current-folder","text":"mkdocs new .","title":"Initialize current folder"},{"location":"mkdocs/misc/#build-documentation","text":"mkdocs build","title":"Build documentation"},{"location":"mkdocs/misc/#deploy-to-github","text":"First synchronize current folder with corresponding github repository. mkdocs gh-deploy","title":"Deploy to github"},{"location":"mkdocs/misc/#material-for-mkdocs-theme","text":"","title":"Material for MkDocs (theme)"},{"location":"mkdocs/misc/#set-up","text":"pip install mkdocs-material","title":"Set up"},{"location":"mkdocs/misc/#configuration","text":"Simply add the following lines to mkdocs.yml to enable the theme. Also feature for code block annotation and extension setup. theme : name : material features : - content.code.annotate markdown_extensions : - pymdownx.highlight : anchor_linenums : true - pymdownx.inlinehilite - pymdownx.snippets - pymdownx.superfences","title":"Configuration"},{"location":"mkdocs/misc/#link","text":"A double dash section title like below ## Node server project Should be referenced like below to be used in a link: #node-server-project To link this section from another markdown file: [Node server project](otherFile.md#node-server-project)","title":"Link"},{"location":"mkdocs/misc/#vs-code-extension","text":"Usefull markdown editing VS Code extensions: Markdown All in One markdownlint Markdown All in One allow code autocompletion for inserting code block by hitting 'Ctrl+space' shortcut keys.","title":"VS Code Extension"},{"location":"mkdocs/misc/#fix-warning","text":"When published on github, fix warning issue","title":"Fix warning"},{"location":"mongodb/mongodb/","text":"MongoDB Clear console > cls","title":"MongoDB - Misc"},{"location":"mongodb/mongodb/#mongodb","text":"","title":"MongoDB"},{"location":"mongodb/mongodb/#clear-console","text":"> cls","title":"Clear console"},{"location":"okrmethodology/redaction/","text":"M\u00e9thodologie OKR - R\u00e9daction R\u00e9daction des Objectifs et R\u00e9sultats Cl\u00e9s. Introduction Comment r\u00e9diger un OKR ? Sources : Comment r\u00e9diger le parfait OKR Objectives and Key Results Creating Better Key Results Set objectives and develop key results How do you define a key result Le guide ultime des objectifs et r\u00e9sultats cl\u00e9s Objectif Objectifs : descriptions qualitatives et m\u00e9morables des objectifs \u00e0 atteindre. objectif, r\u00e9pond \u00e0 2 questions: QUE voulons-nous r\u00e9aliser ? POURQUOI voulons-nous atteindre cet objectif ?\u200b L'objectif doit donc comprendre un \"QUOI\" et un \"POUQUOI\": D\u00e9velopper une nouvelle fonctionnalit\u00e9 afin d'augmenter le taux de r\u00e9tention (= % clients existants qui restent fid\u00e8les par p\u00e9riode). Objectif, intention claire, compr\u00e9hensible en quelques secondes. Key Result R\u00e9sultats cl\u00e9s : ensemble de m\u00e9triques qui permettent d'\u00e9valuer votre avancement vers l'objectif. key result, r\u00e9pond \u00e0 1 question: COMMENT saurons-nous que nous avons atteint notre objectif ? Les r\u00e9sultats cl\u00e9s doivent d\u00e9crire les r\u00e9sultats (d\u00e9livrables), et non les activit\u00e9s. Si les KR incluent des mots comme \u00ab consulter \u00bb, \u00ab aider \u00bb, \u00ab analyser \u00bb, \u00ab participer \u00bb, ils d\u00e9crivent des activit\u00e9s. D\u00e9crivez plut\u00f4t l'impact de ces activit\u00e9s, par exemple, \u00ab publier les niveaux de satisfaction du service client d'ici le 7 mars \u00bb plut\u00f4t que \u00ab \u00e9valuer la satisfaction du service client \u00bb. Un r\u00e9sultats est donc mesurables, par m\u00e9trique ou jalon: M\u00e9trique: Positif : - Passer de 5 \u00e0 20 CHF de profit sur le produit X : [X] 05 CHF [X] 10 CHF [_] 20 CHF N\u00e9gatif : - R\u00e9duire de 7 \u00e0 5 % le taux de churn de nos leads : [X] 07 CHF [_] 06 CHF [_] 05 CHF Seuil : - Maintenir un Net Promoted Score entre +16 et +48 (p\u00e9riode date 1 -> 3) : [X] 33% - valeur 1 = 32 => OK!, date 1 [_] 33% - valeur 2 = -4 => KO!, date 2 [X] 33% - valeur 3 = 19 => OK!, date 3 Jalon (% - r\u00e9sultat produit, du produit total, pas forc\u00e9ment lin\u00e9aire, certaines phases ont plus de poid que d'autres): Milestone : - Avoir le produit X termin\u00e9 [X] 000% Kick Off [X] 005% Concept [X] 015% Requirements [X] 030% Design [_] 060% Development [_] 075% Inhouse Testing [_] 090% Field Testing [_] 100% Released","title":"OKR - R\u00e9daction"},{"location":"okrmethodology/redaction/#methodologie-okr-redaction","text":"R\u00e9daction des Objectifs et R\u00e9sultats Cl\u00e9s.","title":"M\u00e9thodologie OKR - R\u00e9daction"},{"location":"okrmethodology/redaction/#introduction","text":"Comment r\u00e9diger un OKR ? Sources : Comment r\u00e9diger le parfait OKR Objectives and Key Results Creating Better Key Results Set objectives and develop key results How do you define a key result Le guide ultime des objectifs et r\u00e9sultats cl\u00e9s","title":"Introduction"},{"location":"okrmethodology/redaction/#objectif","text":"Objectifs : descriptions qualitatives et m\u00e9morables des objectifs \u00e0 atteindre. objectif, r\u00e9pond \u00e0 2 questions: QUE voulons-nous r\u00e9aliser ? POURQUOI voulons-nous atteindre cet objectif ?\u200b L'objectif doit donc comprendre un \"QUOI\" et un \"POUQUOI\": D\u00e9velopper une nouvelle fonctionnalit\u00e9 afin d'augmenter le taux de r\u00e9tention (= % clients existants qui restent fid\u00e8les par p\u00e9riode). Objectif, intention claire, compr\u00e9hensible en quelques secondes.","title":"Objectif"},{"location":"okrmethodology/redaction/#key-result","text":"R\u00e9sultats cl\u00e9s : ensemble de m\u00e9triques qui permettent d'\u00e9valuer votre avancement vers l'objectif. key result, r\u00e9pond \u00e0 1 question: COMMENT saurons-nous que nous avons atteint notre objectif ? Les r\u00e9sultats cl\u00e9s doivent d\u00e9crire les r\u00e9sultats (d\u00e9livrables), et non les activit\u00e9s. Si les KR incluent des mots comme \u00ab consulter \u00bb, \u00ab aider \u00bb, \u00ab analyser \u00bb, \u00ab participer \u00bb, ils d\u00e9crivent des activit\u00e9s. D\u00e9crivez plut\u00f4t l'impact de ces activit\u00e9s, par exemple, \u00ab publier les niveaux de satisfaction du service client d'ici le 7 mars \u00bb plut\u00f4t que \u00ab \u00e9valuer la satisfaction du service client \u00bb. Un r\u00e9sultats est donc mesurables, par m\u00e9trique ou jalon: M\u00e9trique: Positif : - Passer de 5 \u00e0 20 CHF de profit sur le produit X : [X] 05 CHF [X] 10 CHF [_] 20 CHF N\u00e9gatif : - R\u00e9duire de 7 \u00e0 5 % le taux de churn de nos leads : [X] 07 CHF [_] 06 CHF [_] 05 CHF Seuil : - Maintenir un Net Promoted Score entre +16 et +48 (p\u00e9riode date 1 -> 3) : [X] 33% - valeur 1 = 32 => OK!, date 1 [_] 33% - valeur 2 = -4 => KO!, date 2 [X] 33% - valeur 3 = 19 => OK!, date 3 Jalon (% - r\u00e9sultat produit, du produit total, pas forc\u00e9ment lin\u00e9aire, certaines phases ont plus de poid que d'autres): Milestone : - Avoir le produit X termin\u00e9 [X] 000% Kick Off [X] 005% Concept [X] 015% Requirements [X] 030% Design [_] 060% Development [_] 075% Inhouse Testing [_] 090% Field Testing [_] 100% Released","title":"Key Result"},{"location":"postman/misc/","text":"misc Cookie Access website resources through Postman requests with cookie session of a browser. To get cookie session, after login to web page with dev tool (right click - inspect) opened, retrieve a transaction that concern current website in Network tab. Then in Headers - Request Headers retrieve entry for Cookie SESSION=ZWEwMjYwMWI...; . In Postman, in Headers tab of your request add a KEY = Cookie and VALUE = SESSION=ZWEwMjYwMWI...; with copied session value from above. Now you may access website resources through Postman requests with cookie session of your browser.","title":"misc"},{"location":"postman/misc/#misc","text":"","title":"misc"},{"location":"postman/misc/#cookie","text":"Access website resources through Postman requests with cookie session of a browser. To get cookie session, after login to web page with dev tool (right click - inspect) opened, retrieve a transaction that concern current website in Network tab. Then in Headers - Request Headers retrieve entry for Cookie SESSION=ZWEwMjYwMWI...; . In Postman, in Headers tab of your request add a KEY = Cookie and VALUE = SESSION=ZWEwMjYwMWI...; with copied session value from above. Now you may access website resources through Postman requests with cookie session of your browser.","title":"Cookie"},{"location":"redis/1-redis-misc/","text":"Redis - 01 - Misc Basic operations Basic operations from command line interface ( source ). Test redis-cli ping Client redis-cli Create SET mykey \"Hi!\" Read GET mykey Update SET mykey \"Hello\" Delete DEL mykey List all keys keys * Monitor Source redis-cli monitor Monitor spring session Spring Session This is a sample use case where spring sessions are stored in redis and how to monitor them: redis-cli -a <redis.password> monitor | grep -e 'spring\\:session*' Quit exit Remove all To remove all keys of all existing databases, run: redis-cli FLUSHALL","title":"Redis - 01 - Misc"},{"location":"redis/1-redis-misc/#redis-01-misc","text":"","title":"Redis - 01 - Misc"},{"location":"redis/1-redis-misc/#basic-operations","text":"Basic operations from command line interface ( source ).","title":"Basic operations"},{"location":"redis/1-redis-misc/#test","text":"redis-cli ping","title":"Test"},{"location":"redis/1-redis-misc/#client","text":"redis-cli","title":"Client"},{"location":"redis/1-redis-misc/#create","text":"SET mykey \"Hi!\"","title":"Create"},{"location":"redis/1-redis-misc/#read","text":"GET mykey","title":"Read"},{"location":"redis/1-redis-misc/#update","text":"SET mykey \"Hello\"","title":"Update"},{"location":"redis/1-redis-misc/#delete","text":"DEL mykey","title":"Delete"},{"location":"redis/1-redis-misc/#list-all-keys","text":"keys *","title":"List all keys"},{"location":"redis/1-redis-misc/#monitor","text":"Source redis-cli monitor","title":"Monitor"},{"location":"redis/1-redis-misc/#monitor-spring-session","text":"Spring Session This is a sample use case where spring sessions are stored in redis and how to monitor them: redis-cli -a <redis.password> monitor | grep -e 'spring\\:session*'","title":"Monitor spring session"},{"location":"redis/1-redis-misc/#quit","text":"exit","title":"Quit"},{"location":"redis/1-redis-misc/#remove-all","text":"To remove all keys of all existing databases, run: redis-cli FLUSHALL","title":"Remove all"},{"location":"vscode/vscode/","text":"VS Code Switch windows To switch from editing area (above) and terminal area (below): Ctrl + arrow up/down To switch from same area type windows (edit/terminal): Alt + arrow left/right Code snippet shortcut Ctrl+space Console code block in markdown file: Ctrl+space, then select fenced codeblock, then select console ... Move line Alt + Up/down keys Duplicate line If you want to copy the line to the line above itself, press Shift + Alt + Up Arrow Key. If you want to copy the line to the line below itself, press Shift + Alt + Down Arrow Key. Hit Ctrl + c (without anything selected on a line) and then Ctrl + v will directly copy/paste the entire line even nothing is selected. Column select Mouse Shift + Alt then click and drag Keyboard Ctrl + Shift + Alt then use arrow keys Comment code Comment code block Ctrl + K + C or line Ctrl + \u00a7 Uncomment code block Ctrl + K + U or line Ctrl + \u00a7 Multiple cursors Multiple selections (multi-cursor) Alt+Click . Each cursor operates independently based on the context it sits in. A common way to add more cursors is with Ctrl+Alt+Down or Ctrl+Alt+Up that insert cursors below or above. Open from terminal To open a file in VS Code editor from integrated terminal: code -r fileName Open an other instance To open an other instance (from VS Code integrated terminal) or a VS Code instance from another terminal: code . Keyboard shorcut Set Press Ctrl + Shift + P keyboard shortcut and this will open Command Palette. There type Open Keyboard Shortcuts and select it from the list. In search bar, type e.g.: - 'ctrl + \u00a7' to find default toggle line comment shortcut. Reset Press Ctrl + Shift + P keyboard shortcut and this will open Command Palette. There type Open Keyboard Shortcuts (JSON) and select it from the list. You may remove one or more undesired entries or simply remove everything from keybindings.json and type empty [] into it, then save. IntelliSense IntelliSense is a general term for various code editing features Suggestion widget Hit Ctrl + Space to trigger IntelliSense suggestion widget. Hide terminal To toggle terminal view: Ctrl + J Hide file explorer To toggle file explorer view: Ctrl + B Run code Quickly run a piece of code (JS) to debug, create a new test.js file and your code sample in it: F1 -> Run ..","title":"VS Code - Misc"},{"location":"vscode/vscode/#vs-code","text":"","title":"VS Code"},{"location":"vscode/vscode/#switch-windows","text":"To switch from editing area (above) and terminal area (below): Ctrl + arrow up/down To switch from same area type windows (edit/terminal): Alt + arrow left/right","title":"Switch windows"},{"location":"vscode/vscode/#code-snippet-shortcut","text":"Ctrl+space Console code block in markdown file: Ctrl+space, then select fenced codeblock, then select console ...","title":"Code snippet shortcut"},{"location":"vscode/vscode/#move-line","text":"Alt + Up/down keys","title":"Move line"},{"location":"vscode/vscode/#duplicate-line","text":"If you want to copy the line to the line above itself, press Shift + Alt + Up Arrow Key. If you want to copy the line to the line below itself, press Shift + Alt + Down Arrow Key. Hit Ctrl + c (without anything selected on a line) and then Ctrl + v will directly copy/paste the entire line even nothing is selected.","title":"Duplicate line"},{"location":"vscode/vscode/#column-select","text":"Mouse Shift + Alt then click and drag Keyboard Ctrl + Shift + Alt then use arrow keys","title":"Column select"},{"location":"vscode/vscode/#comment-code","text":"Comment code block Ctrl + K + C or line Ctrl + \u00a7 Uncomment code block Ctrl + K + U or line Ctrl + \u00a7","title":"Comment code"},{"location":"vscode/vscode/#multiple-cursors","text":"Multiple selections (multi-cursor) Alt+Click . Each cursor operates independently based on the context it sits in. A common way to add more cursors is with Ctrl+Alt+Down or Ctrl+Alt+Up that insert cursors below or above.","title":"Multiple cursors"},{"location":"vscode/vscode/#open-from-terminal","text":"To open a file in VS Code editor from integrated terminal: code -r fileName","title":"Open from terminal"},{"location":"vscode/vscode/#open-an-other-instance","text":"To open an other instance (from VS Code integrated terminal) or a VS Code instance from another terminal: code .","title":"Open an other instance"},{"location":"vscode/vscode/#keyboard-shorcut","text":"","title":"Keyboard shorcut"},{"location":"vscode/vscode/#set","text":"Press Ctrl + Shift + P keyboard shortcut and this will open Command Palette. There type Open Keyboard Shortcuts and select it from the list. In search bar, type e.g.: - 'ctrl + \u00a7' to find default toggle line comment shortcut.","title":"Set"},{"location":"vscode/vscode/#reset","text":"Press Ctrl + Shift + P keyboard shortcut and this will open Command Palette. There type Open Keyboard Shortcuts (JSON) and select it from the list. You may remove one or more undesired entries or simply remove everything from keybindings.json and type empty [] into it, then save.","title":"Reset"},{"location":"vscode/vscode/#intellisense","text":"IntelliSense is a general term for various code editing features","title":"IntelliSense"},{"location":"vscode/vscode/#suggestion-widget","text":"Hit Ctrl + Space to trigger IntelliSense suggestion widget.","title":"Suggestion widget"},{"location":"vscode/vscode/#hide-terminal","text":"To toggle terminal view: Ctrl + J","title":"Hide terminal"},{"location":"vscode/vscode/#hide-file-explorer","text":"To toggle file explorer view: Ctrl + B","title":"Hide file explorer"},{"location":"vscode/vscode/#run-code","text":"Quickly run a piece of code (JS) to debug, create a new test.js file and your code sample in it: F1 -> Run ..","title":"Run code"}]}